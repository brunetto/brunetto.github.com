<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Post It!</title>
    <link>http://brunettoziosi.eu/categories/python/</link>
    <description>Recent content in Python on Post It!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Sep 2014 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://brunettoziosi.eu/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Python plot examples</title>
      <link>http://brunettoziosi.eu/posts/python-plot-examples/</link>
      <pubDate>Mon, 29 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/python-plot-examples/</guid>
      <description>&lt;p&gt;Two examples on how to make plots with &lt;a href=&#34;http://home.gna.org/veusz&#34;&gt;Veusz&lt;/a&gt; and
&lt;a href=&#34;http://matplotlib.org&#34;&gt;Matplotlib&lt;/a&gt;.&lt;br /&gt;
I prefer Veusz because it&amp;rsquo;s easier to configure, modify and it produces
perfect &lt;code&gt;pdf&lt;/code&gt; plots, but sometimes Matplotlib it&amp;rsquo;s faster for producing just
a draft plot to inspect data!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
# -*- coding: utf8 -*- 

from __future__ import division # no more &amp;quot;zero&amp;quot; integer division bugs!:P
import time
import numpy as np
import veusz.embed as ve

def sm_hist(data, delta=5, n_bin=None, range_=None):
	dataMin = np.floor(data.min())
	dataMax = np.ceil(data.max())
	n_bin = np.ceil(1.*(dataMax-dataMin) / delta)
	range_ = (dataMin, dataMin + n_bin * delta)
	counts, bin_edges = np.histogram(data, n_bin, range_, density = False)
	return counts, bin_edges
	
def sm_hist2(data, delta=5):
	dataMin = np.floor(data.min())
	dataMax = np.ceil(data.max())
	n_bin = np.ceil(1.*(dataMax-dataMin) / delta) + 1
	idxs = ((data  - dataMin) / delta).astype(int)
	counts = np.zeros(n_bin) 
	bin_edges = np.arange(dataMin, dataMax+2, delta)
	for idx in idxs:
		counts[idx] += 1
	counts = np.hstack((np.array([0]), counts, np.array([0])))
	bin_edges = np.hstack((bin_edges[0], bin_edges, bin_edges[-1]))
	return counts, bin_edges

def plotFunc(inpath=&amp;quot;./&amp;quot;, outpath=&amp;quot;./&amp;quot;):
	font = &amp;quot;Times New Roman&amp;quot;
	colors = [u&#39;blue&#39;, u&#39;green&#39;]
	xmin = [&amp;quot;auto&amp;quot;, &amp;quot;auto&amp;quot;]
	xmax = [&amp;quot;auto&amp;quot;, &amp;quot;auto&amp;quot;]
	ymin = [&amp;quot;auto&amp;quot;, 0]
	ymax = [&amp;quot;auto&amp;quot;, &amp;quot;auto&amp;quot;]
	
	xData = np.arange(100) 
	yData = np.random.randint(0, 100, size=100) + np.sin(np.arange(100))
	
	figure = ve.Embedded(&amp;quot;Window_1&amp;quot;)
	page = figure.Root.Add(&#39;page&#39;, width = &#39;30cm&#39;, height=&#39;15cm&#39;)
	grid = page.Add(&#39;grid&#39;, autoadd = False, rows = 1, columns = 2,
						scaleRows=[0.2],
						topMargin=&#39;1cm&#39;,
						bottomMargin=&#39;1cm&#39;
						)
	graphList = []
	
	graphList.append(grid.Add(&#39;graph&#39;, name=&amp;quot;scatter&amp;quot;, autoadd=False, 
							hide = False, 
							Border__width = &#39;2pt&#39;,
							leftMargin = &#39;0.6cm&#39;,
							rightMargin = &#39;0.4cm&#39;,
							topMargin = &#39;0.5cm&#39;,
							bottomMargin = &#39;1cm&#39;,
							))
	
	graphList.append(grid.Add(&#39;graph&#39;, name=&amp;quot;hist&amp;quot;, autoadd=False, 
							hide = False, 
							Border__width = &#39;2pt&#39;,
							leftMargin = &#39;2cm&#39;,
							rightMargin = &#39;0.4cm&#39;,
							topMargin = &#39;0.5cm&#39;,
							bottomMargin = &#39;1cm&#39;,
							))
	
	for i in range(len(graphList)):
		graphList[i].Add(&#39;axis&#39;, name=&#39;x&#39;, label = &amp;quot;x&amp;quot;,
								min = xmin[i],
								max = xmax[i],
								log = False,
								Label__size = &#39;25pt&#39;,
								Label__font = font,
								TickLabels__size = &#39;17pt&#39;,
								TickLabels__format = u&#39;Auto&#39;,
								MajorTicks__width = &#39;2pt&#39;,
								MajorTicks__length = &#39;10pt&#39;,
								MinorTicks__width = &#39;1pt&#39;,
								MinorTicks__length = &#39;6pt&#39;
							)
		graphList[i].Add(&#39;axis&#39;, name=&#39;y&#39;, label = &amp;quot;y&amp;quot;, 
								direction = &#39;vertical&#39;,
								min = ymin[i],
								max = ymax[i],
								log = False,
								autoRange = u&#39;+5%&#39;,
								Label__size = &#39;25pt&#39;,
								Label__font = font,
								TickLabels__size = &#39;20pt&#39;,
								TickLabels__format = u&#39;Auto&#39;,
								MajorTicks__width = &#39;2pt&#39;,
								MajorTicks__length = &#39;10pt&#39;,
								MinorTicks__width = &#39;1pt&#39;,
								MinorTicks__length = &#39;6pt&#39;
							)
	
	graphList[0].Add(&#39;xy&#39;, key=&amp;quot;scatterPlotKey&amp;quot;, name=&#39;scatterPlotName&#39;,
						marker = u&#39;circle&#39;,
						MarkerFill__color = colors[0],
						markerSize = u&#39;3pt&#39;, 
						)

	xDataName = &amp;quot;xScatterData&amp;quot;
	yDataName = &amp;quot;yScatterData&amp;quot;
	figure.SetData(xDataName, xData)
	figure.SetData(yDataName, yData)
	graphList[0].scatterPlotName.xData.val = xDataName
	graphList[0].scatterPlotName.yData.val = yDataName
	
	
	counts, bin_edges = sm_hist2(yData, delta=5)
	
	graphList[1].Add(&#39;xy&#39;, key=&amp;quot;histPlotKey&amp;quot;, name=&#39;histPlotName&#39;,
						xData = bin_edges,
						yData = counts,
						marker = &#39;none&#39;,
						PlotLine__steps = u&#39;left&#39;,
						PlotLine__color = colors[1],
						PlotLine__style = u&amp;quot;solid&amp;quot;,
						PlotLine__width = u&#39;3&#39;,
						FillBelow__color = colors[1],
						FillBelow__style = &amp;quot;forward 2&amp;quot;,
						FillBelow__hide = False,
						FillBelow__transparency = 70,
						#FillBelow__backtransparency = 50,
						FillBelow__linewidth = &#39;1pt&#39;,
						FillBelow__linestyle = &#39;solid&#39;,
						FillBelow__backcolor = &amp;quot;white&amp;quot;,
						FillBelow__backhide = True,
						Label__posnHorz = &#39;right&#39;,
						Label__size = &#39;14pt&#39;, 
						Label__color = &#39;black&#39;
						)

	histKey = graphList[1].Add(&#39;key&#39;, autoadd=False, 
						horzPosn = &#39;left&#39;,
						vertPosn = &#39;top&#39;,
						Text__font = font,
						Text__size = &#39;15&#39;,
						Border__width = &#39;1.5pt&#39;
						)
	
	end = raw_input(&amp;quot;Press any key to finish...&amp;quot;)
	
	figure.Save(&amp;quot;example.vsz&amp;quot;)
	figure.Export(&amp;quot;example.png&amp;quot;, backcolor=&#39;#ffffff&#39;)
	figure.Export(&amp;quot;example.pdf&amp;quot;)

if __name__ == &amp;quot;__main__&amp;quot;:
	inpath = &amp;quot;./&amp;quot;
	outpath = &#39;./&#39;
	tt = time.time()
	plotFunc(inpath, outpath)
	print &amp;quot;Done in &amp;quot;, time.time()-tt, &amp;quot; seconds.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../../files/example.png&#34; alt=&#34;Veusz plot&#34; title=&#34;Veusz plot&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
# -*- coding: utf8 -*- 

from __future__ import division # no more &amp;quot;zero&amp;quot; integer division bugs!:P
import time
import numpy as np
import matplotlib.pylab as plt
import matplotlib.font_manager as font_manager

# SM like style
params = {&#39;backend&#39;: &#39;png&#39;,
		&#39;font.family&#39;: &amp;quot;serif&amp;quot;,
		&#39;font.size&#39;: 25,
		&#39;axes.labelsize&#39;: 35,
		#&#39;text.fontsize&#39;: 30,
		&#39;legend.fontsize&#39;: 30,
		&#39;xtick.labelsize&#39;: 28,
		&#39;xtick.major.size&#39;: 20.0,
		&#39;xtick.major.width&#39;: 3.0,
		&#39;xtick.minor.size&#39;: 12.0,
		&#39;xtick.minor.width&#39;: 2,
		&#39;ytick.labelsize&#39;: 28,
		&#39;ytick.major.size&#39;: 20.0,
		&#39;ytick.major.width&#39;: 3.0,
		&#39;ytick.minor.size&#39;: 12.0,
		&#39;ytick.minor.width&#39;: 2,
		#&#39;text.usetex&#39;: True,
		&#39;axes.linewidth&#39;: 3.0,
		&#39;lines.linewidth&#39;: 2,
		&#39;lines.markersize&#39;: 15,
		&#39;axes.grid&#39;: False,
		&#39;grid&#39;: {&#39;color&#39;:&#39;gray&#39;, &#39;linestyle&#39;:&#39;-&#39;, &#39;linewidth&#39;:1},
		&#39;figure.figsize&#39;: (10,10),
		&#39;figure.subplot.left&#39;: 0.15,  # the left side of the subplots of the figure
		&#39;figure.subplot.right&#39;   : 0.95,    # the right side of the subplots of the figure
		&#39;figure.subplot.bottom&#39;  : 0.12,   # the bottom of the subplots of the figure
		&#39;figure.subplot.top&#39;     : 0.92,    # the top of the subplots of the figure
		&#39;figure.subplot.wspace&#39;  : 0.2,    # the amount of width reserved for blank space between subplots
		&#39;figure.subplot.hspace&#39;  : 0.2,    # the amount of height reserved for white space between subplots
		&#39;figure.figsize&#39;: (12, 12)
           }
plt.rcParams.update(params)
	
def sm_hist(data, delta=5, n_bin=None, range_=None):
	dataMin = np.floor(data.min())
	dataMax = np.ceil(data.max())
	n_bin = np.ceil(1.*(dataMax-dataMin) / delta)
	range_ = (dataMin, dataMin + n_bin * delta)
	counts, bin_edges = np.histogram(data, n_bin, range_, density = False)
	# These two lines double the points let you make the histogram
	counts = np.ravel(zip(counts,counts)) 
	bin_edges = np.ravel(zip(bin_edges,bin_edges))
	counts = np.hstack((np.array([0]), counts, np.array([0])))
	return counts, bin_edges
	
	
def sm_hist2(data, delta=5):
	dataMin = np.floor(data.min())
	dataMax = np.ceil(data.max())
	n_bin = np.ceil(1.*(dataMax-dataMin) / delta) + 1
	idxs = ((data  - dataMin) / delta).astype(int)
	counts = np.zeros(n_bin) 
	bin_edges = np.arange(dataMin, dataMax+delta, delta)
	for idx in idxs:
		counts[idx] += 1
	print counts
	# These two lines double the points let you make the histogram
	counts = np.ravel(zip(counts,counts)) 
	bin_edges = np.ravel(zip(bin_edges,bin_edges))
	counts = np.hstack((np.array([0]), counts))
	bin_edges = np.hstack((bin_edges, bin_edges[-1]))
	return counts, bin_edges

def singlePlotScatter(xData, yData, nRows, nCols, x0, y0, rowspan, colspan):
	ax = plt.subplot2grid((nRows,nCols), (x0,y0), rowspan, colspan)
	ax.set_xlabel(&amp;quot;x label&amp;quot;)
	ax.set_ylabel(&amp;quot;y label&amp;quot;)
	ax.set_xscale(&amp;quot;linear&amp;quot;)
	ax.set_yscale(&amp;quot;linear&amp;quot;)
	ax.set_title(&amp;quot;Plot title&amp;quot;)
	ax.title.set_y(1.02) # adjust title position
	ax.xaxis.grid(True, which=&amp;quot;both&amp;quot;)
	ax.yaxis.grid(True, which=&amp;quot;major&amp;quot;)
	ax.plot(xData, yData, 
			color = &amp;quot;green&amp;quot;, 
			markeredgewidth = 0.8, 
			linestyle = &#39;-&#39;, 
			linewidth = 2,
			marker = &#39;o&#39;, 
			markersize = 1, 
			label = &amp;quot;label&amp;quot;)
	return ax

def singlePlotHist(yData, nRows, nCols, x0, y0, rowspan, colspan):
	ax = plt.subplot2grid((nRows,nCols), (x0,y0), rowspan, colspan)
	ax.set_xlabel(&amp;quot;x label&amp;quot;)
	ax.set_ylabel(&amp;quot;y label&amp;quot;)
	ax.set_xscale(&amp;quot;linear&amp;quot;)
	ax.set_yscale(&amp;quot;linear&amp;quot;)
	ax.set_title(&amp;quot;Plot title&amp;quot;)
	ax.title.set_y(1.02) # adjust title position
	ax.xaxis.grid(True, which=&amp;quot;both&amp;quot;)
	ax.yaxis.grid(True, which=&amp;quot;major&amp;quot;)
	counts, bin_edges = sm_hist2(yData, delta = 10)
	ax.set_ylim((0, 1.2*counts.max()))
	
	ax.plot(bin_edges, counts, 
				color = &amp;quot;blue&amp;quot;,
				alpha = 0.8,
				linewidth = 2,
				antialiased = True,
				zorder = 3 
				)
	ax.fill(bin_edges, counts, 
				alpha = 0.5,
				hatch = &amp;quot;/&amp;quot;,
				edgecolor = &amp;quot;blue&amp;quot;,
				facecolor = &amp;quot;white&amp;quot;,
				antialiased = True, 
				label = &amp;quot;whatever you want&amp;quot;
				)
	ax.legend(loc=&#39;upper left&#39;, numpoints = 1, prop=font_manager.FontProperties(size=18)).draw_frame(False)

	return ax

if __name__ == &amp;quot;__main__&amp;quot;:
	tt = time.time()
	xData = np.arange(100) 
	yData = np.random.randint(0, 100, size=100) + np.sin(np.arange(100))
	
	fig = plt.figure()
	fig.suptitle(&amp;quot;Figure title&amp;quot;)
	axs = []
	nPlots = 2
	
	axs.append(singlePlotScatter(xData, yData, nRows=1, nCols=2, x0=0, y0=0, rowspan=1, colspan=1))
	axs.append(singlePlotHist(yData, nRows=1, nCols=2, x0=0, y0=1, rowspan=1, colspan=1))
	
	fig.set_size_inches(20, 10)
	plt.savefig(&amp;quot;./grid.png&amp;quot;, dpi=100)
	plt.close(fig)
	
	print &amp;quot;Done in &amp;quot;, time.time()-tt, &amp;quot; seconds.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../../files/grid.png&#34; alt=&#34;Matplotlib plot&#34; title=&#34;Mathplotlib plot&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python references</title>
      <link>http://brunettoziosi.eu/pages/tech/python-references/</link>
      <pubDate>Wed, 21 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/pages/tech/python-references/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kirang89/pycrumbs/blob/master/pycrumbs.md&#34;&gt;Super Python Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.astropy.org/&#34;&gt;Astropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://astrocompute.wordpress.com/&#34;&gt;Astrocompute&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.astropython.org/&#34;&gt;Astropython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://python4astronomers.github.io/&#34;&gt;Python4astronomers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.astrobetter.com/wiki/tiki-index.php?page=Python+Setup+for+Astronomy&#34;&gt;Astrobetter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://astlib.sourceforge.net/&#34;&gt;AstroLib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://oneau.wordpress.com/2010/10/02/python-for-astronomy/&#34;&gt;Python for Astronomy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.astroml.org/&#34;&gt;AstroML&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://bellm.org/blog/2011/05/27/why-astronomers-should-program-in-python/&#34;&gt;Why Astronomers Should Program in Python&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://damianavila.github.io/scipy2013_talks/index.html#/&#34;&gt;Ipython presentations&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/urls/raw.github.com/ipython/ipython/1.x/examples/notebooks/Part%205%20-%20Rich%20Display%20System.ipynb&#34;&gt;Examples notebooks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://scipy-lectures.github.io/&#34;&gt;Scipy lectures&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://slendrmeans.wordpress.com/2012/12/05/better-typography-for-ipython-notebooks/&#34;&gt;Better ipython typography&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/jrjohansson/scientific-python-lectures&#34;&gt;Scientific computing in Python&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Latex together with matplotlib!</title>
      <link>http://brunettoziosi.eu/posts/latex-together-with-matplotlib/</link>
      <pubDate>Fri, 01 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/latex-together-with-matplotlib/</guid>
      <description>&lt;p&gt;Sometimes you need to label your plot with math expression or Greek letters and ASCII is not enough.&lt;br /&gt;
Everything you need is to add to your script&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib import rc
rc(&#39;font&#39;,**{&#39;family&#39;:&#39;sans-serif&#39;,&#39;sans-serif&#39;:[&#39;Helvetica&#39;]})
## for Palatino and other serif fonts use:
#rc(&#39;font&#39;,**{&#39;family&#39;:&#39;serif&#39;,&#39;serif&#39;:[&#39;Palatino&#39;]))
rc(&#39;text&#39;, usetex=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and write you text label like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;label = r&#39;theoretical $f(nu)$&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Essential guide to binning</title>
      <link>http://brunettoziosi.eu/posts/essential-guide-to-binning/</link>
      <pubDate>Fri, 25 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/essential-guide-to-binning/</guid>
      <description>

&lt;p&gt;Often I found myself fighting against data binning, trying to understand the relation between linear and logarithmic bins and how to create the bin starting from the bins number or the bins spacing.&lt;br /&gt;
It&amp;rsquo;s time to write down some consideration and snippet!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To be updated&amp;hellip;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;linear-vs-logarithmic:87&#34;&gt;Linear vs logarithmic&lt;/h2&gt;

&lt;p&gt;I live in a linear space. My advisor and a lot of other scientists live in a logarithmic space. It&amp;rsquo;s quite difficult to easily communicate, but trying to &amp;ldquo;mask&amp;rdquo; this difference life can be more peaceful.&lt;br /&gt;
Hereafter I would like to thing about &amp;ldquo;equally spaced bins&amp;rdquo;. It&amp;rsquo;s not important if they are linearly or logarithmically equally spaced because you can take the same snippet of code and pass to it a logarithmic array, or logarithmic boundaries.&lt;/p&gt;

&lt;h2 id=&#34;from-one-to-the-other:87&#34;&gt;From one to the other&lt;/h2&gt;

&lt;p&gt;Suppose you have an array.  How much will be the bin spacing to obtain &lt;code&gt;n_bin&lt;/code&gt; bins?&lt;br /&gt;
It can be easily computed as&lt;br /&gt;
&lt;code&gt;$\delta_{bin} = (sup-inf)/n_{bin}$&lt;/code&gt;&lt;br /&gt;
From this it&amp;rsquo;s also straightforward to obtain the number of bins from the spacing:&lt;br /&gt;
&lt;code&gt;$n_{bin} = \lfloor(sup-inf)/\delta_{bin}\rfloor$&lt;/code&gt;&lt;br /&gt;
Note that we choose the number of bins to be integer.&lt;/p&gt;

&lt;h2 id=&#34;code:87&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;Here some code to bin your arrays:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import sys
import numpy as np

def binning(inf, sup, n_bin=None, delta_bin=None):
	&amp;quot;&amp;quot;&amp;quot;Given the inf and sup limits of an array and the number of equally spaced
	bins, it returns the bin centers, the bin limits and the bin spacing.
	It&#39;s possible to have a linear or a logarithmic spacing passing linear or
	logarithmic inf and sup, and searchsorting on a linear or logarithmic array, 
	or you can use a linear array and logarithmically spaced bins as
	new_bins = pow(10, logbins)
	&amp;quot;&amp;quot;&amp;quot;
	if (n_bin == 0) or (n_bin == 0):
		print &amp;quot;Error, n_bin and/or delta_bin are/is zero, exit!&amp;quot;
		sys.exit()    
	elif (n_bin == None) and (delta_bin != None):
		n_bin = (sup-inf)/delta_bin
		elif (n_bin == None) and (delta_bin == None):
		print &amp;quot;Error, n_bin and delta_bin are both None, exit!&amp;quot;
		sys.exit()
	temp, half_step = np.linspace(inf, sup, 2*n_bin+1, endpoint = True, retstep = True)
	xrange_limit = int(np.floor(temp.size / 2))
	bin_pos = np.zeros(xrange_limit)
	bin_limits = np.zeros(xrange_limit+1)
	for i in xrange(xrange_limit):
		bin_pos[i] = temp[2*i+1]
		bin_limits[i] = temp[2*i]
		bin_limits[-1] = temp[-1]
	del temp
	return [bin_pos, bin_limits, 2*half_step]

def base_binning(inf, sup, n_bin=None, delta_bin=None):
	&amp;quot;&amp;quot;&amp;quot;More C-like...
	&amp;quot;&amp;quot;&amp;quot;
	if (n_bin == 0) or (n_bin == 0):
		print &amp;quot;Error, n_bin and/or delta_bin are/is zero, exit!&amp;quot;
		sys.exit()    
	elif (n_bin == None) and (delta_bin != None):
		n_bin = int((sup-inf)/(1.*delta_bin))
	elif (n_bin != None) and (delta_bin == None):
		delta_bin = (sup-inf)/(1.*n_bin)
	elif (n_bin == None) and (delta_bin == None):
		print &amp;quot;Error, n_bin and delta_bin are both None, exit!&amp;quot;
		sys.exit()
	bin_pos = np.zeros(n_bin)
	bin_limits = np.zeros(n_bin+1)
	for i in range(n_bin):
		if i%2 == 0:
			bin_limits[i] = inf + i * delta_bin
			bin_limits[i+1] = bin_limits[i] + delta_bin
	bin_pos[i] = bin_limits[i] + delta_bin/2.
	bin_limits[n_bin] = inf + n_bin * delta_bin
	return [bin_pos, bin_limits, delta_bin]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>HDF5 in Python: PyTables</title>
      <link>http://brunettoziosi.eu/posts/hdf5-in-python-pytables/</link>
      <pubDate>Fri, 25 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/hdf5-in-python-pytables/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.hdfgroup.org/HDF5/&#34; target=&#34;_blank&#34; title=&#34;HDF5 Group homepage&#34;&gt;HDF5&lt;/a&gt; is a wonderful file format you can use to put into tons of data with easy, without the need to think about endianess, binary formats and so on.&lt;br /&gt;
Pytables is an extremely optimized library built on top of HDF5 capabilities to make even simpler the use of this type of file.&lt;br /&gt;
It&amp;rsquo;s also possible to navigate into a file graphically with &lt;a href=&#34;http://vitables.org/&#34; target=&#34;_blank&#34; title=&#34;ViTables homepage&#34;&gt;ViTables&lt;/a&gt;.&lt;br /&gt;
Here I would like to present some of the features I use more often.&lt;br /&gt;
&lt;!--TEASER_END--&gt;&lt;br /&gt;
Open, flush and close a file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tables as tb

h5 = tb.openfile(&amp;quot;filename.h5&amp;quot;, &#39;r&#39;)
...
h5.flush()
h5.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;&#39;r&#39;&lt;/code&gt; means &amp;ldquo;open the file in read-only mode&amp;rdquo;. It&amp;rsquo;s also possible to open it as &lt;code&gt;&#39;w&#39;&lt;/code&gt; (create a new file: it overwrites the file if it still exists) and &lt;code&gt;&#39;a&#39;&lt;/code&gt; (append: create if it does not exist, if it exists, read and modify it).&lt;/p&gt;

&lt;p&gt;Create a group to contain some data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;group = h5.createGroup(h5.root, &amp;quot;group&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;h5.root&lt;/code&gt; is the location of the new object created (where we want to create the group) and can also be passe as string (&amp;ldquo;/&amp;rdquo;) and &lt;code&gt;&amp;quot;group&amp;quot;&lt;/code&gt; is the string with the name.&lt;/p&gt;

&lt;p&gt;Store an array&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.array([....])
array = h5.createArray(h5.root.group, &#39;name&#39;, array_to_store, &#39;title&#39;)
array = h5.createArray(&amp;quot;/group&amp;quot;, &#39;name&#39;, array_to_store, &#39;title&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a table (from &lt;a href=&#34;create-recreate-and-remove-duplicates-in-array-manipulation-obviously-in-python.html&#34;&gt;Create, recreate and remove duplicates in array manipulation, obviously in Python!:)&lt;/a&gt; or &lt;a href=&#34;from-csv-to-hdf5-in-python.html&#34;&gt;From .csv to HDF5 in Python&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;f = open(&amp;quot;filename.csv&amp;quot;, &#39;r&#39;)
line = f.readline()
values = np.genfromtxt(StringIO(line), dtype=([(&#39;column_1&#39;, &#39;i8&#39;), (&#39;column_2&#39;, &#39;f4&#39;), (&#39;column_3&#39;, &#39;f4&#39;)]), delimiter=&#39;,&#39;)
values.shape = 1

# or in an equivalent way, if the file dimensions permit to lad the entire file:
# values = np.genfromtxt(&amp;quot;filename.csv&amp;quot;, dtype=([(&#39;column_1&#39;, &#39;i8&#39;), (&#39;column_2&#39;, &#39;f4&#39;), (&#39;column_3&#39;, # &#39;f4&#39;)]), delimiter=&#39;,&#39;)

h5 = tb.openFile(&#39;filename.h5&#39;, &#39;w&#39;)
table = h5.createTable(h5.root, description=values, name=&#39;table_name&#39;, title=&amp;quot;table_description&amp;quot;, expectedrows=12158536)
table.flush()

for line in f:
	values = np.genfromtxt(StringIO(line), dtype=([(&#39;column_1&#39;, &#39;i8&#39;), (&#39;column_2&#39;, &#39;f4&#39;), (&#39;column_3&#39;, &#39;f4&#39;)]), delimiter=&#39;,&#39;)
	values.shape = 1
	table.append(values)
	
table.flush()
h5.flush()
h5.close()
f.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s also possible to walk all the nodes under a group:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in h5.walkNodes(h5.root.group):
	print i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and to delete a node/array/table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5.removeNode(h5.root.group, &#39;name&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All the nodes are available through their path:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5.getNode(&amp;quot;/group&amp;quot;, &amp;quot;name&amp;quot;)
h5.root.group.name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to read a table or an array, you can use the function &lt;code&gt;read()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5.getNode(&amp;quot;/group&amp;quot;, &amp;quot;name&amp;quot;).read()
h5.root.group.name.read()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Create, recreate and remove duplicates in array manipulation, obviously in Python!:)</title>
      <link>http://brunettoziosi.eu/posts/create-recreate-and-remove-duplicates-in-array-manipulation-obviously-in-python/</link>
      <pubDate>Mon, 14 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/create-recreate-and-remove-duplicates-in-array-manipulation-obviously-in-python/</guid>
      <description>

&lt;p&gt;I would like to &amp;ldquo;pin&amp;rdquo; here a pair of quick solution to everyday problems I encounter manipulating arrays.&lt;/p&gt;

&lt;h2 id=&#34;create:90&#34;&gt;Create&lt;/h2&gt;

&lt;p&gt;First, the creation of a structured array (an array composed of records made by different data types) array from a file too big to be read at once with &lt;code&gt;np.genfromtxt&lt;/code&gt;. The new array will be stored in an HDF5 file, so this is a conversion from .csv to .h5 file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import tables as tb
from StringIO import StringIO

f = open(&amp;quot;filename.csv&amp;quot;, &#39;r&#39;)
line = f.readline()
values = np.genfromtxt(StringIO(line), dtype=([(&#39;column_1&#39;, &#39;i8&#39;), (&#39;column_2&#39;, &#39;f4&#39;), (&#39;column_3&#39;, &#39;f4&#39;)]), delimiter=&#39;,&#39;)
values.shape = 1
h5 = tb.openFile(&#39;filename.h5&#39;, &#39;w&#39;)
table = h5.createTable(h5.root, description=values, name=table_name&#39;, title=&amp;quot;table_description&amp;quot;, expectedrows=12158536)
table.flush()

for line in f:
	values = np.genfromtxt(StringIO(line), dtype=([(&#39;column_1&#39;, &#39;i8&#39;), (&#39;column_2&#39;, &#39;f4&#39;), (&#39;column_3&#39;, &#39;f4&#39;)]), delimiter=&#39;,&#39;)
	values.shape = 1
	table.append(values)
	
table.flush()
h5.flush()
h5.close()
f.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The use of &lt;code&gt;StringIO&lt;/code&gt; is necessary to convert the string containing the line read in a &amp;ldquo;I/O&amp;rdquo; object that &lt;code&gt;np.genfromtxt&lt;/code&gt; can eat.&lt;/p&gt;

&lt;h2 id=&#34;remove-duplicates:90&#34;&gt;Remove duplicates&lt;/h2&gt;

&lt;p&gt;Consider the previous file, if there are duplicates row, &lt;code&gt;np.unique&lt;/code&gt; can help in removing them. Note that we use the first column to identify the duplicates and that the result will be sorted respect to this column.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_indexes = np.unique(table[&#39;column_1&#39;], return_index=True, return_inverse=False)[1]
new_array = np.transpose(table[new_indexes])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In general, &lt;code&gt;numpy.unique(array, return_index=True, return_inverse=True)&lt;/code&gt;&lt;br /&gt;
returns an array sorted and without duplicates, the indexes of the original array to create the new array and the indexes of the new one to recreate the old one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;In: a = np.array([5,3,3,7,2,9,1])
In: np.unique(a, return_index=True, return_inverse=True)
Out: 
(array([1, 2, 3, 5, 7, 9]),
	array([6, 4, 1, 0, 3, 5]),
	array([3, 2, 2, 4, 1, 5, 0]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;recreate:90&#34;&gt;Recreate&lt;/h2&gt;

&lt;p&gt;Sometimes it&amp;rsquo;s useful to split a structured array in different arrays, manipulate them and recreate the structured array, or maybe you need to create a structured array from different arrays to fill a Pytables table.&lt;br /&gt;
To do this a possible solution is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;values = np.array(zip([column_1[0]], [column_2[0]]))
print &amp;quot;Creating table...&amp;quot;
table = h5.createTable(h5.root, description=values, name=&#39;fof_data_snap67&#39;, title=&amp;quot;fof_data_snap67&amp;quot;, expectedrows=11697806)

for i in xrange(1, fof.size):
	values = np.array(zip([column_1[i]], [column_2[i]]))
	table.append(values)

table.flush()
h5.flush()
h5.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s also possible to zip the entire arrays if they fit into memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;values = np.array(zip(column_1, column_2))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Parameter space parallel exploration in Python</title>
      <link>http://brunettoziosi.eu/posts/parameter-space-parallel-exploration-in-python/</link>
      <pubDate>Thu, 03 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/parameter-space-parallel-exploration-in-python/</guid>
      <description>&lt;p&gt;Today a friend of mine ask me how to quickly and easily parallelize a parameter space exploration in his code. &amp;ldquo;Quickly and easily&amp;rdquo; means &amp;ldquo;do not try to use &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/phd-question-3-monte-carlo-markov-chain/&#34; target=&#34;_blank&#34; title=&#34;PhD question #3: Monte Carlo Markov chain&#34;&gt;MCMC&lt;/a&gt; or something similar!!!&amp;ldquo;.&lt;/p&gt;

&lt;p&gt;I think a good solution could be use something like &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/python-parallel-job-manager/&#34; target=&#34;_blank&#34; title=&#34;Python parallel job manager&#34;&gt;this&lt;/a&gt; defining objects to contain parameters combinations and result, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    class parameter(object):
      &amp;quot;&amp;quot;&amp;quot;Object to handle all the information about a parameter combination.
      &amp;quot;&amp;quot;&amp;quot;
      def __init__(self, a = None, b = None, c= None, d = None):
        &amp;quot;&amp;quot;&amp;quot;Construct parameters object.&amp;quot;&amp;quot;&amp;quot;
        self.name = str(self.a)+&amp;quot;-&amp;quot;+str(self.b)+&amp;quot;-&amp;quot;+str(self.c)+&amp;quot;-&amp;quot;+str(self.d)
        self.a = None
        self.b = None
        self.c = None
        self.d = None
        self.res_a = None
        self.res_b = None
        self.res_c = None
      def __repr__(self):
        return &amp;quot;&amp;quot; % self.name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and filling the queue in this way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def fill_queue(input_queue):
      &amp;quot;&amp;quot;&amp;quot;Fill the queue&amp;quot;&amp;quot;&amp;quot;
      parameter_set = [xrange(a_limit),xrange(b_limit), xrange(c_limit), xrange(d_limit)]
      # Create all possible combination of the parameters values and from these 
      # generate and put in the queue the objects
      for i in itertools.product(\*l):
        input_queue.put(parameter(i[0], i[1], i[2], i[3]))
      return input_queue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think it&amp;rsquo;s not optimal for a huge amount of combinations, because of the huge amount of objects, but in this case you can change the code to use lists or arrays. Anyway it should be better than four nested loops!:P&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GIF2 files Python reader</title>
      <link>http://brunettoziosi.eu/posts/gif2-files-python-reader/</link>
      <pubDate>Tue, 06 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/gif2-files-python-reader/</guid>
      <description>&lt;p&gt;I create this script on the basis of the code to read the Millennium II data (the same used &lt;a href=&#34;http://elbrunz.wordpress.com/2011/12/02/from-binaries-to-hdf5-using-python/&#34; title=&#34;From binaries to HDF5 using Python&#34;&gt;here&lt;/a&gt;) provided by &lt;a href=&#34;http://mbk.ps.uci.edu/index.html&#34; title=&#34;Mike Boylan-Kolchin&#34;&gt;Mike Boylan-Kolchin&lt;/a&gt;. Being allowed to read the Fortran code to write and read the GIF2 files I could adapt this script to exactly fit this problem.&lt;/p&gt;

&lt;!-- TEASER_END--&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import sys

class head:
    def __init__(self, fname):
        import types
        bo_mark=&#39;&amp;gt;&#39;
        # start by reading in header:    
        if type(fname) is types.StringType:
            f=open(fname, &#39;rb&#39;)
        elif type(fname) is not types.FileType:
            raise TypeError(&#39;argument must either be an open file or &#39; + 
                            &#39;a string containing a file name&#39;)
        else:
            f=fname
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the usual imports we define a class to read and contain the file header. These first lines check the &amp;ldquo;filename&amp;rdquo; argument.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.pad = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the unformatted Fortran binary files the &amp;ldquo;pad&amp;rdquo; is a 4-byte space to separate the values of different quantities in the file. &lt;code&gt;bo_mark&lt;/code&gt; contains the endianess (&amp;ldquo;bo&amp;rdquo; means byte order) of the system allowing Python to correctly interpret the numbers from the binary file. &lt;code&gt;count&lt;/code&gt; sets the number of item of type &lt;code&gt;dtype&lt;/code&gt; to be read. For example &lt;code&gt;count=3&lt;/code&gt; and &lt;code&gt;dtype=bo_mark+&#39;i4&#39;&lt;/code&gt; will store three 4-byte integer in the variable, with the byte order expressed by &lt;code&gt;bo_mark&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt; for big-endian, &lt;code&gt;&amp;gt;&lt;/code&gt; for little-endian.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# npart is an array containing the number of particles in the
        # file divided by type (gas, ...)
        self.npart = np.fromfile(f, count=6, dtype=bo_mark+&#39;i4&#39;)
        self.massarr = np.fromfile(f, count=6, dtype=bo_mark+&#39;f8&#39;)
        self.aaa = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.redshift = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.flag_sfr = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.flag_feedback = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.nall = np.fromfile(f, count=6, dtype=bo_mark+&#39;i4&#39;)
        self.cooling_flag = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.numfiles = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.boxsize = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.Omega = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.OmegaL0 = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.Hubblepar = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.version = np.fromfile(f, count=1, dtype=bo_mark+&#39;a96&#39;)
        self.pad2=np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        if type(fname) is types.StringType: f.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These lines read and store the values of the quantities saved in the header of the file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def read_gif2_file(fname):
    bo_mark = &#39;&amp;gt;&#39;
    f=open(fname, &#39;rb&#39;)
    # start by reading in header:    
    ghead=head(f)
    npt=ghead.npart.sum()````
    
This is the function we use to read the files: it set the endianness to &amp;quot;big-endian&amp;quot;, open the file in read-only mode, read the header and extract the total number of particles.    
````python
f.seek(4, 1)
    pos=np.fromfile(f, count=npt*3, dtype=bo_mark + &#39;f4&#39;).reshape((npt, 3))
    return pos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;f.seek(offset, from_what)&lt;/code&gt; move the pointer through the file to read bits from one position to another. The position is computed from adding &lt;code&gt;offset&lt;/code&gt; to a reference point; the reference point is selected by the &lt;code&gt;from_what&lt;/code&gt; argument. A &lt;code&gt;from_what&lt;/code&gt; value of 0 measures from the beginning of the file, 1 uses the current file position, and 2 uses the end of the file as the reference point. &lt;code&gt;from_what&lt;/code&gt; can be omitted and defaults to 0, using the beginning of the file as the reference point (from the &lt;a href=&#34;http://docs.python.org/tutorial/inputoutput.html&#34; title=&#34;Python documentation&#34;&gt;Python documentation&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This last piece of code read the GIF2 galaxy catalogue (an ASCII file with &amp;ldquo;space separated values&amp;rdquo;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#! /usr/bin/env python

file = open(&#39;lcdm_galaxy_cat.z0.00&#39;, &#39;rb&#39;)
i = 0
for riga in file.readlines():
    parole = riga.split()
    if len(parole) == 11:
        print &amp;quot;iterazione &amp;quot;, i 
        print &amp;quot;aggiungo &amp;quot;
        x.append(parole[5])
        y.append(parole[6])
        z.append(parole[7])
    i+=1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>GIF2 substructures coordinates correction</title>
      <link>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</link>
      <pubDate>Sun, 04 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</guid>
      <description>&lt;p&gt;I used this script to change the coordinates of the substructures in the GIF2 simulation output from the center of mass coordinates to the global ones. The substructures were stored in our server in files referring to the index of the halo to which they belong and their coordinates were respect to the center of the halo. For each halo this script read the subhaloes center of mass coordinates in kpc and change them to global coordinates in Mpc managing the periodic boundary conditions .&lt;/p&gt;

&lt;!-- TEASER_END --&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import numpy as np
import tables as tb
import time
import sys

&amp;quot;&amp;quot;&amp;quot;Legge il file con id e centri degli aloni con sottostrutture, per
ogni alone (id) apre il file Sub.3..053.gv, legge le coordinate
alle colonne 8, 9, 10 (in kpc!!!) e le corregge (tenendo conto delle
condizioni periodiche) con le coordinate del centro prese dalla lista
degli aloni iniziale.  Le coordinate vengono aggiunte ad un vettore
che alla fine viene salvato in un file hdf5.
&amp;quot;&amp;quot;&amp;quot;

t = time.time()
print &amp;quot;Start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As usual: imports, documentation and timing initialization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;halo_centers_file = &#39;haloes_with_substructures_centers.h5&#39;

h5 = tb.openFile(halo_centers_file, &#39;r&#39;)
haloes = h5.root.data.read()
h5.close()

haloes = haloes.astype(float)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code reads the halo centers from a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;substructure_coord = np.empty((0, 3))
files_num = haloes[:, 0].shape[0]
void_files = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we create the first (void) record of the substructures coordinates table, find the number of haloes and create a variable which will tell us how many haloes without subtructures we have.&lt;/p&gt;

&lt;p&gt;Now, for each halo we&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in xrange(files_num):
    print &amp;quot;Loop &amp;quot;, i, &amp;quot; di &amp;quot;, files_num 
    sub_file = &amp;quot;cm/Sub.3.&amp;quot;+&#39;%07d&#39;%int(haloes[i,0])+&amp;quot;.053.gv&amp;quot;
    ````
    
open the related substructures file     
````python
try:
        sub_coord = np.genfromtxt(sub_file, dtype=&#39;float&#39;, usecols=(8, 9, 10))
        file_check = True
    except:
        print &amp;quot;Void file &amp;quot;, sub_file
        file_check = False
        void_files += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;try to read the substructures coordinates, if it fails, we assume that the file is empty (and the halo has no substructures). In this case we increment the counter of the empy files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if file_check:
        if sub_coord.ndim == 1:
            sub_coord = sub_coord.reshape((1, sub_coord.shape[0]))
            #print &amp;quot;sh min &amp;quot;, np.amin(sub_coord, 0)
            #print &amp;quot;sh min &amp;quot;, np.amax(sub_coord, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This reshape the array in case we have only one subhalo: in this case (I hope I remember correct!:P) instead of one row and three columns we should have three values, so we have to reshape the array to pass it to the rest of the code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
            sub_x = sub_coord[:, 0]/1000. + haloes[i,1]
            if not np.all(sub_x &amp;amp;gt; 0):
                sub_x[sub_x&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_x 110]-=110
            if not (np.all(sub_x &amp;amp;gt; 0) and np.all(sub_x  0)
                print sub_x 
                print haloes[i, 1:3]
                sys.exit()
            sub_y = sub_coord[:, 1]/1000. + haloes[i,2]
            if not np.all(sub_y &amp;amp;gt; 0):
                sub_y[sub_y&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_y 110]-=110
            if not (np.all(sub_y &amp;amp;gt; 0) and np.all(sub_y  0)
                print sub_y
                print haloes[i, 1:3]
                sys.exit()
            sub_z = sub_coord[:, 2]/1000. + haloes[i,3]
            if not np.all(sub_z &amp;amp;gt; 0):
                sub_z[sub_z&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_z 110]-=110
            if not (np.all(sub_z &amp;amp;gt; 0) and np.all(sub_z  0)
                print sub_z 
                print haloes[i, 1:3]
                sys.exit()
            substructure_coord = np.vstack((substructure_coord, np.hstack((sub_x.reshape((sub_x.shape[0], 1)), 
                                                                           sub_y.reshape((sub_y.shape[0], 1)), 
                                                                           sub_z.reshape((sub_z.shape[0], 1))))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This corrects the coordinates keeping in mind the periodic boundary conditions and add the new substructures coordinates to the corrected substructures array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;except:
            print &amp;quot;file &amp;quot;, sub_file
            print &amp;quot;sub_coord.shape &amp;quot;, sub_coord.shape
            print &amp;quot;sub_coord &amp;quot;, sub_coord
            print &amp;quot;haloes coord &amp;quot;, haloes[i, :]
            print &amp;quot;exit&amp;quot;
            sys.exit()
    else:
        pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If something goes wrong, we handle the failure printing some information.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5 = tb.openFile(&#39;sub_haloes_global_coords.h5&#39;, &#39;w&#39;)
h5.createArray(h5.root, &#39;data&#39;, substructure_coord)
h5.flush()
h5.close()
print &amp;quot;Done in &amp;quot;, time.time()-t, &amp;quot; with &amp;quot;, void_files, &amp;quot; void files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, we save the array in an HDF5 file!:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From .csv to HDF5 in Python</title>
      <link>http://brunettoziosi.eu/posts/from-csv-to-hdf5-in-python/</link>
      <pubDate>Fri, 02 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/from-csv-to-hdf5-in-python/</guid>
      <description>&lt;p&gt;PyTables is a Python library that provide a simple but really useful interface to manage the HDF5 files with some other interesting features (compression, optimizations, &amp;hellip;). To the library presentation and documentation, for now refers, to the &lt;a href=&#34;http://www.pytables.org/moin&#34; target=&#34;_blank&#34; title=&#34;site&#34;&gt;site&lt;/a&gt;.&lt;br /&gt;
I used it a lot during my master thesis to manage the dataset from the Millennium database.&lt;br /&gt;
Here I provide a brief review of how I used it to store data obtained in .csv (comma separated values) format.&lt;/p&gt;

&lt;!-- TEASER_END --&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python

import numpy as np
import tables as tb
import time

t = time.time()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As usual, we have the initial import of the modules we need and start the timing of the code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fofhdf5 = tb.openFile(&#39;mill2_fof_snap67.h5&#39;, &#39;w&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This create the HDF5-file object we use to work on the file, in write (&lt;code&gt;&#39;w&#39;&lt;/code&gt;) mode. Read-only (&lt;code&gt;&#39;r&#39;&lt;/code&gt;) mode is also possible.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fof_data = np.genfromtxt(&#39;fof0.csv&#39;, dtype=([(&#39;fofId&#39;, &#39;i8&#39;), (&#39;np&#39;, &#39;i4&#39;), (&#39;mass&#39;, &#39;f4&#39;), (&#39;x&#39;, &#39;f4&#39;), (&#39;y&#39;, &#39;f4&#39;), (&#39;z&#39;, &#39;f4&#39;), (&#39;ix&#39;, &#39;i4&#39;), (&#39;iy&#39;, &#39;i4&#39;), (&#39;iz&#39;, &#39;i4&#39;), (&#39;m_crit_200&#39;, &#39;f4&#39;), (&#39;r_crit_200&#39;, &#39;f4&#39;), (&#39;m_mean_200&#39;, &#39;f4&#39;), (&#39;r_meam_200&#39;, &#39;f4&#39;), (&#39;m_tophat&#39;, &#39;f4&#39;), (&#39;r_tophat&#39;, &#39;f4&#39;), (&#39;numSubs&#39;, &#39;i4&#39;)]), comments=&#39;#&#39;, delimiter=&#39;,&#39;, skiprows=26)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we read the .csv/ASCII table and from this we create a table of numpy arrays, each of them with its own name and type. It&amp;rsquo;s also possible to specify the character for the comments in the file (&lt;code&gt;#&lt;/code&gt;), the character separating the values (commas, spaces, &amp;hellip;) and the number of line to be skipped (file header).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table = fofhdf5.createTable(fofhdf5.root, description=fof_data, name=&#39;fof_data_snap67&#39;, title=&amp;quot;fof_data_snap67&amp;quot;, expectedrows=11697806)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;create the HDF5-table with the proper hierarchy, some metadata (description and title). Specify the number of rows one expects to put into the table helps the library to optimize the operations and the space.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 20):
    fof_data = np.genfromtxt(&#39;fof&#39;+str(i)+&#39;.csv&#39;, dtype=([(&#39;fofId&#39;, &#39;i8&#39;), (&#39;np&#39;, &#39;i4&#39;), (&#39;mass&#39;, &#39;f4&#39;), (&#39;x&#39;, &#39;f4&#39;), (&#39;y&#39;, &#39;f4&#39;), (&#39;z&#39;, &#39;f4&#39;), (&#39;ix&#39;, &#39;i4&#39;), (&#39;iy&#39;, &#39;i4&#39;), (&#39;iz&#39;, &#39;i4&#39;), (&#39;m_crit_200&#39;, &#39;f4&#39;), (&#39;r_crit_200&#39;, &#39;f4&#39;), (&#39;m_mean_200&#39;, &#39;f4&#39;), (&#39;r_meam_200&#39;, &#39;f4&#39;), (&#39;m_tophat&#39;, &#39;f4&#39;), (&#39;r_tophat&#39;, &#39;f4&#39;), (&#39;numSubs&#39;, &#39;i4&#39;)]), comments=&#39;#&#39;, delimiter=&#39;,&#39;, skiprows=26)
    
    table.append(fof_data)
    table.flush()
    print &amp;quot;Loop &amp;quot;, i, &amp;quot; done.&amp;quot;

fofhdf5.close()
print &amp;quot;Done in &amp;quot;, time.time()-t, &amp;quot;seconds !!!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;for&lt;/code&gt; loop opens other ASCII tables and append them to the existing HDF5-table. The &lt;code&gt;table.flush()&lt;/code&gt; command let the library physically write the data on the disk instead of maintaining them in memory and write them periodically. After that we close the file object.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From binaries to HDF5 using Python</title>
      <link>http://brunettoziosi.eu/posts/from-binaries-to-hdf5-using-python/</link>
      <pubDate>Fri, 02 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/from-binaries-to-hdf5-using-python/</guid>
      <description>&lt;p&gt;I have used this script to convert the Millennium II data from the unformatted fortran binary formato to the DF5 one.&lt;br /&gt;
The core of the script is a module (&lt;code&gt;modified_read_snapshots&lt;/code&gt;) built on the basis of a script kindly provided by &lt;a href=&#34;http://mbk.ps.uci.edu/index.html&#34; target=&#34;_blank&#34; title=&#34;Mike Boylan-Kolchin&#34;&gt;Mike Boylan-Kolchin&lt;/a&gt; from the group that perform the Millennium II simulation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python    

import time
import kd3hdf5
import tables as tb
import modified_read_snapshots as rs

t = time.time()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The usual imports and time initialization!:P&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bin2hdf5(bin_file, h5_file, tree = False):
    snap = rs.read_snapshot(bin_file)
    h5f = kd3hdf5.KDTree(h5_file, &#39;w&#39;)
    h5f.data_store(snap[&#39;pos&#39;])
    if tree == True:
        h5f.tree_build
    h5f.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function accept as arguments the name of the binary file, the name of the HDF5 file to be created and give the user the possibility to create a KDTree with the data. By default it won&amp;rsquo;t create this tree. If no KDTree must be created the function only uses the part of the &lt;code&gt;kd3hdf5&lt;/code&gt; module that store the data into the HDF5 file. We will have a brief view of this at the end of the post.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def main():
    print &amp;quot;start&amp;quot;
    for i in [0, 10, 100, 200, 511]:
        t2 = time.time()
        print &amp;quot;Loop &amp;quot;, i
        t3 = time.time()
        bin2hdf5(&#39;../binary/snap_newMillen_subidorder_067.&#39;+str(i), &#39;../hdf5/data_&#39;+str(i))
        print &amp;quot;Loop &amp;quot;, i, &amp;quot; finished in &amp;quot;, time.time()-t2

    print &amp;quot;That&#39;s all folks, in &amp;quot;, time.time()-t, &amp;quot;!!!&amp;quot;

if __name__ == &amp;quot;__main__&amp;quot;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nothing more than calling the previous function on the data files looping on their names!:)&lt;br /&gt;
Respect to the other posts here we make use of the &lt;code&gt;main&lt;/code&gt; function but is nothing extraordinary!:P&lt;/p&gt;

&lt;p&gt;The code from the &lt;code&gt;kd3hdf5&lt;/code&gt; module is&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class KDTree(object):
    #Docs [...]

    def __init__(self, filename, mode):
        if mode == &#39;read&#39; or mode == &#39;r&#39;:
            self.h5file = tb.openFile(filename, mode = &amp;quot;r&amp;quot;)
        elif mode == &#39;append&#39; or mode == &#39;a&#39;:
            self.h5file = tb.openFile(filename, mode = &amp;quot;a&amp;quot;)
        elif mode == &#39;build&#39; or mode == &#39;w&#39; or mode == &#39;write&#39;:
            self.h5file = tb.openFile(filename, mode = &amp;quot;w&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This provide the creation of the object, linked to an HDF5 file and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def data_store(self, data):
        t = time.time()
        self.h5file.createArray(self.h5file.root, &#39;data&#39;, np.asarray(data), title=&#39;data&#39;)
        self.h5file.root.data._v_attrs.n_elements = data.shape[0]
        self.h5file.root.data._v_attrs.m_dimensions = data.shape[1]
        self.h5file.root.data._v_attrs.maxes = np.amax(data,axis=0)   # maxes and mins for each coord
        self.h5file.root.data._v_attrs.mins = np.amin(data,axis=0)
        print self.h5file.root.data._v_attrs.n_elements, &amp;quot; Stored in &amp;quot;, time.time()-t, &amp;quot;seconds.&amp;quot;
        t = time.time()
        self.h5file.root.data.flush()
        print time.time()-t, &amp;quot; seconds to commit changes.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;fill the file with the data and some metadata (table dimension and maxes and mins of the data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python slicing, rebinning and indexing</title>
      <link>http://brunettoziosi.eu/posts/python-slicing-rebinning-and-indexing/</link>
      <pubDate>Fri, 25 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/python-slicing-rebinning-and-indexing/</guid>
      <description>&lt;p&gt;During my master thesis I had to manage a lot of (different) data from GIF and GIF2 projects, Millimillennium, Millenium and Millennium 2 simulations and so on. Sometimes there were the need to sort, divide or rearrange these dataset.&lt;br /&gt;
Here some examples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sort the Millennium 2 data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I developed this script to sort the Millennium 2 data in the $x$ coordinate. The dataset was composed of 512 hdf5 files and I would like to create 1000 files of 100 kpc/h each, taking particles for all the original files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/bin/env python
import time
import numpy as np
import tables as tb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the usual modules importing. &lt;code&gt;tables&lt;/code&gt; is the module provided by PyTables to manage, in a great way, hdf5 file under Python.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_glob = time.time()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Timing is always good!!!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# This create the first &amp;quot;5 Mpc/h&amp;quot; file, every loop operate on 100 kpc/h of data 
for i in range(0,50):
    t = time.time()
    i_dist = i/10.0
    j_dist = i_dist + 0.1
    print &amp;quot;Starting with limits [Mpc/h]&amp;quot;, i_dist, j_dist
    temp2 = np.array([])
    temp2.shape = (0,3)
    # This loop open each of the 512 original files
    # selecting the particles inside the limits
    # and stacking them into the array that will 
    # be saved in the new file
    for j in range(0,512):
        filename = &#39;../../hdf5/data_&#39;+str(j)
        print &amp;quot;Open &amp;quot;, filename
        h5 = tb.openFile(filename, &#39;r&#39;)
        original = h5.root.data.read()
        h5.close()
        temp = original[original[:,0]&amp;amp;gt;i_dist]
        if temp.size &amp;amp;lt; 3:
            temp3 = np.array([])
            temp3.shape=(0,3)
        else:
            temp3 = temp[temp[:,0]&amp;amp;lt;=j_dist]
        try:
            temp2 = np.vstack((temp2,temp3))
        except:
            print &amp;quot;Error in stacking&amp;quot;
            print &amp;quot;temp2 dimensions &amp;quot;, temp2.shape
            print &amp;quot;temp3 dimensions &amp;quot;, temp3.shape
            exit()
    tt=time.time()

    # Sorting the array in the x direction
    temp2[temp2[:,0].argsort(),]
    print &amp;quot;Time for argsort &amp;quot;, time.time()-tt

    # Writing data to file
    destname = &#39;../../hdf5_sorted/mill2sort-&#39;+str(i)
    print &amp;quot;Writing &amp;quot;, destname
    dest = tb.openFile(destname, &#39;w&#39;)
    dest.createArray(dest.root, &#39;data&#39;, temp2, title=&#39;data&#39;)
    dest.flush()
    dest.close()
    print &amp;quot;Time for loop  &amp;quot;, i, &amp;quot; is &amp;quot;, time.time()-t
print &amp;quot;Done in &amp;quot;,time.time()-t_glob, &amp;quot;seconds&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Rebin the sorted Millennium 2 data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After I have sorted the data, we need to rebin the dataset because it doesn&amp;rsquo;t fit into our machine memory.&lt;br /&gt;
This script make use of the &lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/python-parallel-job-manager.html&#34; target=&#34;_blank&#34; title=&#34;Python parallel job manager&#34;&gt; parallel job manager&lt;/a&gt; to break the 1000 files of the sorted dataset into 5000 files each containing 20 kpc/h of data, sorted in the $x$ direction.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#! /usr/bin/env python

import sys, os
import numpy as np
import tables as tb
from subprocess import Popen, PIPE
from multiprocessing import Process, Queue


#############################################
sub_slice_dim = 0.02    #Mpc/h
sub_slice_num = 5 # for each of the original slice
n_procs = 5
tot_mill_slice = 999   # number of original slices
############################################


def slicing(i, j, sub_slice_dim, sub_slice_num):
    &amp;quot;&amp;quot;&amp;quot;Given a mill2 slice it creates 5 subslices.
    &amp;quot;&amp;quot;&amp;quot;
    start_dim = j\*sub_slice_dim
    stop_dim = start_dim + sub_slice_dim
    print &amp;quot;Loop &amp;quot;, j, &amp;quot; of &amp;quot;, sub_slice_num , &amp;quot; between &amp;quot;, start_dim, &amp;quot; and &amp;quot;, stop_dim, &amp;quot; in slice &amp;quot;, i 
    pos_greater = mill_slice[start_dim &amp;amp;lt; mill_slice[:,0]]
    pos = pos_greater[pos_greater[:,0] &amp;amp;lt;= stop_dim]
    h5=tb.openFile(&#39;/home/ziosi/mill2_data/hdf5_sorted_rebinned/mill2sort-&#39;+str(i*5+j)+&#39;.h5&#39;, &#39;w&#39;)
    h5.createArray(h5.root, &#39;data&#39;, pos, title=&#39;mill2_sorted_rebinned_pos&#39;)
    h5.flush()
    h5.close()

def breakmill(in_queue):
    &amp;quot;&amp;quot;&amp;quot;Bring a slice from the queue and start the function to
    sub_slice it.
    &amp;quot;&amp;quot;&amp;quot;
    while in_queue.qsize != 0:
        ii = in_queue.get()
        i = ii[0]
        sub_slice_dim = ii[1]
        sub_slice_num = ii[2]
        original_slice = &amp;quot;/home/ziosi/mill2_data/hdf5_sorted/mill2sort-&amp;quot;+str(i)
        mill_slice = tb.openFile(original_slice, &#39;r&#39;)
        slice_data = mill_slice.root.data.read()
        slice_min = np.amin(slice_data[:,0])
        for j in xrange(5):
            slicing(i, j, sub_slice, slice_dim, sub_slice_num)
    
# Create the queues.
in_queue = Queue()
out_queue = Queue()

# Fill the input queue.
try:
    for i in xrange(tot_mill_slice):
        in_queue.put([i, sub_slice_dim, sub_slice_num])
except:
    print &amp;quot;Input queue not filled, exit!&amp;quot;
    sys.exit(1)

procs = []

# Create the processes.
try:
    for i in range(n_procs):
        print &amp;quot;Process creation loop &amp;quot;, i
        procs.append(Process(target=breakmill, args=(in_queue, out_queue)))
except:
    print &amp;quot;Creating processes not complete, exit...&amp;quot;
    sys.exit(1)

# Start the processes.
try:
    for i in procs:
        i.start()
except:
    print &amp;quot;Start processes not complete, exit...&amp;quot;;
    sys.exit(1)

# Check for processes status.
for i in procs:
    print &amp;quot;Process &amp;quot;, i,&amp;quot; @ &amp;quot; , i.pid, &amp;quot; is &amp;quot;, status(i)

print &amp;quot;Done.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Indexing data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We also need to index a data file to can retrieve only a slice from the entire file. We did it using the following three elements:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;flags = np.arange(0, 110, 0.1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is the array containing the markers for the distances we want to index. This means that if we want to index distances from 0 to 10 every 1, we get: 0,1,2,3,4,5,6,7,8,9,10, so we can select, for example, the element between 0 and 1 without load the entire list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;start = indexes[np.maximum(np.searchsorted(indexes[:,0], xstart, side=&#39;left&#39;)-1, 0), 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;contains the index of the cell in the array that contains the first element we want&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stop = indexes[np.minimum(np.searchsorted(indexes[:+1], xstop, side=&#39;right&#39;)+1, indexes.shape[0]-1), 2]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;contains the index of the cell in the array that contains the first element we want.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;indexes = np.hstack((flags.reshape((flags.shape[0], 1)), start.reshape((start.shape[0], 1)), end.reshape((end.shape[0], 1))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is the $(n,3)$ array with the flag, the start and the stop indexes. It is saved into the data file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python CLI and configuration file parser</title>
      <link>http://brunettoziosi.eu/posts/python-cli-and-configuration-file-parser/</link>
      <pubDate>Thu, 17 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/python-cli-and-configuration-file-parser/</guid>
      <description>&lt;p&gt;One of the first things I needed writing the code for my thesis was the ability to read options and parameters both from a configuration file and from the command line. After some attempts I have found (at &lt;a href=&#34;http://www.decalage.info/&#34; target=&#34;_blank&#34; title=&#34;http://www.decalage.info&#34;&gt;&lt;a href=&#34;http://www.decalage.info&#34;&gt;http://www.decalage.info&lt;/a&gt;&lt;/a&gt;) a file parser to read a configuration file and the Python library &lt;a href=&#34;http://docs.python.org/dev/library/argparse.html&#34; target=&#34;_blank&#34; title=&#34;argparse&#34;&gt;argparse&lt;/a&gt; for the command line parsing. In addition I have modified the file parser and I&amp;rsquo;ve added a &amp;ldquo;variable container&amp;rdquo; object, inspired by some snippets found somewhere on the web.&lt;br /&gt;
&lt;!-- TEASER_END --&gt;
I chose to first parse the configuration file and after the command line options: if it&amp;rsquo;s necessary the command line options will overwrite the file options.&lt;br /&gt;
Let&amp;rsquo;s consider the code, fully commented.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import argparse
def parse_config(filename):    
    &amp;quot;&amp;quot;&amp;quot;Read the config file and store the variables into a dictionary.
    Thanks to http://www.decalage.info
    &amp;quot;&amp;quot;&amp;quot;
    mlogger.info(&amp;quot;Reading config file.&amp;quot;)

    COMMENT_CHAR = &#39;#&#39;
    OPTION_CHAR =  &#39;=&#39;

    options = {}
    f = open(filename)
    for line in f:
        # First, remove comments:
        if COMMENT_CHAR in line:
            # split on comment char, keep only the part before
            line, comment = line.split(COMMENT_CHAR, 1)
        # Second, find lines with an option=value:
        if OPTION_CHAR in line:
            # split on option char:
            option, value = line.split(OPTION_CHAR, 1)
            # strip spaces:
            option = option.strip()
            value = value.strip()
            # store in dictionary:
            options[option] = value
    f.close()
    return options
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this is the container for the variables, also clearly commented.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Bunch(object):
    &amp;quot;&amp;quot;&amp;quot;Create a dictionary object containing all the initial variables.
    &amp;quot;&amp;quot;&amp;quot;

    def __init__(self, d=None):
        if d is not None: self.__dict__.update(d)

def var(filename):
    &amp;quot;&amp;quot;&amp;quot;Read the initial variables and return a dictionary object.

    Parameters
    ==========

    filename = string, the name of the config file

    Returns
    =======

    Bunch(locals()) = dictionary object containing the local variables

    &amp;quot;&amp;quot;&amp;quot;

    # Read the config file and create a dictionary.
    options = parse_config(filename)

    # Common variables.
    mlogger.info(&amp;quot;Defining common parameters.&amp;quot;)

    parameter_1 = options[&#39;parameter_1&#39;]
    parameter_2 = options[&#39;parameter_2&#39;]
    parameter_3 = None
    option_1 = False

    return Bunch(locals())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the commented code for the command line parsing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read the configuration file.
    v = mod.var(&#39;../config.txt&#39;)

    # Check if there are CLI arguments, if yes start parsing.
    if argv is None:
        # Check for CLI variables.
        argv = sys.argv

        # Create the parser object.
        parser = argparse.ArgumentParser()

        # Create the parser entry.
        parser.add_argument(&#39;-p1&#39;, &#39;-parameter_1&#39;, &#39;--parameter_1&#39;, action=&#39;store&#39;, dest=&#39;parameter_1&#39;, default=v.parameter_1, 
                    help=&#39;Parameter 1 description&#39;)
        parser.add_argument(&#39;-p2&#39;, &#39;-parameter_2&#39;, &#39;--oparameter_2&#39;, action=&#39;store&#39;, dest=&#39;parameter_2&#39;, default=v.parameter_2, 
                    help=&#39;Parameter 2 description&#39;)
        parser.add_argument(&#39;-o&#39;, &#39;--option&#39;, action=&#39;store_true&#39;, dest=&#39;option&#39;, default=None, 
                    help=&#39;Oprion description&#39;)

        # Add the parsed variables to a container object
        cli = parser.parse_args()

        # Overwrite the config file parameters
        v.parameter_1 = cli.parameter_1
        v.parameter_2 = cli.parameter_2
        v.option = cli.option

    elif isinstance(argv, dict):
        # Reading variables passed to Main as a function (Guido docet
        # http://www.artima.com/weblogs/viewpost.jsp?thread=4829).
        for i in argv.keys():
            if i in (&amp;quot;-p1&amp;quot;, &amp;quot;--parameter_1&amp;quot;): 
                v.parameter_1 = argv[i]
            elif i in (&amp;quot;-p2&amp;quot;, &amp;quot;--parameter_2&amp;quot;): 
                v.parameter_2 = argv[i]
            elif i in (&amp;quot;-o&amp;quot;, &amp;quot;--option&amp;quot;):
                v.option = True
            else:
                print &amp;quot;Wrong parameter passed to main function, exit!!!&amp;quot;
                sys.exit(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The configuration file look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Short description of parameter 1
parameter_1 = value

# Short description of parameter 2
parameter_2 = None
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Python logging</title>
      <link>http://brunettoziosi.eu/posts/python-logging/</link>
      <pubDate>Thu, 17 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/python-logging/</guid>
      <description>&lt;p&gt;After I had a &lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/python-parallel-job-manager.html&#34; target=&#34;_blank&#34; title=&#34;Python parallel job manager&#34;&gt;Python parallel job&amp;nbsp;manager&lt;/a&gt; I realize that all the attempts I have done to log what happen in my code weren&amp;rsquo;t satisfying. It was not comfortable to manage every output when I want to change something and it was impossible to switch off some of logs without changing the code. The Python logging library is a great piece of code that permits to personalize most of the aspect of the logging in a program but maintaining a standard and comfortable interface. It also allows an easy management of many different outputs (file, screen, &amp;hellip;) and different levels of logging (error, info, &amp;hellip;) indipendent one from each other. It also allows to handle the logging of the imported modules.&lt;/p&gt;

&lt;p&gt;Here how I have used it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
logger = logging.getLogger(&amp;quot;Main_log&amp;quot;)
logging.captureWarnings(True)
logger.setLevel(logging.DEBUG)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the first line we import the module and tell it to handle also warnings coming from the Python interpreter or from other libraries. After that we set the minimum level of logging. In this way, if we want, for example, only the errors to be logged, every log below this level will be silenced.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create file handler which logs even debug messages.
fh = logging.FileHandler(v.log_file+&amp;quot;.log&amp;quot;, &#39;w&#39;)
fh.setLevel(logging.DEBUG)
# Create formatter.
formatter = logging.Formatter(&amp;quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&amp;quot;)
fh.setFormatter(formatter)
# Add the handlers to the logger.
logger.addHandler(fh)
if v.console is True:
	# Create console handler.
	ch = logging.StreamHandler()
	ch.setLevel(logging.DEBUG)
	ch.setFormatter(formatter)
	logger.addHandler(ch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we create the object that will handle the log file, choose the level of logging and create an object that describe the format of the logs and apply it to the log handler. We also offer the possibility to have a console log handler.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Start logging.
logger.info(&amp;quot;Log started.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We start the logger and record the first log with log level &amp;ldquo;info&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mlogger = logging.getLogger(&amp;quot;Main_log.modules&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This create the logger for one of the imported modules.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python parallel job manager</title>
      <link>http://brunettoziosi.eu/posts/python-parallel-job-manager/</link>
      <pubDate>Fri, 04 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/python-parallel-job-manager/</guid>
      <description>&lt;p&gt;The final version of the code for my master thesis was the most embarrassing parallel code you can think&amp;hellip; just a serial code to be run on different slices of the dataset. I choose this solution because it permits to manage the different resources (memory, processors, &amp;hellip;) on the different machines available without any restriction. Moreover, this solution has no communication between the processes, with better performances and all the processes are independent, so it minimize the damages due to any failure.&lt;/p&gt;

&lt;p&gt;At this step, however, I didn&amp;rsquo;t know how to manage the different processes in a comfortable way.
My requirements were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to start only one process that will take care of starting the right code on the right data-slice&lt;/li&gt;
&lt;li&gt;the possibility to start &amp;ldquo;n&amp;rdquo; processes depending on the number of processors and memory available (in the actual code this is done by hand)&lt;/li&gt;
&lt;li&gt;the code should be able to start a new process when one of the previous processes end&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hereafter I expose and comment the complete &amp;ldquo;template&amp;rdquo; for this Python code as I wrote it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/bin/env python

import sys, os
from subprocess import Popen, PIPE
from multiprocessing import Process, Queue

&amp;quot;&amp;quot;&amp;quot;This script starts n_procs processes that in parallel take the
number of the data file from a common queue and with a loop apply the
analysys code to the data starting it with a bash command using Popen
&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first line is the declaration of the interpreter to be used for this script, in this case Python. After that we import some libraries and modules used in this code. The last lines, inside the triple quotes, are the documentation string of the code. Python in fact has a self-doc system that can be used to understand what a piece of code does and how it works.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n_procs = 10 # number of processes to be started
file_1 = None
file_2 =  &amp;quot;mill2_fof_sorted.h5&amp;quot;
m_factor = 1    # How many random more than data
start_slice = 0 
end_slice = 99
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we set some parameters: the number of processes to start (set by hand), the two files to use in the analysis, how many random particles we use more than the data particles and the limits in the data slice to analyze. In this case we want to correlate the data in &lt;code&gt;mill2_fof_sorted.h5&lt;/code&gt; with the data contained in the slices between slice 0 and slice 99. file_1 will be replaced after with the right name. This analysis will be carry out using 10 processes at each time. As a process end the code will take care of starting a new process.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def starter(input, output):
	&amp;quot;&amp;quot;&amp;quot;Start the code processes one after the other&amp;quot;&amp;quot;&amp;quot;
	while input.qsize() != 0:
	item = input.get()
	file_1 = &amp;quot;mill2sort-&amp;quot;+str(item[0])       
	cmd = &amp;quot;/opt/epd-7.0-2-rh5-x86_64/bin/python -u \
			../serial.py --file_1 &amp;quot;+file_1+&amp;quot; --file_2 &amp;quot;+file_2+\
		&amp;quot; -l 400 -n 0 --m_factor &amp;quot;+str(m_factor)+&amp;quot; \
			--slicing --log ../logs/&amp;quot;+file_1+&amp;quot;-&amp;quot;+file_2

	try:
		pid = os.getpid()
		pid_cmd = &#39;echo &amp;quot;&#39;+str(item[0])+&#39;&amp;quot; &amp;amp;gt;&amp;amp;gt; &#39;+str(pid)+&#39;.log&#39;
		os.system(pid_cmd)
		p = Popen(cmd, shell=True, close_fds=True).wait()

	except:
		print &amp;quot;Popen/os.system not done, exit...&amp;quot;
		sys.exit()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code defines the function that will start the processes. It&amp;rsquo;s called &lt;code&gt;started&lt;/code&gt; and it takes &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; as arguments. These are two &amp;ldquo;queue objects&amp;rdquo;, they can be filled and emptied in the FIFO way. The starter has a while loop that check if the queue is empty, if not it takes the next elements. This is the number of the slice to be used by the analysis code and with it we build the name of the file to be opened. &lt;code&gt;cmd&lt;/code&gt; is the string we use to start the analysis code with some options (&lt;code&gt;serial.py&lt;/code&gt; is the actual &amp;ldquo;cool&amp;rdquo; name I gave to my code!:P).&lt;br /&gt;
The &lt;code&gt;try-exept&lt;/code&gt; syntax is the particular Python way to manage the possible errors in the execution, giving the ability to the programmer to handle possible problems (exceptions).&lt;br /&gt;
So we catch the pid of the starter and save into its log the number of sliced it start and pass to the &lt;code&gt;Popen&lt;/code&gt; (process-open) command the string to start the analysis process telling it to wait the end of the process. If something goes wrong we print that there were some errors and exit the code in a clean way.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fill_queue(task_queue, start_slice, end_slice):
	&amp;quot;&amp;quot;&amp;quot;Fill the queue&amp;quot;&amp;quot;&amp;quot;
	for i in range(start_slice, end_slice, 1):
		task_queue.put([i])
	return task_queue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This functions only fill the queue with the number of the sliced to be used.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def status(proc):
	&amp;quot;&amp;quot;&amp;quot;Check for processes status&amp;quot;&amp;quot;&amp;quot;
	if proc.is_alive()==True:
		return &#39;alive&#39;
	elif proc.is_alive()==False:
		return &#39;dead&#39;
	else:
		return proc.is_alive()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code check the status (dead or alive) of one process (&lt;code&gt;proc&lt;/code&gt; is the process object&amp;hellip; yeah, in Python everything is an object!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_queue = Queue()
output_queue = Queue()

try:
    input_queue = fill_queue(input_queue, start_slice, end_slice)
except:
    print &amp;quot;Queue not filled, exit...&amp;quot;
    sys.exit()

procs = []    # processes container
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we create the empty queues and (try to) fill them, and create the container for the processes objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    for i in range(n_procs):
        print &amp;quot;Process loop &amp;quot;, i
        procs.append(Process(target=starter, args=(input_queue, output_queue)))
except:
    print &amp;quot;Creating processes not complete, exit...&amp;quot;
    sys.exit()

try:
    for i in procs:
        i.start()
except:
    print &amp;quot;Start processes not complete, exit...&amp;quot;
    sys.exit()

for i in procs:
    print &amp;quot;Process &amp;quot;, i,&amp;quot; @ &amp;quot; , i.pid, &amp;quot; is &amp;quot;, status(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the central part of this code: we create the processes objects and put them into the container, start them and check for their status. Everything is inside the &lt;code&gt;try-except&lt;/code&gt; environment to check for possible errors and handle them.&lt;br /&gt;
In practice we start &amp;ldquo;n&amp;rdquo; processes and every process take the number of a slice from the queue and use it to start the analysis code, waiting for its end. When the analysis is finished it takes another number from the queue and start again the code. When the queue is empty everything is automatically switched off.&lt;br /&gt;
Future improvements will consider the automatically detection of the hardware resources and the possibility to mail the status of the code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>