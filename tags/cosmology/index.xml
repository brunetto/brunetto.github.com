<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cosmology on Post It!</title>
    <link>http://brunettoziosi.eu/tags/cosmology/</link>
    <description>Recent content in Cosmology on Post It!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 May 2012 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://brunettoziosi.eu/tags/cosmology/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Learning not to hate C: things worth nothing</title>
      <link>http://brunettoziosi.eu/posts/learning-not-to-hate-c-things-worth-nothing/</link>
      <pubDate>Mon, 21 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/learning-not-to-hate-c-things-worth-nothing/</guid>
      <description>

&lt;p&gt;Here I would like to post some of the ideas, tricks, truths, and so on I&amp;rsquo;ve learned during the two-days course at the computing center. Two intense days of base C-programming full immersion, far beyond the flat and boring (and unuseful) orthodoxy you can find in a book.&lt;/p&gt;

&lt;h2 id=&#34;header-files:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Header files&lt;/h2&gt;

&lt;p&gt;They are one of the first annoying things I see in C/C++ languages. Two files for each &amp;ldquo;module&amp;rdquo;, two lines to be modified every times, &amp;hellip; but I have eventually obtained a reasonable justification for their existence. When you distribute your code, or download someone&amp;rsquo;s code, you can share or download compiled code+header files and not all the sources.&lt;/p&gt;

&lt;h2 id=&#34;compile:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Compile&lt;/h2&gt;

&lt;p&gt;The easiest way to compile the sources and get the executable is&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;gcc pippo.c -o pippo.x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;pippo.c&lt;/code&gt; is the source(s) (you can have &lt;code&gt;pippo1.c&lt;/code&gt;, &lt;code&gt;pippo2.c&lt;/code&gt; etc, but only one &lt;code&gt;main&lt;/code&gt; function) file.&lt;br /&gt;
A better way is to include some flags&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;gcc -std=gnu99 -Wall -pedantic -Werror -O0 -g -o pippo.x pippo.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-std=gnu99&lt;/code&gt; forces the compiler to the C99 standard (it includes some new features)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-Wall&lt;/code&gt; to print all the compiler warnings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-pedantic&lt;/code&gt; &amp;ldquo;Issue all the warnings demanded by strict ISO C and ISO C++; reject all programs that use forbidden extensions, and some other programs that do not follow ISO C and ISO C++&amp;rdquo; (from the &lt;code&gt;gcc&lt;/code&gt; manual)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-Werror&lt;/code&gt; transform the warnings into errors&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-O0&lt;/code&gt; no optimization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-g&lt;/code&gt; produces debugging informations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-o&lt;/code&gt; to specify the output&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;macros-preprocessor:572a5990ff18de3fef742f6ac648edcf&#34;&gt;MACROS &amp;amp; preprocessor&lt;/h2&gt;

&lt;p&gt;The preprocessor is a sort of automated text editor. It cut and paste piece of text in the sources files according to some simple rules. With it you can define &amp;ldquo;constant&amp;rdquo; values or simple functions (danger, can lead to unpredicted effects) across the whole source and make easy to change it&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#define PI 3.14
float a = PI;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but you can also check for already defined quantities, compile pieces of code (&amp;ldquo;conditional compilation&amp;rdquo;) or implement a &amp;ldquo;header-guard&amp;rdquo; to avoid multiple imports of the same header&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#define CONDITION
#ifdef CONDITION
....
#else
....
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#ifndef HEADER_H
#define HEADER_H
.....
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compiling with the &lt;code&gt;-E&lt;/code&gt; flag you can view the result of the preprocessor action.&lt;/p&gt;

&lt;h2 id=&#34;numbers:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Numbers&lt;/h2&gt;

&lt;p&gt;Number variables are similar to those in other programming languages, but there&amp;rsquo;s some peculiarities.&lt;br /&gt;
Integer dimension depends on the machine and on the compiler, on the contrary floats and doubles are fixed by an international standard. For the rest you can find detailed description on every C books.&lt;/p&gt;

&lt;h2 id=&#34;cast:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Cast&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s possible to convert one type to some other type. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;float pippo;
pippo = (float) k*1024L;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;bool:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Bool&lt;/h2&gt;

&lt;p&gt;There aren&amp;rsquo;t boolean variables, but 0 stands for &lt;code&gt;False&lt;/code&gt; and any other non-zero value means &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;side-effect-and-short-circuit:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Side effect and short-circuit&lt;/h2&gt;

&lt;p&gt;Something like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;i = k + 10 - ++k;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;has indefinite result because it&amp;rsquo;s not predictable the order of evaluation of operand/expression &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;++k&lt;/code&gt;, and those expressions have &lt;em&gt;side effects&lt;/em&gt; on the other expression involved in the result.&lt;br /&gt;
Logical operators &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; and &lt;code&gt;||&lt;/code&gt; instead are evaluated strictly from left to right. This is c&lt;em&gt;alled short-circuit evaluation&lt;/em&gt; and the operators are synchronization points, just like &amp;ldquo;;&amp;rdquo;. At a synchronization point all the code before has to be already evaluated and the code after has not yet.&lt;/p&gt;

&lt;h2 id=&#34;ternary-operator:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Ternary operator&lt;/h2&gt;

&lt;p&gt;The ternary operator accept three arguments and has the form of&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;result = a &amp;amp;gt; b ? x : y;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means &lt;code&gt;[expression_to_evaluate] ? [if true] : [if false]&lt;/code&gt;. Obviously you can nest as many expression and operator as you want.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variables qualifiers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In C there&amp;rsquo;are some qualifier that can be added to the type specification. Three of them are &lt;code&gt;const&lt;/code&gt;, &lt;code&gt;extern&lt;/code&gt; and &lt;code&gt;static&lt;/code&gt;.&lt;br /&gt;
The first can be added in the function declaration to tell the compiler that the values has not to be changed by the function, for example&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;int pippo(const int pluto, const int* paperino);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;means that the integer &lt;code&gt;pluto&lt;/code&gt; and the pointer to integer &lt;code&gt;paperino&lt;/code&gt; can not be changed by the function &lt;code&gt;pippo&lt;/code&gt;.&lt;br /&gt;
&amp;ldquo;extern&amp;rdquo; specifies that the variable is a global variable, coming from outside the function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;int pippo;
int main(...){
	extern int pippo;
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;ldquo;static&amp;rdquo; has different meanings depending on where you put it.&lt;br /&gt;
Before a global variable means that the variable is private of the file, and other files can not use it. Inside a function means that the variable survive after the end of the function (when the stack frame - the stack memory associated to the function - is deleted) and is available to the succeeding calls to the same function. Static is also used before a function to specify that the function is private of the file and can not be called outside:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;static int i;
static void foo(void){
	extern int i;
	static int j
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt; is global inside the file but can not be called outside&lt;/li&gt;
&lt;li&gt;the function &lt;code&gt;foo&lt;/code&gt; is private of the file and can not be called outside&lt;/li&gt;
&lt;li&gt;&lt;code&gt;j&lt;/code&gt; survive after the termination of the function and maintains its value until the next call.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Pointers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pointers are special integer values that contains addresses of other values, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;int i;
int* p;
p = &amp;amp;amp;i;
*p = 13;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;first we define the integer &lt;code&gt;i&lt;/code&gt;, then a pointer &lt;code&gt;p&lt;/code&gt; to a integer value (here &lt;code&gt;*&lt;/code&gt; means &amp;ldquo;the argument on the right is a pointer to a value of type defined by the argument on the left&amp;rdquo;), then we put the address of &lt;code&gt;i&lt;/code&gt; in &lt;code&gt;p&lt;/code&gt; (&lt;code&gt;&amp;amp;amp;&lt;/code&gt; means &amp;ldquo;give me the address of the right argument&amp;rdquo;) and at the end we modify the value of &lt;code&gt;i&lt;/code&gt; dereferencing &lt;code&gt;p&lt;/code&gt;, that is modifying the memory cell pointed by the address contained in p (here &lt;code&gt;*&lt;/code&gt; means &amp;ldquo;follow the address contained in the argument on the right&amp;rdquo;).&lt;br /&gt;
It&amp;rsquo;s possible to define pointers to pointers to pointers etc. adding &amp;ldquo;*&amp;ldquo;. It exists the null pointer, that is an invalid pointer, and follow it result in a &amp;ldquo;segmentation fault&amp;rdquo; that means &amp;ldquo;you are trying to access memory you are not allowed to access&amp;rdquo;. A pointer defined but not filled point to a random location in memory, so following it results in an indefinite result, probably a segmentation fault or the modification of another variable.&lt;br /&gt;
Because the dereferencing operator has not the highest precedence, sometimes you will have to put parenthesis to specify the order of dereferencing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;/*Filling a &amp;quot;heap&amp;quot; array using pointers*/
(*ptr)[i] = (float*) malloc(sizeof(float)*lenghtx);

/*In struct, the two forms are equivalent*/
(*pippo).pluto
pippo-&amp;amp;gt;pluto
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;stack-and-heap:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Stack and heap&lt;/h2&gt;

&lt;p&gt;The stack is the automatic memory used for the function calls, their local variables and the automatic arrays. It&amp;rsquo;s managed automatically. The heap is the memory available to the user to define his variables. This has to be managed by the user so he has to deallocate the allocated memory and to keep track of the pointers (address). If you loose the pointer to some allocated memory this is lost until the end of the program and the memory can saturate. This is known as &amp;ldquo;&lt;em&gt;memory leak&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;arrays:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Arrays&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;are basically two ways to create an array. First, you can create an automatic array in the stack memory, managed by the compiler. For this reason automatic arrays must be small and have to be defined at compile time, that means that you have to know and declare the dimension, for example;&lt;/p&gt;

&lt;pre&gt;float array[10];&lt;/pre&gt;
    

&lt;p&gt;You can access the elements of the array in the natural way &lt;code&gt;array[3] = 10&lt;/code&gt;.&lt;br /&gt;
As a matter of fact, because an array it&amp;rsquo;s a masked pointer, all of these are equivalent&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;*V = 10;
V[0] = 10;
*(V+0) = 10;
0[V] = 10;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The operator &amp;ldquo;&lt;code&gt;[]&lt;/code&gt;&amp;rdquo; is equivalent to dereferencing the address contained into the pointer on the left shifted by the quantity inside the operator, and to shift an address is equivalent to sum it to the shift because addresses are integer values and the memory occupied by an array is contiguous.&lt;/p&gt;

&lt;p&gt;The second type of array is defined by the user and manually allocated (and manually deallocated) in the heap memory with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;void* malloc(size_t size);
void free(void* heapBlockPointer);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In practice, the following is an example of how to implement two functions that create and destroy a two dimensional array given a pointer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#include 
#include 
#include &amp;quot;array_management.h&amp;quot;

/* Function definition, takes as argument the address of a pointer to pointer */
int array_creation(float*** ptr, int lenghtx, int lenghty){
	printf(&amp;quot;Create array of pointers...n&amp;quot;);

	/* In the memory cell of the pointer put the address of the first 
		element of an array of pointers to float and allocate the necessary space */
	*ptr = (float**) malloc(sizeof(float*)*lenghty); 

	/* Check that the allocation worked fine */
	if(*ptr==NULL){
	return -1;
	}
	printf(&amp;quot;Create arrays...n&amp;quot;);

	/* Allocate memory for an array of float for each pointer and put the address 
		of the first element in the respective cell of the array of pointers */
	for(int i=0; i&amp;amp;lt;lenghty; i++){
	(*ptr)[i] = (float*) malloc(sizeof(float)*lenghtx);
	if( (*ptr)[i] == NULL){
		return -1;
	}
	}
	return 0;
}

/* Function to deallocate the memory */
void array_destruction(float** ptr, int lenghty){
	printf(&amp;quot;Destructing array...n&amp;quot;);
	for(int i=0; i&amp;amp;lt;lenghty; i++){
	free(ptr[i]);
	}
	free(ptr); 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In both types of array there isn&amp;rsquo;t a control on the indexes, it&amp;rsquo;s up to the user not to read or write beyond the array limits.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also important to keep in mind that arrays are contiguous memory blocks, and a multidimensional array is &amp;ldquo;linearized&amp;rdquo; row-major order so it&amp;rsquo;s faster to &amp;ldquo;run&amp;rdquo; on the row index (that on the right-most one). This is known as &lt;a href=&#34;http://en.wikipedia.org/wiki/Stride_of_an_array&#34; target=&#34;_blank&#34;&gt;stride one access&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;structs:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Structs&lt;/h2&gt;

&lt;p&gt;An array is a structure containing homogeneous data, if you need to put together different types of data you can use structs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;struct point {
int x;
int y;}; /* Don&#39;t forget this semicolon!*/

struct point pippo;
pippo.x = 12;
pippo.y = 15;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s also possible to use &lt;code&gt;typedef&lt;/code&gt; to symplify the sintax:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;typedef struct treenode* Tree;
struct treenode {
int data;
Tree smaller, larger; /* equivalently, this line could say
};                       &amp;quot;struct treenode *smaller, *larger&amp;quot; */  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(From Cineca&amp;rsquo;s slides)&lt;/p&gt;

&lt;h2 id=&#34;strings:572a5990ff18de3fef742f6ac648edcf&#34;&gt;Strings&lt;/h2&gt;

&lt;p&gt;Strings are simply arrays of characters, usually terminated with the &amp;ldquo;null&amp;rdquo; character &amp;ldquo;&lt;code&gt;&amp;quot; to be able to use the string manipulation libraries (without &amp;quot;&lt;/code&gt;&amp;rdquo; you should give the length of the string to the library, but they are not designed to do this).&lt;br /&gt;
Because of this, the two expression that follows are equivalent:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;void strcpy(char *s, char *t) {
	while ((*s = *t) != ‘’) {
	s++;
	t++;
	}
}
void strcpy(char s[], char *t) {
	/* s arr, t ptr */
	int i = 0;
	while ((*s = t[i]) != ‘’) {
	s++; /* s ptr */
	i++; /* t arr */
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(From Cineca&amp;rsquo;s slides)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assignement evaluation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Because the assignment operator &amp;ldquo;=&amp;rdquo; return the value assigned, it&amp;rsquo;s possible to use this in expression evaluation to shorten the code, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;void strcpy(char *s, char *t){while((*s++==*t++));}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is equivalent to the expressions in the previous paragraph.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD students&#39; Journal Clubs and Sozi</title>
      <link>http://brunettoziosi.eu/posts/phd-students-journal-clubs-and-sozi/</link>
      <pubDate>Mon, 07 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-students-journal-clubs-and-sozi/</guid>
      <description>&lt;p&gt;Some times ago I have been intrigued by &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/infographics-attempts/&#34; title=&#34;Infographics attempts!&#34;&gt;infographics&lt;/a&gt; and different ways of communicating. The first software that catchad my attention was &lt;a href=&#34;http://prezi.com/&#34; target=&#34;_blank&#34; title=&#34;Prezi Homepage&#34;&gt;Prezi&lt;/a&gt; but its technology (Flash) and the fact that it&amp;rsquo;s no opensource and has a lot of limitations let me move to &lt;a href=&#34;http://sozi.baierouge.fr/wiki/en:welcome&#34; target=&#34;_blank&#34; title=&#34;Sozi Homepage&#34;&gt;Sozi&lt;/a&gt;, an &lt;a href=&#34;http://inkscape.org/&#34; target=&#34;_blank&#34; title=&#34;Inkscape Homepage&#34;&gt;Inkscape&lt;/a&gt; extension.&lt;br /&gt;
I&amp;rsquo;ve tried it in making my presentation for the PhD students Journal Club, that is a meeting for PhD students and other researchers and professors taking place every Monday. It&amp;rsquo;s purpose is to let us practice on making lectures and to be up to date with recent development in other research areas.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve also ported my first JC presentation (that on my PhD project) to Inkscape but loosing part of the beauty of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Beamer_(LaTeX)&#34; target=&#34;_blank&#34;&gt;Beamer&lt;/a&gt; original theme.&lt;/p&gt;

&lt;p&gt;These are the two presentations. Right click and &amp;ldquo;Open in new tab&amp;rdquo; will let you able to view the presentations, navigatin with left and right arrows. Mid-click to view the summary and up/down arrow to navigate without any effect.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../../files/first_JC_with_proposal.svg&#34;&gt;PhD project presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../files/2012-05-07-Atacama_telescope_constrain_on_primordial_power_spectrum.svg&#34;&gt;JC on the ACT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #4: calculate the value of M*</title>
      <link>http://brunettoziosi.eu/posts/phd-question-4-calculate-the-value-of-m/</link>
      <pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-4-calculate-the-value-of-m/</guid>
      <description>

&lt;p&gt;Some post ago &lt;a href=&#34;phd-question-1-m.html&#34;&gt;I&amp;rsquo;ve written about M*&lt;/a&gt;, the typical non-linear mass collapsing at the redshift we are considering. Now I have to find a value for it.&lt;/p&gt;

&lt;p&gt;I said that &lt;code&gt;$M^*$&lt;/code&gt; is the typical mass of a perturbation that, at the time we are looking, has the associated liner density contrast &lt;code&gt;$\delta(\mathbf{x})\sim1$, or, in the formalism of the excursion set, pass the barrier of &amp;amp;nbsp;$\delta_c=1.686$.     
This means that we are looking for a perturbation with&lt;/code&gt;$\sigma\simeq1.686$` and trying to quantify the mass it contains.&lt;/p&gt;

&lt;h2 id=&#34;sigma-and-r:fc25179ded4233c0e9f8a261bf2b1314&#34;&gt;&lt;code&gt;$\sigma$&lt;/code&gt; and R&lt;/h2&gt;

&lt;p&gt;First of all we need to find the radius of a perturbation whose &lt;code&gt;$\sigma$&lt;/code&gt; reached the value of 1.686. To do this we can use the &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/the-initial-conditions-saga/&#34; target=&#34;_blank&#34; title=&#34;The “Initial Conditions” saga&#34;&gt;code&lt;/a&gt; developed to manage the CAMB files in order to find the matter power spectrum and its normalization. Then we add few lines to &amp;ldquo;sample&amp;rdquo; the &lt;code&gt;$\sigma(R)$&lt;/code&gt; distribution and find the radius of the perturbation reaching the excursion set barrier for the collapse.&lt;/p&gt;

&lt;h2 id=&#34;m:fc25179ded4233c0e9f8a261bf2b1314&#34;&gt;&lt;code&gt;$M^*$&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Once we have the radius for which &lt;code&gt;$\sigma = \delta_c$&lt;/code&gt; we need to know the mean density in the universe to find &lt;code&gt;$M^*$&lt;/code&gt; with:&lt;/p&gt;

&lt;p&gt;$M^* = \frac{4}{3}\pi R_*^3\rho_m$`&lt;/p&gt;

&lt;p&gt;&lt;del&gt;I don&amp;rsquo;t know why we only need to use &lt;code&gt;$\rho_m$&lt;/code&gt; and not &lt;code&gt;$\rho_m\delta$&lt;/code&gt; or something similar is not clear to me, but it&amp;rsquo;s correct.&lt;/del&gt;We use this formula because &lt;code&gt;$M^*$&lt;/code&gt; is a quantity related to the linear perturbations. It&amp;rsquo;s correct because the difference between a linear and a non-linear perturbation is the value of the density contrast, but the mass is the same. In other words, the mass of a perturbation is the same both in the linear and in the non-linear evolution, but linear perturbations have smaller density contrasts and larger radii, non-linear perturbations instead have larger density contrasts and smaller radii. To be precise, the previous equation can be written:&lt;br /&gt;
$M^* = \frac{4}{3}\pi R&lt;em&gt;*^3\rho&lt;/em&gt;{bg}(1+1.686) = \frac{4}{3}\pi R&lt;em&gt;{vir}^3\rho&lt;/em&gt;{bg}(1+200)$.&lt;br /&gt;
To obtain &lt;code&gt;$\rho_m$&lt;/code&gt; we find the value of the critical density &lt;code&gt;$\rho_c$&lt;/code&gt; and multiply it for `$\Omega = \rho_m / \rho_c$. These two values can be obtained from books (&lt;a href=&#34;http://www.amazon.com/Cosmology-Prof-Peter-Coles/dp/0471489093/ref=ntt_at_ep_dpt_2&#34; target=&#34;_blank&#34; title=&#34;Coles &amp;amp; Lucchin&#34;&gt;Lucchin&lt;/a&gt;, &lt;a href=&#34;http://www.amazon.com/Galaxy-Formation-Evolution-Houjun-Mo/dp/0521857937/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1333458377&amp;amp;sr=1-1&#34; target=&#34;_blank&#34; title=&#34;Mo, van den Bosch &amp;amp; White&#34;&gt;Mo&amp;amp;White&lt;/a&gt; for example) or in the &lt;a href=&#34;http://lambda.gsfc.nasa.gov/product/map/dr2/params/lcdm_wmap.cfm&#34; target=&#34;_blank&#34; title=&#34;WMAP data page&#34;&gt;WMAP data page&lt;/a&gt;. In the second case we prefer to use the single data fit because it&amp;rsquo;s simpler to refer to it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import time
import numpy as np
import matplotlib.pyplot as plt
import random as rnd
from scipy import integrate

&amp;quot;&amp;quot;&amp;quot; Calculate M* . M* is propto the mass contained in the radius for which 
s_8=delta_c refer to: 
http://www.brunettoziosi.eu/blog/wordpress/the-initial-conditions-saga/
http://www.brunettoziosi.eu/blog/wordpress/phd-question-3-calculate-the-value-of-m/
&amp;quot;&amp;quot;&amp;quot;

t = time.time()

#===============================================================================
#    Compute sigma
#===============================================================================

### Load data from the nasa-CAMB file 

# matterpower is the file with the k and the total matter spectrum
# transfer is the file with the k and the transfer function for the various 
#species, the 6th column is for the total matter (baryons+DM)
transfer = np.genfromtxt(&#39;camb_88704620_transfer_out_z0.dat&#39;, usecols = (0,6))
#matterpower = np.genfromtxt(&#39;2012-01-30_data/camb_88704620_matterpower_z0.dat&#39;)

# CAMB CDM transfer output
camb_k = transfer[:,0]
camb_tf = transfer[:,1] 

R = 8 #Mpc/h
s_8 = 0.9#0.8118405 #from WMAP7 but we need the values for the Millennium-2, so 
# we use its s_8
sp_ind = 1
delta_c = 1.686

### Calculate the amplitude to normalize the spectrum:
### P(k) = Ak^nT^2(k) 

# FT of the window function (spherical top-hat)
def FTW(R, k): 
	&amp;quot;&amp;quot;&amp;quot; Return the Fourier transform of the window function 
	(spherical top-hat)
	&amp;quot;&amp;quot;&amp;quot;
	return 3.*(np.sin(k*R)-k*R*np.cos(k*R)) / (k*R)**3

def spectrum():
	&amp;quot;&amp;quot;&amp;quot;Calculate the power spectrum given the transfer function and the FT of the window 
	function.
	&amp;quot;&amp;quot;&amp;quot;
	# camb_k**(2+sp_ind) that is k^(2+n) because d^3k=4pi k^2dk
	amp_integrand = camb_k**(2+sp_ind)*camb_tf**2 * FTW(R, camb_k)**2
	amp_integral = integrate.trapz(amp_integrand, camb_k)
	# Amplitude for s_8 = 1
	amp_0 = 2*np.pi**2/amp_integral
	# Amplitude
	amp = amp_0*s_8**2  # 9.9197881817e-09
	#print amp
	# Calculate the power spectrum
	return camb_k**sp_ind*camb_tf**2 * amp

# Calculate the power spectrum
ps = spectrum()

# Calculate sigma on the radii
def sigma(R):
	&amp;quot;&amp;quot;&amp;quot;Return the sigma for the current radius.
	&amp;quot;&amp;quot;&amp;quot;
	return pow(integrate.trapz(camb_k**2 * ps * FTW(R, camb_k)**2, camb_k)/(2*np.pi**2), 0.5)

#===============================================================================
#    Find the radius containing M*
#===============================================================================

# Initialize some variables
neigh = np.ones(2) # two nearest neighbours sigmas
r_min = 10**(-2) # min r to sample
r_max = 10**2 # max r ti sample
i = 0 # loop counter

# While stops when the computed sigma is less then 0.001 from delta_c
while np.abs(np.amin(delta_c+neigh)) &amp;amp;gt; 0.001:
print &amp;quot;Loop &amp;quot;, i
i+=1
print &amp;quot;Condition start &amp;quot;, np.abs(np.amin(delta_c+neigh))
# radii to be sampled
r = np.linspace(r_min, r_max, num=100)
# Compute sigma for those radii, the minus sign is to avoid resorting of the
# array to be used by np.searchsorted
s_r = -np.asarray(map(sigma, r))
# Find the two nearest neighbours
neigh[0] = np.amax(s_r[s_r  -delta_c])
# Find the corresponding radii
r_min = r[np.searchsorted(s_r, neigh).min()]
r_max = r[np.searchsorted(s_r, neigh).max()]
print &amp;quot;Sigmas&amp;quot;, -neigh[0], -neigh[1]
print &amp;quot;Radii [Mpc/h] &amp;quot;, r_min, r_max
print &amp;quot;Condition end &amp;quot;, np.abs(np.amin(delta_c+neigh))

# Selected values
s_star = neigh[np.argmin(delta_c+neigh)]
r_star = r[np.searchsorted(s_r, s_star)]
deviation = np.abs(np.amin(delta_c+neigh))

print &amp;quot;############################################&amp;quot;

print &amp;quot;Selected sigma &amp;quot;, -s_star
print &amp;quot;Selected radius [Mpc/h] &amp;quot;, r_star

print &amp;quot;Calculate M* using:&amp;quot;
print &amp;quot;Gt4.299 x 10^(-9) Mpc /M_sun (km/s)^2tfrom Mo&amp;amp;amp;White&amp;quot;
print &amp;quot;Ht100*h^2&amp;quot;
print &amp;quot;Omega_mt0.25tfrom the Millennium-2 simulation&amp;quot;
print &amp;quot;Omega_mt0.241tfrom WMAP7&amp;quot;

#===============================================================================
#    Cosmological parameters and find the mean density in the Universe
#===============================================================================

H = 100
h = 0.732 #WMAP http://lambda.gsfc.nasa.gov/product/map/dr2/params/lcdm_wmap.cfm
G = 4.299*10**(-9)
omega_m_mill = 0.25
omega_m_WMAP = 0.241

# Until here it&#39;s correct
rho_c = 3*H**2/(8*np.pi*G) # 2.7766040316101764 * h**2 x 10^11 M_sun/Mpc^3
	# 2.778 from Lucchin book
	# 2.775 from Mo&amp;amp;amp;White book

rho_mean_mill = rho_c * omega_m_mill # 6.94151007903 * h**2 x 10^10 M_sun/Mpc^3 = 3.71942769658 x 10^10 M_sun/Mpc^3
	rho_mean_WMAP = rho_c * omega_m_WMAP # 6.69161571618 * h**2 x 10^10 M_sun/Mpc^3 = 3.58552829951 x 10^10 M_sun/Mpc^3

print &amp;quot;rho_c = 3H^2/8 pi G = &amp;quot;, rho_c,&amp;quot; h^2 M_sun/Mpc^3&amp;quot;
print &amp;quot;Millennium-2 rho_mean = omega_m_mill * rho_c = &amp;quot;, rho_mean_mill, &amp;quot; h^2 M_sun/Mpc^3 = &amp;quot;, rho_mean_mill*h**2
print &amp;quot;WMAP7 rho_mean = omega_m_WMAP * rho_c = &amp;quot;, rho_mean_WMAP, &amp;quot; h^2 M_sun/Mpc^3 = &amp;quot;, rho_mean_WMAP*h**2 

#===============================================================================
#    Compute M*
#===============================================================================

M_star_mill = np.pi * r_star**3 * rho_mean_mill * 4./3#* (delta_c + 1)
M_star_WMAP = np.pi * r_star**3 * rho_mean_WMAP * 4./3#* (delta_c + 1)

print &amp;quot;M* Millennium-2 &amp;quot;, M_star_mill, &amp;quot; M_sun/h&amp;quot; # 4.81467115575e+12 M_sun/h
print &amp;quot;M* WMAP &amp;quot;, M_star_WMAP, &amp;quot; M_sun/h&amp;quot; # 4.64134299414e+12 M_sun/h

#===============================================================================
#    Compare with Hayashi&amp;amp;amp;White 2008 article
#===============================================================================

print &amp;quot;Hayashi&amp;amp;amp;White&#39;s value:&amp;quot;

M_star_white = 6.15*10**(12)
omega_m_white = 3.*M_star_white/(4*np.pi*r_star**3*rho_c)

print &amp;quot;M*: &amp;quot;, M_star_white
print &amp;quot;Omega_m&amp;quot;, omega_m_white

r_s = pow(3.*M_star_white/(4*np.pi*rho_mean_mill), 1./3)
s_white = sigma(r_s)

print &amp;quot;As alternative:&amp;quot;
print &amp;quot;R* &amp;quot;, r_s
print &amp;quot;Sigma &amp;quot;, s_white

#===============================================================================
#    Summary
#===============================================================================

print &amp;quot;&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;SUMMARY&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;&amp;quot;
print &amp;quot;WhottttM*ttR*ttOmegattSigmattSigma-delta_c&amp;quot;
print &amp;quot;--------------------------------------------------------------------------------------------------------------&amp;quot;
print &amp;quot;Hayashi&amp;amp;amp;White given Rtt{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_white,r_star,omega_m_white,-s_star,deviation)
print &amp;quot;Hayashi&amp;amp;amp;White given Omegat{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_white,r_s,omega_m_mill,s_white,np.abs(s_white-delta_c))
print &#39;Me WMAP datattt{:e}t{:e}t{:e}t{:e}t{:e}&#39;.format(M_star_WMAP,r_star,omega_m_WMAP,-s_star,deviation)
print &amp;quot;Me Millennium-2 datatt{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_mill,r_star,omega_m_mill,-s_star,deviation)
print &amp;quot;&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;&amp;quot;
print &amp;quot;Done in &amp;quot;, time.time()-t
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The differences between the different values of &lt;code&gt;$M^*$&lt;/code&gt; are acceptable and probably depends on different integration boundaries for `$\sigma$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #9: Gadget-2 (N-body part)</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</link>
      <pubDate>Mon, 20 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</guid>
      <description>&lt;p&gt;Here I would like to do a brief presentation of the main features of Gadget-2.&lt;br /&gt;
Gadget-2 (&lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; target=&#34;_blank&#34; title=&#34;Gadget2 homepage&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/my-first-gadget2-tests/&#34; target=&#34;_blank&#34; title=&#34;My first Gadget-2 tests&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2005.09655.x/abstract;jsessionid=DED86CDB5CD8A572F3631F0C42828086.d01t03&#34; target=&#34;_blank&#34; title=&#34;Gadget-2 paper&#34;&gt;here&lt;/a&gt;) is a cosmological simulation code developed primarily by &lt;a href=&#34;http://www.mpa-garching.mpg.de/~volker/&#34; target=&#34;_blank&#34; title=&#34;Volker Springel&#39;s homepage&#34;&gt;Volker Springel&lt;/a&gt;. It is a &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/cosmological-simulations-3-calculating-the-force/&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #3: force calculation!&#34;&gt;TreePM&lt;/a&gt; code so it splits forces between long-range (PM part) and short-range (tree part using multipole expansion to approximate the force of distant particles groups).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The tree&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gadget-2 uses a&lt;a href=&#34;http://en.wikipedia.org/wiki/Octree&#34; target=&#34;_blank&#34; title=&#34;oct-tree&#34;&gt; BH oct-tree&lt;/a&gt; (see also &lt;a href=&#34;http://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation&#34; target=&#34;_blank&#34; title=&#34;Barnes&amp;amp;Hut simulation on wikipedia&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.artcompsci.org/~makino/softwares/C++tree/index.html&#34; target=&#34;_blank&#34; title=&#34;NBODY, an implementation of Barnes-Hut treecode&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://ifa.hawaii.edu/~barnes/software.html&#34; target=&#34;_blank&#34; title=&#34;Barnes&#39; page&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.cita.utoronto.ca/~dubinski/treecode/treecode.html&#34; target=&#34;_blank&#34; title=&#34;A parallel tree code explenation&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.prism.gatech.edu/~gth716h/BNtree/&#34; target=&#34;_blank&#34; title=&#34;Barnes-Hut Implementation in HTML/Javascript&#34;&gt;here&lt;/a&gt;) to calculate the short-range forces in the real space. This choice was done because this type of tree, compared to other types (KD-Tree, &amp;hellip;), requires the creation of less nodes, that imply that less memory is used. It&amp;rsquo;s characterized by eight sub-nodes for each node and has only one particle in each leaf. The code decides to open a leaf according to a certain leaf opening criterion based on the estimated force error. The force for distant groups of particles is approximated with the multipole (here octopole) of the tree node and the error depends on the dimensions and the distances of the node considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PM part&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The PM part of the code is used to calculate the long-range forces. The algorithm is something like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CIC (cloud-in-cell) assignment is used to construct the mass density field on to the mesh from the information on the particles&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the discrete FT of the mesh is multiplied for the Green function for the potential in periodic boundaries (modified with the exponential truncation for the force splitting)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;deconvolution for the CIC kernel twice: the first for the smoothing effect of CIC assignment, the second for the force interpolation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$\mathrm{FT}^{-1}$ to obtain the potential on the mesh&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;finite differentiate the potential to obtain the forces&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;interpolate the forces to the particles positions using CIC&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: real-to-complex FT are used to save times and memory respect to full complex transforms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time step&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This type of code has a large dynamic range in time scale, from the denser regions where the evolution is rapid to the less denser regions in which the evolution occur slower so we can describe it with larger time resolution. In this scenario evolving all particles with the smallest time-scale is a waste of time and computational resources. Because using different time-steps for each particle add instabilities to the system, Gadget-2 separates time-step between long-range (longer time step) and short-range (shorter time step) force computations. The perturbation of the system for different time-steps is related to the symplectic nature of the system, but I still have not understood what it really means and implies! I know that it refers to the phase space volume and has effect on the information conservation. May be in the future I&amp;rsquo;ll write a post about this!&lt;br /&gt;
Despite these arguments, sometimes individual time step are allowed because they perturb the system but not the symplecticity of the single particle.&lt;br /&gt;
In the normal integration mode time-steps are discretized in a power of two hierarchy and particles can always move to smaller time steps but to longer time steps only in subsequent step, synchronized with higher time-steps. Alternatively the code can populate time-steps discretizeing them as integer multiples of the minimum time-step among the particles set. This lead to a more homogeneous distribution of particles across the time-line which can simplify work load balancing.&lt;br /&gt;
The integration is performed using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Leapfrog_integration&#34; target=&#34;_blank&#34; title=&#34;Leapfrog method&#34;&gt;leapfrog method&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually the parallelization distributes particles across the CPUs using an orthogonal domain decomposition but in this way the trees built-in each domain depend on the domain geometry. Because the force depend on the tree (through the multipole expansion of the mass distribution) the force can be different if you change the number of processors.&lt;br /&gt;
Gadget-2 introduce a space-filling fractal, the Peano-Hilber (PH) curve to map 3D space into a 1D curve that encompasses all the particles. Now the PH curve can be cut and each piece assigned to a CPU and in this way the force is independent of the processors number. If you cut every segment in eight pieces recursively you find again the tree decomposition, so there is a close correspondence between the decomposition obtained with the BH oct-tree and that of the PH curve.&lt;br /&gt;
The PH curve has some remarkably properties, for example points that are close along the 1D PH curve are in general close in 3D space, so the mapping preserves locality and if we cut the PH curve into segments of a certain length we obtain a domain decomposition which has the property that the spatial domains are simply connected and quite &amp;ldquo;compact&amp;rdquo; (i.e., they tend to have small surface-to-volume ratios and low aspect ratio, a highly desirable property for reducing communication costs with neighbouring domains and for speeding up the local computation).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Operations scheme&lt;/strong&gt;&lt;br /&gt;
Here a brief scheme on how the short range force calculation works on multiple processors. The PM computation uses the &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_4.html&#34; target=&#34;_blank&#34; title=&#34;Parallel FFTWs&#34;&gt;parallel FFTWs&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Compute the PH key for each particle&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sort the keys locally and split the PH curve into segments&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adjust the sorted segments to a global sort, splitting and joining segments if needed, with little communication&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Assign the particles to the processes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Construct a BH tree for the particles of each processors representing particles on other processors with pseudo-particles (acting like placeholders)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During the tree traverse (e.g. in processor A) these pseudo-particles cannot be opened, the are flagged and inserted into a list that collects all the particles that are to be sent (=requested) to the other processors (e.g. to processor B)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Processor B traverse again its local tree and send back the resulting force contribution to processor A&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The initial condition saga</title>
      <link>http://brunettoziosi.eu/posts/the-initial-condition-saga/</link>
      <pubDate>Tue, 31 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/the-initial-condition-saga/</guid>
      <description>

&lt;p&gt;If reading the previous posts on N-body simulations you have though that initial conditions are a little and easy task, you were wrong! And me with you!&lt;br /&gt;
The first things I have understood banging against them were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gadget requires initial conditions (ICs) generate by (for example) N-GenIC&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;N-GenIC requires an initial power spectrum from CMBFast that is no longer used&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CMBeasy should substitute CMBFast but it doesn&amp;rsquo;t, it only works for CMB anisotropies&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CAMB should substitute CMBFast&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the &lt;a href=&#34;http://lambda.gsfc.nasa.gov/toolbox/tb_camb_form.cfm&#34;&gt;CAMB interface&lt;/a&gt; is terrible and not very well documentated&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;camb:62c9ecd742ba00d6577ca64643723f05&#34;&gt;CAMB&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the
&lt;a href=&#34;http://lambda.gsfc.nasa.gov/toolbox/tb_camb_form.cfm&amp;quot; target&#34;&gt;main CAMB interface&lt;/a&gt;.
If you, like me, need an initial power spectrum as input for your ICs generator
you are interested in just few options of the interface. First, you should select
&amp;ldquo;Transfer functions&amp;rdquo; from the &amp;ldquo;Actions to Perform&amp;rdquo; section. You can leave the
default selection on &amp;ldquo;Scalar Cl&amp;rsquo;s&amp;rdquo; and &amp;ldquo;Linear&amp;rdquo;. After that, check the
&amp;ldquo;Cosmological Parameters&amp;rdquo; section if it&amp;rsquo;s ok for you and maybe, leave the
default &amp;ldquo;Initial Scalar Perturbation Mode&amp;rdquo; that is &amp;ldquo;Adiabatic&amp;rdquo;. The last
things you should be interested in are the maximum &lt;code&gt;$k$&lt;/code&gt;, for me it is &lt;code&gt;$10^4$&lt;/code&gt;,
&amp;ldquo;k per logint&amp;rdquo; (it should be something like the k-sampling, for me, 50), the
number of redshift (1) and the (transfer) redshift (0). Now select between
&amp;ldquo;Interpolated Grid&amp;rdquo; or &amp;ldquo;Calculated Values&amp;rdquo; (this parameter switch between and
interpolated regular grid in log k or array at actual computed values that are
better for later re-interpolation, according with the CAMB README) and choose
if you want high precision computation. When you have finished you can click
on &amp;ldquo;Go!&amp;rdquo;. What you obtain is a page with some links to download:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;camb_*.ini&lt;/code&gt;: the configuration file to run the standalone CAMB code on your own&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*.log&lt;/code&gt;: the calculation log&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_scalcls.dat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_scalcls.fits&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_transfer_out_z0.dat&lt;/code&gt;: this is the file containing the transfer
functions for CDM, baryon, photon, massless neutrino, massive neutrinos,
and total (massive) respectively as function of &lt;code&gt;$k$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_matterpower_z0.dat&lt;/code&gt;: it contains the conventionally normalized
matter power spectrum (for baryons+cdm+massive neutrinos), in h/Mpc units&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where &lt;code&gt;*&lt;/code&gt; is a number that identifies the computation and z0 can be different
if you choose to use a different redshift.&lt;br /&gt;
You can use &lt;code&gt;camb_*_matterpower_z0.dat&lt;/code&gt; as normalized input power spectrum or
you can calculate (also as a check) it on you own using the first and the last
column of &lt;code&gt;camb_*_transfer_out_z0.dat&lt;/code&gt;. You should use the last column because
DM simulations represents with DM particles all the mass, included that of the
baryons, so the initial power spectrum should be the total power spectrum.&lt;br /&gt;
If you want to understand better how CAMB works you can try to read the
&lt;a href=&#34;http://camb.info/readme.html&#34;&gt;CAMB README&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initial power spectrum theory&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here some theory if you want to understand how to calculate the power spectrum
on your own. Maybe it is well-known and trivial, but for me it wasn&amp;rsquo;t and,
like many of the trivial and well-known things in (not only) cosmology,
it&amp;rsquo;s not easy to find anywhere.&lt;br /&gt;
So, the spectrum is defined as &lt;code&gt;$P(k) = A k^n T(k)^2$&lt;/code&gt;, where k
&lt;code&gt;$n$&lt;/code&gt; is the
&amp;ldquo;primordial spectral index&amp;rdquo; and is taken near the unity. This is a
&amp;ldquo;scale-fee&amp;rdquo; spectrum. &lt;code&gt;$T(k)$&lt;/code&gt; is the transfer function that give a
synthetic and parametric description of how the initial spectrum survive
to the microphysic. &lt;code&gt;$A=\left[\frac{D(z_{fin})}{D(z_{in})}\right]^2$&lt;/code&gt; is the amplitude.&lt;br /&gt;
The normalization of the spectrum is given by the value of &lt;code&gt;$\sigma_8$&lt;/code&gt;,
that is the mean square amplitude of the density field filtered on the scale
of 8 Mpc/h. This values comes from the &amp;lsquo;80s, when Peebles and others
(Davis &amp;amp; Peebles 1983) measured &lt;code&gt;$\sigma_{galaxies}$&lt;/code&gt; and &lt;code&gt;$\sigma_{gal}(R=8)\sim1$&lt;/code&gt;
so they took that values as reference.&lt;br /&gt;
&lt;code&gt;$\sigma_8$&lt;/code&gt; is defined by
&lt;code&gt;$\sigma^2(R) = \frac{1}{(2\pi)^3}\int \mathrm{d}^3kP(k)\tilde W(kR)$&lt;/code&gt; with &lt;code&gt;$R=8\mathrm{Mpc/h}$&lt;/code&gt; and
&lt;code&gt;$\tilde W(kR)$&lt;/code&gt; the Fourier transform of the window (filter) function, usually a top-hat in the positions space.&lt;br /&gt;
The last thing we need to know to obtain the spectrum is the value of the amplitude,
and it can be find by imposing its value so that &lt;code&gt;$\sigma_8$&lt;/code&gt; has a certain (observed) value.&lt;br /&gt;
Because&lt;br /&gt;
&lt;code&gt;$$\sigma^2(R) = \frac{1}{(2\pi)^3}\int \mathrm{d}^3kP(k)\tilde W(kR) = \int \mathrm{d}^3k Ak^nT^2(k))\tilde W(kR)$$&lt;/code&gt;&lt;br /&gt;
we have&lt;br /&gt;
&lt;code&gt;$$A_0 = \frac{s^2_R(R=8) }{ \int\mathrm{d}^3k\, k^n T^2(k) \tilde W(8*k)}$$&lt;/code&gt;.&lt;br /&gt;
Usually &lt;code&gt;$A_0$&lt;/code&gt; is calculated for &lt;code&gt;$\sigma_8=1$&lt;/code&gt; and then scaled with
&lt;code&gt;$A = A_0\sigma^2_{8;obs}$&lt;/code&gt; where &lt;code&gt;$\sigma_{8;obs}$&lt;/code&gt; is the observed values for
&lt;code&gt;$\sigma_8$&lt;/code&gt;. With the last observations we have &lt;code&gt;$\sigma_8 = 0.8118405$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Just to have an idea you can do these computation with a simple python code.
The code below compare the computation done in Python with the values from
&lt;code&gt;camb_*_transfer_out_z0.dat&lt;/code&gt; with the normalized power spectrum from &lt;code&gt;camb_*_matterpower_z0.dat&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import numpy as np
import matplotlib.pyplot as plt

# Load data
transfer = np.genfromtxt(&#39;camb_88704620_transfer_out_z0.dat&#39;, usecols = (0,6))
matterpower = np.genfromtxt(&#39;camb_88704620_matterpower_z0.dat&#39;)

# Python calculations
camb_k = transfer[:,0]
camb_tf = transfer[:,1] 
R = 8 #Mpc/h
s_8 = 0.8118405
sp_ind = 1

# Define the FT of the window function
def FTW(R, k):
  return 3*(np.sin(k*R)-k*R*np.cos(k*R)) / (k*R)**3

# camb_k**(2+sp_ind) that is k^(2+n) because d^3k=4\pi k^2dk
amp_integrand = camb_k**(2+sp_ind)*camb_tf**2 * FTW(8, camb_k)**2
amp_integral = integrate.trapz(amp_integrand, camb_k)
amp_0 = 2*np.pi**2/amp_integral
amp = amp_0*s_8**2
spectrum = camb_k**sp_ind*camb_tf**2 * amp

ax = fig.add_subplot(111)
ax.set_title(&#39;Fortran/Python CDM initial power spectrum&#39;)
ax.set_xlabel(&#39;k&#39;)
ax.set_ylabel(&#39;P(k)&#39;)
ax.set_xscale(&#39;log&#39;)
ax.set_yscale(&#39;log&#39;)

ax.plot(transfer[:,0], spectrum[:], color = &amp;quot;magenta&amp;quot;, 
           linestyle = &#39;-&#39;, marker = &#39;&#39;, label = &amp;quot;* python amp&amp;quot;) 
ax.plot(matterpower[:,0], matterpower[:,1], color = &amp;quot;black&amp;quot;, 
            linestyle = &#39;--&#39;, marker = &#39;&#39;, label = &amp;quot;matterpower&amp;quot;)
ax.legend(loc=&#39;best&#39;)
ax.grid(True)

# Adjust figure size and save
fig.set_size_inches(20, 20)
plt.savefig(&#39;camb_f90_py_check&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../files/camb_f90_py_check.png&#34; alt=&#34;CAMB vs Python calculated initial power spectrum&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want you can check the power spectrum we have obtained by integrating it
to find &lt;code&gt;$\sigma_8$&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sigma_integrand = camb_k**2 * spectrum * FTW(8, camb_k)**2
s_8_check = pow(integrate.trapz(sigma_integrand, camb_k)/(2*np.pi**2), 0.5)
print &amp;quot;s_8_calculated&amp;quot;, s_8_check
print &amp;quot;s_8 observed&amp;quot;, s_8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;obtaining&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ziosi@uno:~/Code/spettro_CMB$ ./CAMB_check_plot.py
s_8_calculated 0.8118405
s_8 observed 0.8118405
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;N-GenIC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we have the initial power spectrum ready for the ICs generator. After N-GenIC
have been compiled (try to read &lt;a href=&#34;my-first-gadget2-tests&#34;&gt;this&lt;/a&gt;
if you have compilation problems related to the parallel double precision FFTW libraries
or if you want to know how to customize the Makefile) we should have a look at the configuration file.&lt;br /&gt;
We are interested in:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Nmesh&lt;/code&gt;: the size (=the number of nodes) of the FFT grid used to compute the
displacement field, should be &lt;code&gt;Nmesh&lt;/code&gt; &lt;code&gt;$\geq$&amp;amp;nbsp;&lt;/code&gt;Nsample`&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Nsample&lt;/code&gt;: this is the maximum k that the code uses, i.e. this effectively
determines the Nyquist frequency that the code assumes, &lt;code&gt;$k_{Nyquist} = 2\cdot PI/Box \cdot  Nsample/2$&lt;/code&gt; Normally,
one chooses &lt;code&gt;Nsample&lt;/code&gt; such that &lt;code&gt;$Ntot =  Nsample^3$, where&lt;/code&gt;Ntot&lt;code&gt;is the total number 
of particles. Because the grid sample the particles quantities, Nmesh sets the Nyquist 
frequency of Nsample, so it&#39;s good if&lt;/code&gt;$Nmesh = 2\cdot Nsample$&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ReNormalizeInputSpectrum&lt;/code&gt;: set it to 0 because we are going to use the previous
spectrum that is already normalized, if you don&amp;rsquo;t remember this the code will have
integration problems&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TileFac&lt;/code&gt;: it represents how many times you need to tile the glass file
(for each dimension) to cover the number of particles you want to use.
The glass file contains 4097 particles. Glass particles positions will be
automatically stretched to cover the box dimension. When you download N-GenIC
you find &lt;code&gt;$Nsample = 128$&lt;/code&gt; and &lt;code&gt;$TileFac = 8$, this is because the total number of 
particles is&lt;/code&gt;$Ntot = Nsample^3 = 128^3=2097125$&lt;code&gt;and the number of glass particles 
is&lt;/code&gt;$TileFac^3\cdot Nglass = 8^3\cdot 4096 = 2097125$. In practice, if you want to
know what &lt;code&gt;TileFac&lt;/code&gt; should be, and you have &lt;code&gt;Ntot&lt;/code&gt; particles in you simulation,
&lt;code&gt;TileFac&lt;/code&gt; will be &lt;code&gt;$\frac{Ntot^{1/3}}{4096} = \frac{Nsample}{4096}$&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WhichSpectrum&lt;/code&gt;: let you choose if you want to use an internal spectrum
(calculated with a function) or the spectrum from CAMB&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FileWithInputSpectrum&lt;/code&gt;: it&amp;rsquo;s, obviously, the name of the file containing the spectrum&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The other parameters are cosmological parameters or are related to the folders, the name and the
number of files, the parallelization and to the internal measure units.&lt;br /&gt;
Other options are (more or less) documented with comments in the code or in the README.&lt;br /&gt;
We can now start N-GenIC with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mpiexec -np 2  ./N-GenIC  ics.param
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;-np&lt;/code&gt; let you set the number of processors you want to use in parallel.&lt;br /&gt;
Before using the output files with gadget we should open those and calculate the power
spectrum to check that this realization of it is a good one. This problem arise because
of the sampling of the k-space where few modes are available, but I will deepen on those matter in a future post.&lt;br /&gt;
There is also an improved version of N-GenIC, 2LPTIC, but it need a different
installation of the FFTW so I didn&amp;rsquo;t try it.&lt;/p&gt;

&lt;p&gt;Many of these things can be found &lt;a href=&#34;http://www.annualreviews.org/doi/abs/10.1146/annurev.astro.36.1.599&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #2: spherical collapse</title>
      <link>http://brunettoziosi.eu/posts/phd-question-2-spherical-collapse/</link>
      <pubDate>Tue, 10 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-2-spherical-collapse/</guid>
      <description>&lt;p&gt;With this post I would like to collect and present in a simple and consistent
form some of the various analytical derivation of the spherical collapse
model, in particular the challenge was to find how to obtain the famous
$\delta_{lin}\sim1.686$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/spherical_collapse2.pdf&#34;&gt;Here&lt;/a&gt; you can find a pdf file with the analytic computation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My first Gadget2 tests</title>
      <link>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</link>
      <pubDate>Sat, 07 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</guid>
      <description>

&lt;p&gt;This post is about my first experience with the cosmological simulation code &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget2&#34;&gt;Gadget2&lt;/a&gt;. To start I followed the instructions found &lt;a href=&#34;http://astrobites.com/2011/04/02/installing-and-running-gadget-2/&#34;&gt;here&lt;/a&gt;. All I&amp;rsquo;m going to write refers to an Ubuntu/Kubuntu 11.10 installation.&lt;/p&gt;

&lt;h2 id=&#34;installation-of-gsl-and-fftw:b6a0a5ab44b8eeb8249d7bb237655d64&#34;&gt;Installation of GSL and fftw&lt;/h2&gt;

&lt;p&gt;We can download Gadget &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget download&#34;&gt;here&lt;/a&gt;, the GSL (GNU scientific library) &lt;a href=&#34;http://mirror.rit.edu/gnu/gsl/gsl-1.9.tar.gz&#34; title=&#34;GSL download&#34;&gt;here&lt;/a&gt; and the FFTW (fastest Fourier transform in the West library) &lt;a href=&#34;http://www.fftw.org/fftw-2.1.5.tar.gz&#34; title=&#34;FFTW download&#34;&gt;here&lt;/a&gt;. We also need an MPI library (Open-MPI or MPICH, try install it using your package manager).&lt;br /&gt;
Following the Astrobites suggestions let&amp;rsquo;s decompress the archives with &lt;code&gt;tar -xzf &amp;amp;lt;archive name&amp;amp;gt;&lt;/code&gt;. Now we can install the libraries following the Astrobites post:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd gsl-1.9/
goldbaum@~/Documents/code/gsl-1.9: ./configure
snip: lots of diagnostic ouput
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix --enable-float
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As described &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_6.html#SEC69&#34; target=&#34;_blank&#34; title=&#34;FTTW installation and customization&#34;&gt;here&lt;/a&gt; is convenient to install both the single and the double precision version of the FFTW (for example to compile the initial conditions generators) with (that is, without &lt;code&gt;--enable-float&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;play-with-gadget2:b6a0a5ab44b8eeb8249d7bb237655d64&#34;&gt;Play with Gadget2&lt;/h2&gt;

&lt;p&gt;Now it&amp;rsquo;s time to play with Gadget!:) In this code, for performance reasons, requires to specify some parameters at compile time while other can be set at run time, so that we have to customize the Makefile. This also imply that we should have separate binary files and directories for each simulation.&lt;br /&gt;
To start with something easy, we will customize one of the examples given with the code, the &amp;ldquo;galaxy&amp;rdquo; one. It simulate the collision of two galaxies using 40000 DM particles for the haloes and 20000 baryonic particles for the disks.&lt;/p&gt;

&lt;p&gt;Inside the Gadget directory we have&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Analysis
AUTHORS
COPYING
COPYRIGHT
Documentation
Gadget2
ICs
README
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the &lt;code&gt;Analysis&lt;/code&gt; folder we can fin some analysis routines provided by the author, the &lt;code&gt;Documentation&lt;/code&gt; folder contains the user guide and the original paper, and the &lt;code&gt;AUTHORS, COPYING, COPYRIGHT&lt;/code&gt; self-explanatory. The &lt;code&gt;ICs&lt;/code&gt; folder contains the initial conditions for the example simulations and the &lt;code&gt;Gadget2&lt;/code&gt; folder contains the sources and the html documentation.&lt;/p&gt;

&lt;p&gt;To be tidy and organized is better to have a folder for every simulations, so we will create a (descriptive) with everything we need to customize&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir 2012-01-07-Gadget2-galaxy_test_01
cd 2012-01-07-Gadget2-galaxy_test_01
mkdir out
cp ../ICs/galaxy_littleendian.dat ./
cp ../Gadget2/parameterfiles/galaxy.param ../Gadget2/parameterfiles/galaxy.Makefile ./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the folder Gadget2 we can find the general &lt;code&gt;Makefile&lt;/code&gt; but for now let&amp;rsquo;s use the galaxy&amp;rsquo;s one provided by the author and just copied to our position. Open it with your preferred text editor (for example, in a command line environment, &lt;code&gt;emacs -nw Makefile&lt;/code&gt;).&lt;br /&gt;
This &lt;code&gt;Makefile&lt;/code&gt; is already customized for the galaxy collision simulation and if you want to understand every option you can read the description in the guide, but we need some more customization. Here what I&amp;rsquo;ve changed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPT   +=  -DHAVE_HDF5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so I activate the HDF5 format for the output and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#--------------------------------------- Select target computer

SYSTYPE=&amp;quot;Uno&amp;quot;
\#SYSTYPE=&amp;quot;MPA&amp;quot;
\#SYSTYPE=&amp;quot;Mako&amp;quot;
\#SYSTYPE=&amp;quot;Regatta&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster-gcc&amp;quot;
\#SYSTYPE=&amp;quot;Opteron&amp;quot;

\#--------------------------------------- Adjust settings for target computer

ifeq ($(SYSTYPE),&amp;quot;Uno&amp;quot;)
CC       =  mpicc   
OPTIMIZE =  -O3 -Wall
GSL_INCL =  -I/usr/local/include
GSL_LIBS =  -L/usr/local/lib
FFTW_INCL=  -I/usr/local/include
FFTW_LIBS=  -L/usr/local/lib
MPICHLIB =  -L/usr/lib
HDF5INCL =  
HDF5LIB  =  -lhdf5 -lz 
endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to select the set the options for my system.&lt;br /&gt;
Now we have to customize the &lt;code&gt;run/galaxy.param&lt;/code&gt; file changing it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;InitCondFile      ./galaxy_littleendian.dat
OutputDir          ./galaxy_out/
OutputListFilename ./out/output_list.txt
SnapFormat         3  %to select the HDF5 format
TimeBegin           0.0        % Begin of the simulation
TimeMax             40.0        % End of the simulation

% Output frequency
TimeBetSnapshot        0.1% original 0.5 &amp;lt;/pre&amp;gt;
    
    
Now we should go to the sources folder and compile the code with    
&amp;lt;pre&amp;gt;cd ../Gadget2
make -f 2012-01-07-Gadget2-galaxy_test_01/galaxy.Makefile
cp Gadget2 ../2012-01-07-Gadget2-galaxy_test_01/Gadget2
make clean
cd 2012-01-07-Gadget2-galaxy_test_01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command clean the build leaving only the sources files, so we are ready for a new build.&lt;br /&gt;
We can also create a script for automatize all this steps, something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
dir=$1
ics=$2
param_file=$3
mk_file=$4
CPUs=$5

if [ $# -lt 5 ] ; then
  echo &amp;quot;usage: gadget_set directory_name initial_conditions_file
parameters_file make_file number_of_CPUs&amp;quot;
  exit 0
fi

echo &amp;quot;Assuming to use $dir as the run folder,&amp;quot; 
echo &amp;quot;$ics as initial conditions,&amp;quot;
echo &amp;quot;$paramfile as parameter file, &amp;quot;
echo &amp;quot;$mk_file as makefile &amp;quot;
echo &amp;quot;and to run on $CPUs CPUs.&amp;quot;

mkdir $dir
cd $dir
mkdir out
cp ../ICs/$ics ./
cp ../Gadget2/parameterfiles/$param_file
../Gadget2/parameterfiles/mk_file ./
cd ../Gadget2
make -f ../$dir/$mk_file
cp Gadget2 ../$dir/Gadget2
make clean
cd $dir
mpirun -np $CPUs ./Gadget2 $param_file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very raw and untested script, but it&amp;rsquo;s just to give an idea.&lt;/p&gt;

&lt;p&gt;Now we are ready to start the simulation with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mpirun -np 2 ./Gadget2 galaxy.param
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;-np&lt;/code&gt; sets the number of processes/processors to be used in parallel.&lt;br /&gt;
When the simulation stops we can analyze it with the tools provided in the &lt;code&gt;Analysis&lt;/code&gt; folder or, if you like me don&amp;rsquo;t own an IDL license and don&amp;rsquo;t feel comfortable with IDL/Fortran/C for the data analysis, with something like (to be run in out/plots/):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import sys, os
from subprocess import Popen, PIPE
from multiprocessing import Process, Queue

import numpy as np
import tables as tb
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

&amp;quot;&amp;quot;&amp;quot;This script will plot in parallel the .h5 snapshots created by Gadget2 test
runs one after the other!:).
FIXME: i need a way to wait for the final time count the end of the processes
and a way to print the status
&amp;quot;&amp;quot;&amp;quot;

# Set the max number of processes
n_procs = 3

# Set the number of snapshot to be plotted
n_snap = 401

t = time.time()

print &amp;quot;Defining workers...&amp;quot;

def worker(input, output):
    while input.qsize() != 0:
        item = input.get()
        if item[0]= 10 and item[0]&amp;amp;lt;100: j=&amp;quot;0&amp;quot;+str(item[0])
        else: j=str(item[0])
        try:
#     print &amp;quot;considering file ../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;
#     print &amp;quot;open file &amp;quot;
            h5 = tb.openFile(&amp;quot;../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;, &#39;r&#39;)
#     print &amp;quot;file opened, set variables&amp;quot;
            halo = h5.root.PartType1
            disk = h5.root.PartType2
#            print &amp;quot;setted, inizialize figure&amp;quot;
            fig2 = plt.figure()
            ax = Axes3D(fig2)
            ax.scatter(disk.Coordinates[:,0], 
                       disk.Coordinates[:,1],
                       disk.Coordinates[:,2],
                       color=&#39;red&#39;, s=0.5)
            ax.scatter(halo.Coordinates[:,0], 
                       halo.Coordinates[:,1],
                       halo.Coordinates[:,2],
                       color=&#39;blue&#39;, s=0.01)
            plt.savefig(&#39;snap_&#39;+j)
#            print &amp;quot;done, closing file&amp;quot;
            h5.close()  
#            print &amp;quot;closed&amp;quot;

        except:
            print &amp;quot;Work &amp;quot;+j+&amp;quot; not done, exit...&amp;quot;
            sys.exit()

def fill_queue(task_queue):
    for i in range(n_snap):
        task_queue.put([i])
    return task_queue

def status(proc):
    if proc.is_alive==True:
        return &#39;alive&#39;
    elif proc.is_alive==False:
        return &#39;dead&#39;
    else:
        return proc.is_alive()

print &amp;quot;Define queues...&amp;quot;

input_queue = Queue()
output_queue = Queue()

try:
    input_queue = fill_queue(input_queue)
except:
    print &amp;quot;Queue not filled, exit...&amp;quot;
    sys.exit()

procs = []

try:
    for i in range(n_procs):
        procs.append(Process(target=worker, args=(input_queue,
output_queue)))
except:
    print &amp;quot;Creating processes not complete, exit...&amp;quot;
    sys.exit()

try:
    for i in procs:
        i.start()
except:
    print &amp;quot;Start processes not complete, exit...&amp;quot;
    sys.exit()

for i in procs:
    print &amp;quot;Process &amp;quot;, i,&amp;quot; @ &amp;quot; , i.pid, &amp;quot; is &amp;quot;, status(i)

print &amp;quot;Done in &amp;quot;+str(time.time()-t)+&amp;quot; seconds.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have one image for each snapshot, and if we are interested we can produce a video with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mencoder mf://*.png -mf fps=25:type=png -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -vf scale=720:360 -oac copy -o output.mp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;youtube http://www.youtube.com/watch?v=b7HyafKMkxI&amp;amp;amp;w=560&amp;amp;amp;h=315&#34;&gt;This&lt;/a&gt; is the first basic video, with logarithmic time and perhaps there&amp;rsquo;s something wrong with the coordinates on the axes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GIF2 substructures coordinates correction</title>
      <link>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</link>
      <pubDate>Sun, 04 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</guid>
      <description>&lt;p&gt;I used this script to change the coordinates of the substructures in the GIF2 simulation output from the center of mass coordinates to the global ones. The substructures were stored in our server in files referring to the index of the halo to which they belong and their coordinates were respect to the center of the halo. For each halo this script read the subhaloes center of mass coordinates in kpc and change them to global coordinates in Mpc managing the periodic boundary conditions .&lt;/p&gt;

&lt;!-- TEASER_END --&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import numpy as np
import tables as tb
import time
import sys

&amp;quot;&amp;quot;&amp;quot;Legge il file con id e centri degli aloni con sottostrutture, per
ogni alone (id) apre il file Sub.3..053.gv, legge le coordinate
alle colonne 8, 9, 10 (in kpc!!!) e le corregge (tenendo conto delle
condizioni periodiche) con le coordinate del centro prese dalla lista
degli aloni iniziale.  Le coordinate vengono aggiunte ad un vettore
che alla fine viene salvato in un file hdf5.
&amp;quot;&amp;quot;&amp;quot;

t = time.time()
print &amp;quot;Start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As usual: imports, documentation and timing initialization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;halo_centers_file = &#39;haloes_with_substructures_centers.h5&#39;

h5 = tb.openFile(halo_centers_file, &#39;r&#39;)
haloes = h5.root.data.read()
h5.close()

haloes = haloes.astype(float)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code reads the halo centers from a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;substructure_coord = np.empty((0, 3))
files_num = haloes[:, 0].shape[0]
void_files = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we create the first (void) record of the substructures coordinates table, find the number of haloes and create a variable which will tell us how many haloes without subtructures we have.&lt;/p&gt;

&lt;p&gt;Now, for each halo we&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in xrange(files_num):
    print &amp;quot;Loop &amp;quot;, i, &amp;quot; di &amp;quot;, files_num 
    sub_file = &amp;quot;cm/Sub.3.&amp;quot;+&#39;%07d&#39;%int(haloes[i,0])+&amp;quot;.053.gv&amp;quot;
    ````
    
open the related substructures file     
````python
try:
        sub_coord = np.genfromtxt(sub_file, dtype=&#39;float&#39;, usecols=(8, 9, 10))
        file_check = True
    except:
        print &amp;quot;Void file &amp;quot;, sub_file
        file_check = False
        void_files += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;try to read the substructures coordinates, if it fails, we assume that the file is empty (and the halo has no substructures). In this case we increment the counter of the empy files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if file_check:
        if sub_coord.ndim == 1:
            sub_coord = sub_coord.reshape((1, sub_coord.shape[0]))
            #print &amp;quot;sh min &amp;quot;, np.amin(sub_coord, 0)
            #print &amp;quot;sh min &amp;quot;, np.amax(sub_coord, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This reshape the array in case we have only one subhalo: in this case (I hope I remember correct!:P) instead of one row and three columns we should have three values, so we have to reshape the array to pass it to the rest of the code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
            sub_x = sub_coord[:, 0]/1000. + haloes[i,1]
            if not np.all(sub_x &amp;amp;gt; 0):
                sub_x[sub_x&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_x 110]-=110
            if not (np.all(sub_x &amp;amp;gt; 0) and np.all(sub_x  0)
                print sub_x 
                print haloes[i, 1:3]
                sys.exit()
            sub_y = sub_coord[:, 1]/1000. + haloes[i,2]
            if not np.all(sub_y &amp;amp;gt; 0):
                sub_y[sub_y&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_y 110]-=110
            if not (np.all(sub_y &amp;amp;gt; 0) and np.all(sub_y  0)
                print sub_y
                print haloes[i, 1:3]
                sys.exit()
            sub_z = sub_coord[:, 2]/1000. + haloes[i,3]
            if not np.all(sub_z &amp;amp;gt; 0):
                sub_z[sub_z&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_z 110]-=110
            if not (np.all(sub_z &amp;amp;gt; 0) and np.all(sub_z  0)
                print sub_z 
                print haloes[i, 1:3]
                sys.exit()
            substructure_coord = np.vstack((substructure_coord, np.hstack((sub_x.reshape((sub_x.shape[0], 1)), 
                                                                           sub_y.reshape((sub_y.shape[0], 1)), 
                                                                           sub_z.reshape((sub_z.shape[0], 1))))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This corrects the coordinates keeping in mind the periodic boundary conditions and add the new substructures coordinates to the corrected substructures array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;except:
            print &amp;quot;file &amp;quot;, sub_file
            print &amp;quot;sub_coord.shape &amp;quot;, sub_coord.shape
            print &amp;quot;sub_coord &amp;quot;, sub_coord
            print &amp;quot;haloes coord &amp;quot;, haloes[i, :]
            print &amp;quot;exit&amp;quot;
            sys.exit()
    else:
        pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If something goes wrong, we handle the failure printing some information.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5 = tb.openFile(&#39;sub_haloes_global_coords.h5&#39;, &#39;w&#39;)
h5.createArray(h5.root, &#39;data&#39;, substructure_coord)
h5.flush()
h5.close()
print &amp;quot;Done in &amp;quot;, time.time()-t, &amp;quot; with &amp;quot;, void_files, &amp;quot; void files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, we save the array in an HDF5 file!:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #6: adding gas!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-6-adding-gas/</link>
      <pubDate>Tue, 29 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-6-adding-gas/</guid>
      <description>&lt;p&gt;As we have seen, cosmological N-body simulations consider only gravitational interactions. This is good for large scale distribution of matter such as galaxy distribution or cluster formation, but if we are interested in smaller details we have to add gas physics.&lt;br /&gt;
There are two main approaches, completely different one from the other.&lt;/p&gt;

&lt;p&gt;The first approach uses a grid to solve fluid equations. Fluid interactions are short range ones so we can use information from few nearby points to compute and evolve the quantities we are interested in at a point. Fluid must also respond to the gravitational field of the matter distribution. One can easily study shocks and discontinuities with this type of code.&lt;br /&gt;
It is also possible to improve the resolution using mesh refinement but it&amp;rsquo;s important to bear in mind that the resolution of mass particles and of hydrodynamics should be improved together. Grid codes can be expanded to include other effects such as magnetohydrodynamics.&lt;br /&gt;
The second way is the Smoothed Particles Hydrodynamics (SPH). In these type of algorithms the fluid properties (pressure, density, temperature, &amp;hellip;) at any point can be found by averaging over particles in a region using a weight function. Because of this smoothing functions it has poor resolution of shocks and discontinuities and low resolution in low density regions. However these codes are quite easily to implement and has high resolution in high density regions.&lt;/p&gt;

&lt;p&gt;In both types of codes effects such as elementary chemical reactions, e.g. formation of hydrogen, molecules, cooling, heating, etc can be added because they are local effects. On the contrary star formation, feedback from stellar and other source, radiation transport, and so on are non-local and include a big range of scales so are difficult to implement.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #5: initial conditions!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-5-initial-conditions/</link>
      <pubDate>Wed, 16 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-5-initial-conditions/</guid>
      <description>

&lt;p&gt;After we had a look on why we need cosmological simulations, what they are and how they are performed, it&amp;rsquo;s time to learn more about the preparation of a simulation, in particular what are initial conditions and how we build them.&lt;/p&gt;

&lt;p&gt;Usually, for a cosmological simulation, all the fields are linear so we can use linear theory to compute them. In linear theory the density contrast evolves as a combination of a growing and a decaying mode, but only the first survives, so we can set the initial system considering only the growing mode, and because until linearity is verified the density field and velocities are related to the gravitational potential we only need to generate the gravitational field and with it we can produce density and velocity fields.&lt;br /&gt;
Once we have the initial potential there are two ways to generate the initial conditions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;we can uniformly distribute the particles in the simulation box and choose the masses according to the gravitational field; velocities can be zero, in which case we increase the potential to account for the decaying mode, or non-zero and appropriate for the growing mode&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;we can displace uniformly distributed particles using the velocities of the linear theory; the displacement should be smaller than the interparticle separation or we can obtain a wrong power spectrum because a bigger displacement would lead the trajectories to cross and in this case, formally, the density contrast grows to infinite (this is not acceptable in linear approximation) and after that it decreases while the particles go away and this is not physical&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The relation that linking velocity and gravitational potential in the second entry:&lt;br /&gt;
$\frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t}+2\frac{\mathrm{d}a/\mathrm{d}t}{a}\mathbf{v}\propto \mathbf{g}=-\frac{\nabla\phi}{a}$
is the same both in linear theory and in the Zel&amp;rsquo;dovich approximation (this takes the non-linear equations together with the acceleration given by the linear theory to obtain results in the quasi-linear regime), so it&amp;rsquo;s often said that the Zel&amp;rsquo;dovich approximation is used to set up initial conditions.&lt;/p&gt;

&lt;p&gt;Given that, the homogeneity of the initial unperturbed distribution it&amp;rsquo;s very important because any inhomogeneity would combine with density perturbations modifying initial conditions. This homogeneity can be thought in different ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a homogeneous but not random way is to put particles on a cubic grid but it could produce visible features due to the regular distribution&lt;/li&gt;
&lt;li&gt;we can consider putting particles at random positions but the $\sqrt{n}$ fluctuations will result in spurious clustering that will eventually dominate the fluctuations we want to simulate&lt;/li&gt;
&lt;li&gt;a good compromise could be placing particles in lattice cells with a random displacement from the center: this remove regularity but maintain uniformity&lt;/li&gt;
&lt;li&gt;the last possibility are &amp;ldquo;glass initial conditions&amp;rdquo; obtained by evolving an arbitrary distribution of particles in an n-body simulation with a repulsive force; it was invented by Simon White and the name refers to the molecular structure of glass, uniform but not regular&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Considering the last two possibilities, it&amp;rsquo;s worth noting that the former still has a lot of noise, and the second is better than the grid only for aesthetic reasons.&lt;/p&gt;

&lt;h2 id=&#34;the-initial-gravitational-potential:e7d3a1009e17d7d6009dfef83606ab41&#34;&gt;The initial gravitational potential&lt;/h2&gt;

&lt;p&gt;In standard cosmological models the initial perturbations density field is Gaussian. Since the linear evolution doesn&amp;rsquo;t modify the statistics of density fields except for evolving the amplitude of perturbations, and because the potential and the density contrast are linked by a linear relation, the gravitational potential is also a Gaussian random field, and such a field is completely described in terms of the power spectrum of density perturbations. The Fourier components of a Gaussian random field (both real and imaginary part) are random numbers with a normal distribution and variance proportional to the power spectrum at that wave number. Also the phase of this random numbers has to be random, and changing it the resulting field is completely different.
We can generate the gravitational field in the Fourier space just using these properties and inverse transform it (or the force) to obtain the initial potential in real space. If we use adaptive mesh refinement codes we need Gaussian random fields with a variable resolution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #3: force calculation!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-3-force-calculation/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-3-force-calculation/</guid>
      <description>

&lt;p&gt;In the previous posts we wrote about cosmological simulations, why we need them, how they are performed and which are the important things to care about.
Now we are ready to learn the algorithms developed to compute gravitational forces between the particles.&lt;/p&gt;

&lt;h3 id=&#34;direct-summation-pp-method:b4d5cf4d6678e662a616552bea757490&#34;&gt;Direct summation: PP method&lt;/h3&gt;

&lt;p&gt;The raw approach is to calculate forces between all pairs in the simulation. This works well for less than $10^3$. The number of pairs, and therefore the computational time, scales as $N^2$, so this method that is usually unacceptable given the number of particles in a simulation.&lt;br /&gt;
It&amp;rsquo;s difficult to implement periodic boundary conditions in this method (because you have to manage it by hand) and the only convenient way to do this is to use the Ewald summation.&lt;br /&gt;
This method is mostly used to test other methods.&lt;/p&gt;

&lt;h3 id=&#34;tree-method:b4d5cf4d6678e662a616552bea757490&#34;&gt;Tree method&lt;/h3&gt;

&lt;p&gt;The disadvantage of the pp-method comes from having to compute the interaction of any new particle with all the others. The tree method, on the contrary, approximate the force of a distant group of particles with the force exerted by only one particle, in the center of mass of the group, with mass equal the total mass of the group. In this way the computation scales as $N\log N$.&lt;br /&gt;
To create the groups the particles are divided and arranged in a tree structure, that is, the initial volume is divided, and every resulting volume divided again. This procedure goes on until in the resulting volumes there&amp;rsquo;s only one particle. In this situation it&amp;rsquo;s very important the &amp;ldquo;cell acceptance criterion&amp;rdquo; that decide whether or not a cell is far enough to be used as it is or if it has to be divided into its subvolumes.&lt;br /&gt;
The accuracy and the speed of this method can be improved storing some information about the particles in the volumes (such as moments of the mass distribution, usually the quadrupole).&lt;br /&gt;
Moreover, close particles can share information about distant groups because the force is very similar, and the tree can be parallelized efficiently. However, it&amp;rsquo;s not so easy to consider periodic boundary conditions.&lt;/p&gt;

&lt;h3 id=&#34;fast-multipole-method:b4d5cf4d6678e662a616552bea757490&#34;&gt;Fast multipole method&lt;/h3&gt;

&lt;p&gt;This method is an improved version of the tree method. It is based on including higher moments of the mass distribution in cells and some other optimizations. This method has a computational costs which scales as $N$.&lt;/p&gt;

&lt;p&gt;However all these method has problems with the open boundary conditions and it&amp;rsquo;s difficult to adapt it to the cosmological needs. An extension of the tree method with explicit momentum conservation has been also developed.&lt;/p&gt;

&lt;h3 id=&#34;particle-mesh-method:b4d5cf4d6678e662a616552bea757490&#34;&gt;Particle-mesh method&lt;/h3&gt;

&lt;p&gt;This is the first method used extensively for cosmological simulations and the first used for simulation with order of $10^5$ particles. The PM method solve the Poisson equation (partial differential equation that links the gravitational field with the mass distribution) in the Fourier space, where it is a simple algebraic equation, using the FFT (Fast Fourier Transform) routine to change from coordinate space to the Fourier space and viceversa&amp;nbsp;. FFT requires to sample the functions at uniformly spaced points and here we use a grid (mesh). We compute density at the grid points using weight functions on the particles representing the density and velocity fields.&lt;br /&gt;
The use of Fourier methods automatically include periodic boundary conditions without no additional effort (because of the nature of periodic function in the Fourier space) and the use of the mesh automatically soften the force at small scales (because of the interpolation needed to compute the gravitational field on the grid) but it underestimate the force at scales larger (toughly up to 3 times) than the softening length.&lt;br /&gt;
This method is parallelized using parallel FFT and dividing the volume among the processors.&lt;br /&gt;
Softening at the mesh scale give collisionless evolution but the code cannot resolve structure at scales smaller than the mesh scale, moreover the mesh makes the force anisotropic at small scales. This happens because the grid points don&amp;rsquo;t sample very well the filed on such small scales.&lt;/p&gt;

&lt;h3 id=&#34;adaptive-mesh-refinement:b4d5cf4d6678e662a616552bea757490&#34;&gt;Adaptive-mesh refinement&lt;/h3&gt;

&lt;p&gt;In AMR methods the mesh is refined in high density regions using a finer grid, but it&amp;rsquo;s important to pay attention to the conservation of momentum and angular momentum switching from a grid to another.&lt;/p&gt;

&lt;h3 id=&#34;p3m-particle-particle-particle-mesh:b4d5cf4d6678e662a616552bea757490&#34;&gt;P3M: particle-particle+particle-mesh&lt;/h3&gt;

&lt;p&gt;The idea is to add a correction to the force calculated with the PM method by using the PP method on the closest particles. The correction is assumed to be isotropic and depend only upon the distance. Usually it&amp;rsquo;s considered a distance of the order of 2 times the internode mesh distance. This method has been parallelized but with some problems, also because load balancing is quite difficult to achieve.&lt;/p&gt;

&lt;p&gt;Some other problems are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the correction to the force is isotropic but the PM method has anisotropies at small scales&lt;/li&gt;
&lt;li&gt;the correction is at scales up to 2 times the grid scale but the PM method underestimate the force on scales larger than these&lt;/li&gt;
&lt;li&gt;the refined interparticle softening is at scales smaller than the interparticle separation and this can lead to two-body scattering and relaxation&lt;/li&gt;
&lt;li&gt;P3M simulations slow down at late times when the distribution of particles becomes highly clustered because the short range force dominate the compute operations&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tree-pm-tpm:b4d5cf4d6678e662a616552bea757490&#34;&gt;Tree+PM: TPM&lt;/h3&gt;

&lt;p&gt;Some hybrid codes has been developed trying to combine the PM and the Tree methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Grid of Trees PM (GOTPM)replaces the PP part of P3M with a local tree in each region. This resolve only the last problem of the P3M.&lt;/li&gt;
&lt;li&gt;TPM correct the short range force only if the particle is in a highly dense region and with a tree to compute the forces.&lt;/li&gt;
&lt;li&gt;TreePM divide the force in long-range and short-range instead of correcting the PM force at a certain scale, greater than that of the P3M. The short-range forces are calculated with a global tree.&lt;/li&gt;
&lt;li&gt;The Adaptive TreePM (ATreePM) try to resolve the two-body relaxation and scattering using an adaptive softening length, determined by the local density. To ensure momentum conservation force is simmetrized for particles closer than the softening length. The softening correspond to consider the particles with a density profile and not only mass points and if the softening length is different for two close particles, the force they feel is different between them. This happens &amp;nbsp;because particle A feel a force due to its entire mass and a certain fraction of particle B mass, but particle B feels a force due to its own mass and a different fraction of the mass of particle A. In this case the forces are symmetrized taking the mean of the two forces.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To compare different methods we can consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dynamic range, that is the range of scales over which the force is computed reliably. Usually the limit is at small scales rather than at large scales.&lt;/li&gt;
&lt;li&gt;The code should integrate the equation of motion in a reproducible way and momentum should be conserved.&lt;/li&gt;
&lt;li&gt;The code should be efficient and run with the minimum possible time&lt;/li&gt;
&lt;li&gt;Requirement of memory and other resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #7: Limitations and some considerations</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-7-limitations-and-some-considerations/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-7-limitations-and-some-considerations/</guid>
      <description>

&lt;h2 id=&#34;limitations:be6e6b0278d604ddee51c801f8740b2c&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;In the previous posts we encountered some of the limitations of cosmological&lt;br /&gt;
simulations. Let&amp;rsquo;s review these in detail.&lt;br /&gt;
First, we can consider a simulation composed of a finite box in a bigger space but to represent a real system, this box shouldn&amp;rsquo;t be isolated so we use the periodic boundary conditions (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-2-how.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #2: how?&#34;&gt;here&lt;/a&gt;). This means that all the space around the box is filled with images of the box itself: a particle that leaves the box from one side will come in&lt;br /&gt;
from the opposite side.&lt;/p&gt;

&lt;p&gt;Second, the mass inside the box is not continuous. Instead, it is made by particles of mass of the order of $10^9$ solar masses. These particles represent collisionless fluid elements (made by a huge quantity of real particles) with a certain&lt;br /&gt;
volume and can&amp;rsquo;t be treated as solid spheres. When two simulation particles are&lt;br /&gt;
separated by a distance smaller than the radius of the volumes they represents&lt;br /&gt;
they must feel less than the force coming from the entire mass (thanks to the&lt;br /&gt;
Gauss/Birkhoff&amp;rsquo;s theorem). To do this we soften (read &amp;ldquo;we reduce&amp;rdquo;) the force at&lt;br /&gt;
such small scales (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-2-how.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #2: how?&#34;&gt;here&lt;/a&gt;). Third, time is not continuous and its discreteness was also treated (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-4-moving.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations: #4: Moving the particles!&#34;&gt;here&lt;/a&gt;) with some&lt;br /&gt;
criteria to decide the time steps.&lt;br /&gt;
Until now, however, we haven&amp;rsquo;t consider the effects of taking into account&lt;br /&gt;
initial density fluctuations over a range of scales that is finite. In&lt;br /&gt;
addition to this, the finite size of the box pose a limit on the force&lt;br /&gt;
resolution, because fluctuations on scales bigger than the box side will not&lt;br /&gt;
included in the simulation due to the way the Fourier transforms act on a&lt;br /&gt;
period box. Some tests in literature show that the exclusion of small&lt;br /&gt;
scales shouldn&amp;rsquo;t affect too much large scales when they reach the non linear&lt;br /&gt;
regime but this not holds for the exclusion of large scales, those scales bigger&lt;br /&gt;
than the box side. Following Bagla, the large scale exclusion should not&lt;br /&gt;
disturb the formation of small haloes but could change their distribution.&lt;br /&gt;
This effect will appear as an underestimation of the correlation function. Bagla&lt;br /&gt;
finds that the best way of quantifying the effects of long wave modes is to&lt;br /&gt;
check whether including them in the simulation will change the number of&lt;br /&gt;
massive haloes or not and this can be estimated using the Press-Schecther mass&lt;br /&gt;
function.&lt;br /&gt;
In Tormen&amp;amp;Bertschinger (1996) the missing power on large scales will cause&lt;br /&gt;
something like a statistical cosmic bias decreasing the number of high-density&lt;br /&gt;
regions, the strength of the clustering and the amplitude of the peculiar&lt;br /&gt;
velocities.&lt;br /&gt;
Methods have been developed to take the missing &amp;ldquo;larger than the box&amp;rdquo; wave modes&lt;br /&gt;
into account and we will have a look on these in a future post.&lt;/p&gt;

&lt;h2 id=&#34;some-considerations:be6e6b0278d604ddee51c801f8740b2c&#34;&gt;Some considerations&lt;/h2&gt;

&lt;p&gt;As we have seen (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-1-why-and-what.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #1: why and what?&#34;&gt;here&lt;/a&gt;) N-body cosmological simulations&lt;br /&gt;
are useful to understand aspects of non-linear gravitational clustering,&lt;br /&gt;
since it&amp;rsquo;s not possible to carry out laboratory experiments in gravitational&lt;br /&gt;
dynamics and the analytic models fail when the system reach the non linear&lt;br /&gt;
regime, i.e. when the density contrast overcome the unity. Related with&lt;br /&gt;
cosmological simulations there are a pair of aspects that Bagla underlines in its&lt;br /&gt;
articles that interesting to consider.&lt;br /&gt;
The first issue is whether or not the gravitational clustering&lt;br /&gt;
erase memory of initial conditions. Is there a one-to-one correspondence between&lt;br /&gt;
some characterization of initial perturbations and the final state?&lt;br /&gt;
N-body simulations shows that gravitational clustering does not erase memory of&lt;br /&gt;
the initial conditions, the final power spectrum is a function of the initial&lt;br /&gt;
power spectrum and this relationship can be written as a one-step mapping and&lt;br /&gt;
the functional form of this mapping depends on the initial power spectrum.&lt;br /&gt;
However density profiles of massive haloes have a form independent of&lt;br /&gt;
initial conditions but there is a considerable scatter in density profiles&lt;br /&gt;
obtained from N-body simulations and it is difficult to state whether a given&lt;br /&gt;
functional form is always the best fit or not. I must admit that these last concepts are not very clear to me at the moment, and that I trust Bagla but I will deepen them as soon as possible to be able to comfortably master them.&lt;br /&gt;
The second question is if it is possible to predict the masses and distribution&lt;br /&gt;
of haloes that form as a result of gravitational clustering.&lt;br /&gt;
The initial density field is taken to be a Gaussian random field and for&lt;br /&gt;
hierarchical models the simple assumption that each peak undergoes collapse&lt;br /&gt;
independent of the surrounding density distribution can be used to estimate the&lt;br /&gt;
mass function and several related quantities but N-body simulations shows that&lt;br /&gt;
this simple set of approximations is incorrect. However, the resulting mass&lt;br /&gt;
function estimation is fairly accurate over a wide range of masses. Merger rates&lt;br /&gt;
can be thus computed using the extended Press-Schecther formalism. Modifying&lt;br /&gt;
some of this assumption can lead to improved predictions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://iopscience.iop.org/0004-637X/472/1/14&#34; target=&#34;_blank&#34; title=&#34;G. Tormen and E. Bertschinger, Adding long wavelenght modes to an N-body simulation&#34;&gt;Giuseppe Tormen and Edmund Bertschinger, Adding long wavelenght modes to an N-body simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/full/1997MNRAS.286...38C&#34; target=&#34;_blank&#34; title=&#34;S. Cole, Adding long-wavelength power to N-body simulations&#34;&gt;S. Cole, Adding long-wavelength power to N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations: #4: Moving the particles!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-4-moving-the-particles/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-4-moving-the-particles/</guid>
      <description>

&lt;h3 id=&#34;equations-of-motion:0e7dc98bf378aec2e704ff7a998df960&#34;&gt;Equations of motion&lt;/h3&gt;

&lt;p&gt;Once we have learned how to calculate gravitational forces and decided which way is best for us, it&amp;rsquo;s time to move particles according to the force field.&lt;br /&gt;
To have an idea of what happens we can look at a simplified version of the pp-method, where the new coordinates and velocities are expressed starting from the previous values:&lt;/p&gt;

&lt;p&gt;$\mathbf{v}_i^{new}=\mathbf{v}_i^{old}+\frac{\mathbf{F}_i}{m_i}\Delta t$
$\mathbf{x}_i^{new}=\mathbf{x}_i^{old}+\mathbf{v}_i\Delta t$&lt;/p&gt;

&lt;p&gt;More in general we start from the Newtonian equation of motion for a set of particles interacting only through gravity, in particular we consider the Euler and Poisson equations. We express these in comoving coordinates because in this way we can focus on density and velocity perturbations, while the expansion of the universe is included in the scale factor $a$ (obtained from the Friedmann equations):&lt;/p&gt;

&lt;p&gt;$\frac{\mathrm{d}^2\mathbf{x}}{\mathrm{d}t^2}+2\frac{\mathrm{d}a/\mathrm{d}t}{a}\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=-\frac{1}{a^2}\nabla_x\phi$&lt;/p&gt;

&lt;p&gt;$\nabla_x^2\phi=4\pi Ga^2\bar \rho\delta = \frac{3}{2}H_0^2\Omega_0\frac{\delta}{a}$&lt;/p&gt;

&lt;p&gt;Here $\mathbf{x}=\frac{\mathbf{r}}{a(t)}$ is the comoving coordinate of a particle, with $\mathbf{r}$ the physical coordinate; $\phi$ is the gravitational potential due to the density perturbations, $H_0$ the Hubble constant and $\Omega_0$ the density parameter at the present time. In real simulations these equations are modified changing variables to simplify the integration but the idea remain the same. They are valid for non-relativistic matter and on scales smaller than the Hubble radius $c/H_0$. The expansion of the universe act as a viscous force that opposes to the gravitational infall slowing down the growth of perturbations.&lt;br /&gt;
Usually to integrate (read it as &amp;ldquo;to solve&amp;rdquo;!:P) the equations and obtain the new positions and velocities we use the Leap-Frog method because it minimizes the number of force evaluations. This is the most time consuming part of the code: Leap-Frog only requires one evaluation and the corresponding error is of the order of $\Delta t^3$. In this method positions and accelerations are computed at integer time steps and velocities at half time step: for example, positions are calculated at $t$ and velocities at $t+\Delta t/2$.&lt;/p&gt;

&lt;h3 id=&#34;time-stepping:0e7dc98bf378aec2e704ff7a998df960&#34;&gt;Time stepping&lt;/h3&gt;

&lt;p&gt;Choosing the simulation time step, that is the intervals between two calculation of positions and/or velocities, is crucial both for performances of the code and errors generation. Shorter time steps give smaller errors, but slow down the simulation.&lt;br /&gt;
Time steps depend on the distribution of particles and may change during the evolution of the simulation. Use of individual time steps for each particle may speed up calculations, because we can tune the time step on for different particles, increasing the time resolution for particles in denser regions and decreasing it in low density regions. In this way larger $\Delta t$ are sufficient. When we decide the time step we have also to take care of the conservation of momentum and evolution of energy if it&amp;rsquo;s not conserved (it can evolve according to the Irvine-Layzer equation).&lt;br /&gt;
There are different possibility for how to calculate the time step, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;we can take care of the validity of the Irvine-Layzer during the simulation; in this case we have to compute the force at mesh points and then interpolate it at particles positions but, because the force doesn&amp;rsquo;t equal the gradient of the potential and the correction requires a direct sum on all the particles, this is not a good method&lt;/li&gt;
&lt;li&gt;we can look at the convergence of velocities and final positions of particles using different time steps: reducing their duration we should approach the correct values&lt;/li&gt;
&lt;li&gt;the reproducibility of initial conditions may be it&amp;rsquo;s the most stringent criterion because it ensures that the results are correct: under certain conditions (linearity) running the particles forward and then back again we should get back the initial positions&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #1: M*</title>
      <link>http://brunettoziosi.eu/posts/phd-question-1-m/</link>
      <pubDate>Fri, 11 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-1-m/</guid>
      <description>&lt;p&gt;In parallel with the series &amp;ldquo;Cosmological simulations&amp;rdquo; I&amp;rsquo;m starting now another
series of posts about cosmology, astro/physics and related arguments.
It may happen that your PhD advisor ask you a question about something and you
are supposed to know the answer&amp;hellip; but you don&amp;rsquo;t! Or it may happen that you have
to pass an admittance/qualification exam to enter your PhD student career on
general astrophysical knowledge but you can&amp;rsquo;t even remember some arguments exist!
Because of these consideration and for my remembrance I will &amp;nbsp;write
down some of these questions and I will try to answer.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;These posts don&amp;rsquo;t pretend to be nor totally correct neither complete, but they
reflect the answers I have found with, maybe, some corrections by other students or professors.&lt;br /&gt;
&lt;br /&gt;
So let&amp;rsquo;s start with the first question: what is, how it is defined and how you can calculate &lt;code&gt;$M_\*$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;$M_*$ is the typical non-linear mass collapsing at the redshift we are considering.
This means that &lt;code&gt;$M_\*$&lt;/code&gt; is the typical mass of a perturbation that, at the time we
are looking, has the associated liner density contrast &lt;code&gt;$\delta(\mathbf{x})\sim1$&lt;/code&gt;,
or, in the formalism of the excursion set, pass the barrier of &lt;code&gt;$\delta_c=1.686$&lt;/code&gt;.&lt;br /&gt;
Starting from this and with the results of the linear theory we can obtain some
qualitative laws for the non-linear evolution.&lt;br /&gt;
From the linear theory we have that perturbations grow in a self-similar way
(&lt;code&gt;$\delta(\mathbf{x},t)=\delta(\mathbf{x})D(t)$&lt;/code&gt;, with &lt;code&gt;$D(t)$&lt;/code&gt; the growth factor)
and, in an Einstein-de Sitter universe (a spatially flat universe containing only
matter, the Friedmann universe in which the density exactly matches the critical one),
the growth factor is proportional to the scale factor.&lt;br /&gt;
Now, if we make a choice for the spectrum (scale-free spectrum), we can define the
typical non-linear mass that is collapsing as&lt;br /&gt;
&lt;code&gt;$$M_\*(t)\propto D(t)^{6/(3+n)}$$&lt;/code&gt;&lt;br /&gt;
that, in an Einstein-de Sitter universe, becomes&lt;br /&gt;
&lt;code&gt;$$M_\*(t)\propto a^{6/(3+n)}\propto (1+z)^{-6/(3+n)}$$&lt;/code&gt;&lt;br /&gt;
where &lt;code&gt;$n$&lt;/code&gt; is the spectral index.&lt;br /&gt;
From this we can derive other relations (still in the case of a EdS universe):&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;code&gt;$t_\*\propto (1+z)^{-3/2}$&lt;/code&gt; the typical time of formation for a structure of mass &lt;code&gt;$M_\*$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\rho_\*\propto (1+z)^3$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$R\propto M_\*^{1/3}(1+z)^{-1}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\langle v\rangle^2M_\*^{2/3}(1+z)$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #1: why and what?</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-1-why-and-what/</link>
      <pubDate>Wed, 02 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-1-why-and-what/</guid>
      <description>&lt;p&gt;This is the first of a series of posts dedicated to cosmological simulations!&lt;br /&gt;
&lt;br /&gt;
I do this because, as stressed by my PhD advisor,
I need to practice in explaining in a clear way specialized knowledge and in
linking it with its background and motivations. Also I would like to keep track
of my progress and of what I&amp;rsquo;m learning!&lt;br /&gt;
&lt;br /&gt;
So, let&amp;rsquo;s start with &amp;ldquo;Why we need cosmological simulations? What are they?&amp;rdquo;!&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the night sky: if we are so lucky to be in a dark site like
the mountains or a desert, we can see the stars, and our Galaxy, The Milky Way.
With a little telescope we can also see other galaxies, like Andromeda. We can
find them in group of galaxies or clusters of galaxies. On bigger scales these
form sheets and filaments as you can see in the figure (taken from the Millenium
simulation).&lt;br /&gt;
&lt;br /&gt;
&lt;p  align=&#34;center&#34; &gt;&lt;img alt=&#34;&#34; class=&#34;alignnone&#34; height=&#34;400&#34; src=&#34;../../files/supercube.jpg&#34; title=&#34;A figure of the output of the Milleniun Simulation&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;br /&gt;
&lt;br /&gt;
Theoretical models, widely accepted, say that these structures formed from initial
small density fluctuations in the matter, grew under their self gravity, lead by a
special type of matter that can interact only through gravity, called &amp;ldquo;dark matter&amp;rdquo;.&lt;br /&gt;
&lt;br /&gt;
A homogeneous and isotropic universe is described by the
&lt;a href=&#34;http://en.wikipedia.org/wiki/Friedmann_equations&#34; target=&#34;_blank&#34; title=&#34;Friedmann equations&#34;&gt;Friedmann equations&lt;/a&gt; in general relativity. Until the density fluctuations are small we can treat them as perturbations in a Friedmann universe. If the matter under consideration is non-relativistic (and it is!) and on scales smaller than those of the observable universe we can study the evolution of these perturbations in the Newtonian limit. We also consider gravity as the only interaction to be taken into account for now. On large scales (more than some &lt;a href=&#34;http://en.wikipedia.org/wiki/Parsec&#34; target=&#34;_blank&#34; title=&#34;megaparsecs&#34;&gt;kiloparsecs&lt;/a&gt;) this is not a bad approximation as gravity is the only interaction working efficiently in driving the evolution of the fluctuations on that scales.&lt;br /&gt;
&lt;br /&gt;
We have good analytic models for the evolution of these perturbation until the
density contrast (we call density contrast the quantity &lt;code&gt;$\delta(\vec x,t)=(\rho(\vec x, t)-\rho_{bg}(\vec x, t))/\rho_{bg}(\vec x, t)$&lt;/code&gt;
where &lt;code&gt;$\rho_{bg}(\vec x, t)$&lt;/code&gt; is the background density at given position and time)
is smaller than the unity.
This is the &amp;ldquo;linear regime&amp;rdquo;.
We call it &amp;ldquo;linear&amp;rdquo; because we can describe the system using first order
perturbations and the solutions we find are in good agreement with the exact solutions.
When the density contrast reaches and exceeds the unity, perturbations become
&amp;ldquo;non-linear&amp;rdquo; and the analytic models break.&lt;br /&gt;
&lt;br /&gt;
Cosmological N-body simulations are then the only way we have to study perturbations
in the non-linear regime. Note that when the fluctuations collapse forming what we
call a &amp;ldquo;halo&amp;rdquo;, the density contrast is of the order of 100. Cosmological simulations
are called &amp;ldquo;N-body&amp;rdquo; because they involve the calculation of the (gravitational)
force among all the bodies (particles) of the simulation.&lt;br /&gt;
&lt;br /&gt;
With cosmological simulations we can also play with the initial conditions of our
model of the universe, change its contents, &amp;hellip; and see what will happen. It&amp;rsquo;s the
closest thing to a laboratory that we have.&lt;br /&gt;
&lt;br /&gt;
Using different techniques is also possible to include non-gravitational effects
in the simulations, such as gas hydrodynamics, star formation, and so on.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;em&gt;Reference&lt;/em&gt;:&amp;nbsp;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>