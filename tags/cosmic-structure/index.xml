<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cosmic Structure on Post It!</title>
    <link>http://brunettoziosi.eu/tags/cosmic-structure/</link>
    <description>Recent content in Cosmic Structure on Post It!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Apr 2012 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://brunettoziosi.eu/tags/cosmic-structure/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>PhD question #4: calculate the value of M*</title>
      <link>http://brunettoziosi.eu/posts/phd-question-4-calculate-the-value-of-m/</link>
      <pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-4-calculate-the-value-of-m/</guid>
      <description>

&lt;p&gt;Some post ago &lt;a href=&#34;phd-question-1-m.html&#34;&gt;I&amp;rsquo;ve written about M*&lt;/a&gt;, the typical non-linear mass collapsing at the redshift we are considering. Now I have to find a value for it.&lt;/p&gt;

&lt;p&gt;I said that &lt;code&gt;$M^*$&lt;/code&gt; is the typical mass of a perturbation that, at the time we are looking, has the associated liner density contrast &lt;code&gt;$\delta(\mathbf{x})\sim1$, or, in the formalism of the excursion set, pass the barrier of &amp;amp;nbsp;$\delta_c=1.686$.     
This means that we are looking for a perturbation with&lt;/code&gt;$\sigma\simeq1.686$` and trying to quantify the mass it contains.&lt;/p&gt;

&lt;h2 id=&#34;sigma-and-r:96&#34;&gt;&lt;code&gt;$\sigma$&lt;/code&gt; and R&lt;/h2&gt;

&lt;p&gt;First of all we need to find the radius of a perturbation whose &lt;code&gt;$\sigma$&lt;/code&gt; reached the value of 1.686. To do this we can use the &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/the-initial-conditions-saga/&#34; target=&#34;_blank&#34; title=&#34;The “Initial Conditions” saga&#34;&gt;code&lt;/a&gt; developed to manage the CAMB files in order to find the matter power spectrum and its normalization. Then we add few lines to &amp;ldquo;sample&amp;rdquo; the &lt;code&gt;$\sigma(R)$&lt;/code&gt; distribution and find the radius of the perturbation reaching the excursion set barrier for the collapse.&lt;/p&gt;

&lt;h2 id=&#34;m:96&#34;&gt;&lt;code&gt;$M^*$&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Once we have the radius for which &lt;code&gt;$\sigma = \delta_c$&lt;/code&gt; we need to know the mean density in the universe to find &lt;code&gt;$M^*$&lt;/code&gt; with:&lt;/p&gt;

&lt;p&gt;$M^* = \frac{4}{3}\pi R_*^3\rho_m$`&lt;/p&gt;

&lt;p&gt;&lt;del&gt;I don&amp;rsquo;t know why we only need to use &lt;code&gt;$\rho_m$&lt;/code&gt; and not &lt;code&gt;$\rho_m\delta$&lt;/code&gt; or something similar is not clear to me, but it&amp;rsquo;s correct.&lt;/del&gt;We use this formula because &lt;code&gt;$M^*$&lt;/code&gt; is a quantity related to the linear perturbations. It&amp;rsquo;s correct because the difference between a linear and a non-linear perturbation is the value of the density contrast, but the mass is the same. In other words, the mass of a perturbation is the same both in the linear and in the non-linear evolution, but linear perturbations have smaller density contrasts and larger radii, non-linear perturbations instead have larger density contrasts and smaller radii. To be precise, the previous equation can be written:&lt;br /&gt;
$M^* = \frac{4}{3}\pi R&lt;em&gt;*^3\rho&lt;/em&gt;{bg}(1+1.686) = \frac{4}{3}\pi R&lt;em&gt;{vir}^3\rho&lt;/em&gt;{bg}(1+200)$.&lt;br /&gt;
To obtain &lt;code&gt;$\rho_m$&lt;/code&gt; we find the value of the critical density &lt;code&gt;$\rho_c$&lt;/code&gt; and multiply it for `$\Omega = \rho_m / \rho_c$. These two values can be obtained from books (&lt;a href=&#34;http://www.amazon.com/Cosmology-Prof-Peter-Coles/dp/0471489093/ref=ntt_at_ep_dpt_2&#34; target=&#34;_blank&#34; title=&#34;Coles &amp;amp; Lucchin&#34;&gt;Lucchin&lt;/a&gt;, &lt;a href=&#34;http://www.amazon.com/Galaxy-Formation-Evolution-Houjun-Mo/dp/0521857937/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1333458377&amp;amp;sr=1-1&#34; target=&#34;_blank&#34; title=&#34;Mo, van den Bosch &amp;amp; White&#34;&gt;Mo&amp;amp;White&lt;/a&gt; for example) or in the &lt;a href=&#34;http://lambda.gsfc.nasa.gov/product/map/dr2/params/lcdm_wmap.cfm&#34; target=&#34;_blank&#34; title=&#34;WMAP data page&#34;&gt;WMAP data page&lt;/a&gt;. In the second case we prefer to use the single data fit because it&amp;rsquo;s simpler to refer to it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import time
import numpy as np
import matplotlib.pyplot as plt
import random as rnd
from scipy import integrate

&amp;quot;&amp;quot;&amp;quot; Calculate M* . M* is propto the mass contained in the radius for which 
s_8=delta_c refer to: 
http://www.brunettoziosi.eu/blog/wordpress/the-initial-conditions-saga/
http://www.brunettoziosi.eu/blog/wordpress/phd-question-3-calculate-the-value-of-m/
&amp;quot;&amp;quot;&amp;quot;

t = time.time()

#===============================================================================
#    Compute sigma
#===============================================================================

### Load data from the nasa-CAMB file 

# matterpower is the file with the k and the total matter spectrum
# transfer is the file with the k and the transfer function for the various 
#species, the 6th column is for the total matter (baryons+DM)
transfer = np.genfromtxt(&#39;camb_88704620_transfer_out_z0.dat&#39;, usecols = (0,6))
#matterpower = np.genfromtxt(&#39;2012-01-30_data/camb_88704620_matterpower_z0.dat&#39;)

# CAMB CDM transfer output
camb_k = transfer[:,0]
camb_tf = transfer[:,1] 

R = 8 #Mpc/h
s_8 = 0.9#0.8118405 #from WMAP7 but we need the values for the Millennium-2, so 
# we use its s_8
sp_ind = 1
delta_c = 1.686

### Calculate the amplitude to normalize the spectrum:
### P(k) = Ak^nT^2(k) 

# FT of the window function (spherical top-hat)
def FTW(R, k): 
	&amp;quot;&amp;quot;&amp;quot; Return the Fourier transform of the window function 
	(spherical top-hat)
	&amp;quot;&amp;quot;&amp;quot;
	return 3.*(np.sin(k*R)-k*R*np.cos(k*R)) / (k*R)**3

def spectrum():
	&amp;quot;&amp;quot;&amp;quot;Calculate the power spectrum given the transfer function and the FT of the window 
	function.
	&amp;quot;&amp;quot;&amp;quot;
	# camb_k**(2+sp_ind) that is k^(2+n) because d^3k=4pi k^2dk
	amp_integrand = camb_k**(2+sp_ind)*camb_tf**2 * FTW(R, camb_k)**2
	amp_integral = integrate.trapz(amp_integrand, camb_k)
	# Amplitude for s_8 = 1
	amp_0 = 2*np.pi**2/amp_integral
	# Amplitude
	amp = amp_0*s_8**2  # 9.9197881817e-09
	#print amp
	# Calculate the power spectrum
	return camb_k**sp_ind*camb_tf**2 * amp

# Calculate the power spectrum
ps = spectrum()

# Calculate sigma on the radii
def sigma(R):
	&amp;quot;&amp;quot;&amp;quot;Return the sigma for the current radius.
	&amp;quot;&amp;quot;&amp;quot;
	return pow(integrate.trapz(camb_k**2 * ps * FTW(R, camb_k)**2, camb_k)/(2*np.pi**2), 0.5)

#===============================================================================
#    Find the radius containing M*
#===============================================================================

# Initialize some variables
neigh = np.ones(2) # two nearest neighbours sigmas
r_min = 10**(-2) # min r to sample
r_max = 10**2 # max r ti sample
i = 0 # loop counter

# While stops when the computed sigma is less then 0.001 from delta_c
while np.abs(np.amin(delta_c+neigh)) &amp;amp;gt; 0.001:
print &amp;quot;Loop &amp;quot;, i
i+=1
print &amp;quot;Condition start &amp;quot;, np.abs(np.amin(delta_c+neigh))
# radii to be sampled
r = np.linspace(r_min, r_max, num=100)
# Compute sigma for those radii, the minus sign is to avoid resorting of the
# array to be used by np.searchsorted
s_r = -np.asarray(map(sigma, r))
# Find the two nearest neighbours
neigh[0] = np.amax(s_r[s_r  -delta_c])
# Find the corresponding radii
r_min = r[np.searchsorted(s_r, neigh).min()]
r_max = r[np.searchsorted(s_r, neigh).max()]
print &amp;quot;Sigmas&amp;quot;, -neigh[0], -neigh[1]
print &amp;quot;Radii [Mpc/h] &amp;quot;, r_min, r_max
print &amp;quot;Condition end &amp;quot;, np.abs(np.amin(delta_c+neigh))

# Selected values
s_star = neigh[np.argmin(delta_c+neigh)]
r_star = r[np.searchsorted(s_r, s_star)]
deviation = np.abs(np.amin(delta_c+neigh))

print &amp;quot;############################################&amp;quot;

print &amp;quot;Selected sigma &amp;quot;, -s_star
print &amp;quot;Selected radius [Mpc/h] &amp;quot;, r_star

print &amp;quot;Calculate M* using:&amp;quot;
print &amp;quot;Gt4.299 x 10^(-9) Mpc /M_sun (km/s)^2tfrom Mo&amp;amp;amp;White&amp;quot;
print &amp;quot;Ht100*h^2&amp;quot;
print &amp;quot;Omega_mt0.25tfrom the Millennium-2 simulation&amp;quot;
print &amp;quot;Omega_mt0.241tfrom WMAP7&amp;quot;

#===============================================================================
#    Cosmological parameters and find the mean density in the Universe
#===============================================================================

H = 100
h = 0.732 #WMAP http://lambda.gsfc.nasa.gov/product/map/dr2/params/lcdm_wmap.cfm
G = 4.299*10**(-9)
omega_m_mill = 0.25
omega_m_WMAP = 0.241

# Until here it&#39;s correct
rho_c = 3*H**2/(8*np.pi*G) # 2.7766040316101764 * h**2 x 10^11 M_sun/Mpc^3
	# 2.778 from Lucchin book
	# 2.775 from Mo&amp;amp;amp;White book

rho_mean_mill = rho_c * omega_m_mill # 6.94151007903 * h**2 x 10^10 M_sun/Mpc^3 = 3.71942769658 x 10^10 M_sun/Mpc^3
	rho_mean_WMAP = rho_c * omega_m_WMAP # 6.69161571618 * h**2 x 10^10 M_sun/Mpc^3 = 3.58552829951 x 10^10 M_sun/Mpc^3

print &amp;quot;rho_c = 3H^2/8 pi G = &amp;quot;, rho_c,&amp;quot; h^2 M_sun/Mpc^3&amp;quot;
print &amp;quot;Millennium-2 rho_mean = omega_m_mill * rho_c = &amp;quot;, rho_mean_mill, &amp;quot; h^2 M_sun/Mpc^3 = &amp;quot;, rho_mean_mill*h**2
print &amp;quot;WMAP7 rho_mean = omega_m_WMAP * rho_c = &amp;quot;, rho_mean_WMAP, &amp;quot; h^2 M_sun/Mpc^3 = &amp;quot;, rho_mean_WMAP*h**2 

#===============================================================================
#    Compute M*
#===============================================================================

M_star_mill = np.pi * r_star**3 * rho_mean_mill * 4./3#* (delta_c + 1)
M_star_WMAP = np.pi * r_star**3 * rho_mean_WMAP * 4./3#* (delta_c + 1)

print &amp;quot;M* Millennium-2 &amp;quot;, M_star_mill, &amp;quot; M_sun/h&amp;quot; # 4.81467115575e+12 M_sun/h
print &amp;quot;M* WMAP &amp;quot;, M_star_WMAP, &amp;quot; M_sun/h&amp;quot; # 4.64134299414e+12 M_sun/h

#===============================================================================
#    Compare with Hayashi&amp;amp;amp;White 2008 article
#===============================================================================

print &amp;quot;Hayashi&amp;amp;amp;White&#39;s value:&amp;quot;

M_star_white = 6.15*10**(12)
omega_m_white = 3.*M_star_white/(4*np.pi*r_star**3*rho_c)

print &amp;quot;M*: &amp;quot;, M_star_white
print &amp;quot;Omega_m&amp;quot;, omega_m_white

r_s = pow(3.*M_star_white/(4*np.pi*rho_mean_mill), 1./3)
s_white = sigma(r_s)

print &amp;quot;As alternative:&amp;quot;
print &amp;quot;R* &amp;quot;, r_s
print &amp;quot;Sigma &amp;quot;, s_white

#===============================================================================
#    Summary
#===============================================================================

print &amp;quot;&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;SUMMARY&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;&amp;quot;
print &amp;quot;WhottttM*ttR*ttOmegattSigmattSigma-delta_c&amp;quot;
print &amp;quot;--------------------------------------------------------------------------------------------------------------&amp;quot;
print &amp;quot;Hayashi&amp;amp;amp;White given Rtt{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_white,r_star,omega_m_white,-s_star,deviation)
print &amp;quot;Hayashi&amp;amp;amp;White given Omegat{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_white,r_s,omega_m_mill,s_white,np.abs(s_white-delta_c))
print &#39;Me WMAP datattt{:e}t{:e}t{:e}t{:e}t{:e}&#39;.format(M_star_WMAP,r_star,omega_m_WMAP,-s_star,deviation)
print &amp;quot;Me Millennium-2 datatt{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_mill,r_star,omega_m_mill,-s_star,deviation)
print &amp;quot;&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;&amp;quot;
print &amp;quot;Done in &amp;quot;, time.time()-t
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The differences between the different values of &lt;code&gt;$M^*$&lt;/code&gt; are acceptable and probably depends on different integration boundaries for `$\sigma$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #9: Gadget-2 (N-body part)</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</link>
      <pubDate>Mon, 20 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</guid>
      <description>&lt;p&gt;Here I would like to do a brief presentation of the main features of Gadget-2.&lt;br /&gt;
Gadget-2 (&lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; target=&#34;_blank&#34; title=&#34;Gadget2 homepage&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/my-first-gadget2-tests/&#34; target=&#34;_blank&#34; title=&#34;My first Gadget-2 tests&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2005.09655.x/abstract;jsessionid=DED86CDB5CD8A572F3631F0C42828086.d01t03&#34; target=&#34;_blank&#34; title=&#34;Gadget-2 paper&#34;&gt;here&lt;/a&gt;) is a cosmological simulation code developed primarily by &lt;a href=&#34;http://www.mpa-garching.mpg.de/~volker/&#34; target=&#34;_blank&#34; title=&#34;Volker Springel&#39;s homepage&#34;&gt;Volker Springel&lt;/a&gt;. It is a &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/cosmological-simulations-3-calculating-the-force/&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #3: force calculation!&#34;&gt;TreePM&lt;/a&gt; code so it splits forces between long-range (PM part) and short-range (tree part using multipole expansion to approximate the force of distant particles groups).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The tree&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gadget-2 uses a&lt;a href=&#34;http://en.wikipedia.org/wiki/Octree&#34; target=&#34;_blank&#34; title=&#34;oct-tree&#34;&gt; BH oct-tree&lt;/a&gt; (see also &lt;a href=&#34;http://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation&#34; target=&#34;_blank&#34; title=&#34;Barnes&amp;amp;Hut simulation on wikipedia&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.artcompsci.org/~makino/softwares/C++tree/index.html&#34; target=&#34;_blank&#34; title=&#34;NBODY, an implementation of Barnes-Hut treecode&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://ifa.hawaii.edu/~barnes/software.html&#34; target=&#34;_blank&#34; title=&#34;Barnes&#39; page&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.cita.utoronto.ca/~dubinski/treecode/treecode.html&#34; target=&#34;_blank&#34; title=&#34;A parallel tree code explenation&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.prism.gatech.edu/~gth716h/BNtree/&#34; target=&#34;_blank&#34; title=&#34;Barnes-Hut Implementation in HTML/Javascript&#34;&gt;here&lt;/a&gt;) to calculate the short-range forces in the real space. This choice was done because this type of tree, compared to other types (KD-Tree, &amp;hellip;), requires the creation of less nodes, that imply that less memory is used. It&amp;rsquo;s characterized by eight sub-nodes for each node and has only one particle in each leaf. The code decides to open a leaf according to a certain leaf opening criterion based on the estimated force error. The force for distant groups of particles is approximated with the multipole (here octopole) of the tree node and the error depends on the dimensions and the distances of the node considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PM part&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The PM part of the code is used to calculate the long-range forces. The algorithm is something like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CIC (cloud-in-cell) assignment is used to construct the mass density field on to the mesh from the information on the particles&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the discrete FT of the mesh is multiplied for the Green function for the potential in periodic boundaries (modified with the exponential truncation for the force splitting)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;deconvolution for the CIC kernel twice: the first for the smoothing effect of CIC assignment, the second for the force interpolation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$\mathrm{FT}^{-1}$ to obtain the potential on the mesh&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;finite differentiate the potential to obtain the forces&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;interpolate the forces to the particles positions using CIC&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: real-to-complex FT are used to save times and memory respect to full complex transforms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time step&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This type of code has a large dynamic range in time scale, from the denser regions where the evolution is rapid to the less denser regions in which the evolution occur slower so we can describe it with larger time resolution. In this scenario evolving all particles with the smallest time-scale is a waste of time and computational resources. Because using different time-steps for each particle add instabilities to the system, Gadget-2 separates time-step between long-range (longer time step) and short-range (shorter time step) force computations. The perturbation of the system for different time-steps is related to the symplectic nature of the system, but I still have not understood what it really means and implies! I know that it refers to the phase space volume and has effect on the information conservation. May be in the future I&amp;rsquo;ll write a post about this!&lt;br /&gt;
Despite these arguments, sometimes individual time step are allowed because they perturb the system but not the symplecticity of the single particle.&lt;br /&gt;
In the normal integration mode time-steps are discretized in a power of two hierarchy and particles can always move to smaller time steps but to longer time steps only in subsequent step, synchronized with higher time-steps. Alternatively the code can populate time-steps discretizeing them as integer multiples of the minimum time-step among the particles set. This lead to a more homogeneous distribution of particles across the time-line which can simplify work load balancing.&lt;br /&gt;
The integration is performed using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Leapfrog_integration&#34; target=&#34;_blank&#34; title=&#34;Leapfrog method&#34;&gt;leapfrog method&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually the parallelization distributes particles across the CPUs using an orthogonal domain decomposition but in this way the trees built-in each domain depend on the domain geometry. Because the force depend on the tree (through the multipole expansion of the mass distribution) the force can be different if you change the number of processors.&lt;br /&gt;
Gadget-2 introduce a space-filling fractal, the Peano-Hilber (PH) curve to map 3D space into a 1D curve that encompasses all the particles. Now the PH curve can be cut and each piece assigned to a CPU and in this way the force is independent of the processors number. If you cut every segment in eight pieces recursively you find again the tree decomposition, so there is a close correspondence between the decomposition obtained with the BH oct-tree and that of the PH curve.&lt;br /&gt;
The PH curve has some remarkably properties, for example points that are close along the 1D PH curve are in general close in 3D space, so the mapping preserves locality and if we cut the PH curve into segments of a certain length we obtain a domain decomposition which has the property that the spatial domains are simply connected and quite &amp;ldquo;compact&amp;rdquo; (i.e., they tend to have small surface-to-volume ratios and low aspect ratio, a highly desirable property for reducing communication costs with neighbouring domains and for speeding up the local computation).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Operations scheme&lt;/strong&gt;&lt;br /&gt;
Here a brief scheme on how the short range force calculation works on multiple processors. The PM computation uses the &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_4.html&#34; target=&#34;_blank&#34; title=&#34;Parallel FFTWs&#34;&gt;parallel FFTWs&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Compute the PH key for each particle&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sort the keys locally and split the PH curve into segments&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adjust the sorted segments to a global sort, splitting and joining segments if needed, with little communication&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Assign the particles to the processes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Construct a BH tree for the particles of each processors representing particles on other processors with pseudo-particles (acting like placeholders)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During the tree traverse (e.g. in processor A) these pseudo-particles cannot be opened, the are flagged and inserted into a list that collects all the particles that are to be sent (=requested) to the other processors (e.g. to processor B)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Processor B traverse again its local tree and send back the resulting force contribution to processor A&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #2: spherical collapse</title>
      <link>http://brunettoziosi.eu/posts/phd-question-2-spherical-collapse/</link>
      <pubDate>Tue, 10 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-2-spherical-collapse/</guid>
      <description>&lt;p&gt;With this post I would like to collect and present in a simple and consistent
form some of the various analytical derivation of the spherical collapse
model, in particular the challenge was to find how to obtain the famous
$\delta_{lin}\sim1.686$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/spherical_collapse2.pdf&#34;&gt;Here&lt;/a&gt; you can find a pdf file with the analytic computation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My first Gadget2 tests</title>
      <link>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</link>
      <pubDate>Sat, 07 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</guid>
      <description>

&lt;p&gt;This post is about my first experience with the cosmological simulation code &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget2&#34;&gt;Gadget2&lt;/a&gt;. To start I followed the instructions found &lt;a href=&#34;http://astrobites.com/2011/04/02/installing-and-running-gadget-2/&#34;&gt;here&lt;/a&gt;. All I&amp;rsquo;m going to write refers to an Ubuntu/Kubuntu 11.10 installation.&lt;/p&gt;

&lt;h2 id=&#34;installation-of-gsl-and-fftw:103&#34;&gt;Installation of GSL and fftw&lt;/h2&gt;

&lt;p&gt;We can download Gadget &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget download&#34;&gt;here&lt;/a&gt;, the GSL (GNU scientific library) &lt;a href=&#34;http://mirror.rit.edu/gnu/gsl/gsl-1.9.tar.gz&#34; title=&#34;GSL download&#34;&gt;here&lt;/a&gt; and the FFTW (fastest Fourier transform in the West library) &lt;a href=&#34;http://www.fftw.org/fftw-2.1.5.tar.gz&#34; title=&#34;FFTW download&#34;&gt;here&lt;/a&gt;. We also need an MPI library (Open-MPI or MPICH, try install it using your package manager).&lt;br /&gt;
Following the Astrobites suggestions let&amp;rsquo;s decompress the archives with &lt;code&gt;tar -xzf &amp;lt;archive name&amp;gt;&lt;/code&gt;. Now we can install the libraries following the Astrobites post:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd gsl-1.9/
goldbaum@~/Documents/code/gsl-1.9: ./configure
snip: lots of diagnostic ouput
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix --enable-float
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As described &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_6.html#SEC69&#34; target=&#34;_blank&#34; title=&#34;FTTW installation and customization&#34;&gt;here&lt;/a&gt; is convenient to install both the single and the double precision version of the FFTW (for example to compile the initial conditions generators) with (that is, without &lt;code&gt;--enable-float&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;play-with-gadget2:103&#34;&gt;Play with Gadget2&lt;/h2&gt;

&lt;p&gt;Now it&amp;rsquo;s time to play with Gadget!:) In this code, for performance reasons, requires to specify some parameters at compile time while other can be set at run time, so that we have to customize the Makefile. This also imply that we should have separate binary files and directories for each simulation.&lt;br /&gt;
To start with something easy, we will customize one of the examples given with the code, the &amp;ldquo;galaxy&amp;rdquo; one. It simulate the collision of two galaxies using 40000 DM particles for the haloes and 20000 baryonic particles for the disks.&lt;/p&gt;

&lt;p&gt;Inside the Gadget directory we have&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Analysis
AUTHORS
COPYING
COPYRIGHT
Documentation
Gadget2
ICs
README
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the &lt;code&gt;Analysis&lt;/code&gt; folder we can fin some analysis routines provided by the author, the &lt;code&gt;Documentation&lt;/code&gt; folder contains the user guide and the original paper, and the &lt;code&gt;AUTHORS, COPYING, COPYRIGHT&lt;/code&gt; self-explanatory. The &lt;code&gt;ICs&lt;/code&gt; folder contains the initial conditions for the example simulations and the &lt;code&gt;Gadget2&lt;/code&gt; folder contains the sources and the html documentation.&lt;/p&gt;

&lt;p&gt;To be tidy and organized is better to have a folder for every simulations, so we will create a (descriptive) with everything we need to customize&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir 2012-01-07-Gadget2-galaxy_test_01
cd 2012-01-07-Gadget2-galaxy_test_01
mkdir out
cp ../ICs/galaxy_littleendian.dat ./
cp ../Gadget2/parameterfiles/galaxy.param ../Gadget2/parameterfiles/galaxy.Makefile ./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the folder Gadget2 we can find the general &lt;code&gt;Makefile&lt;/code&gt; but for now let&amp;rsquo;s use the galaxy&amp;rsquo;s one provided by the author and just copied to our position. Open it with your preferred text editor (for example, in a command line environment, &lt;code&gt;emacs -nw Makefile&lt;/code&gt;).&lt;br /&gt;
This &lt;code&gt;Makefile&lt;/code&gt; is already customized for the galaxy collision simulation and if you want to understand every option you can read the description in the guide, but we need some more customization. Here what I&amp;rsquo;ve changed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPT   +=  -DHAVE_HDF5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so I activate the HDF5 format for the output and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#--------------------------------------- Select target computer

SYSTYPE=&amp;quot;Uno&amp;quot;
\#SYSTYPE=&amp;quot;MPA&amp;quot;
\#SYSTYPE=&amp;quot;Mako&amp;quot;
\#SYSTYPE=&amp;quot;Regatta&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster-gcc&amp;quot;
\#SYSTYPE=&amp;quot;Opteron&amp;quot;

\#--------------------------------------- Adjust settings for target computer

ifeq ($(SYSTYPE),&amp;quot;Uno&amp;quot;)
CC       =  mpicc   
OPTIMIZE =  -O3 -Wall
GSL_INCL =  -I/usr/local/include
GSL_LIBS =  -L/usr/local/lib
FFTW_INCL=  -I/usr/local/include
FFTW_LIBS=  -L/usr/local/lib
MPICHLIB =  -L/usr/lib
HDF5INCL =  
HDF5LIB  =  -lhdf5 -lz 
endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to select the set the options for my system.&lt;br /&gt;
Now we have to customize the &lt;code&gt;run/galaxy.param&lt;/code&gt; file changing it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;InitCondFile      ./galaxy_littleendian.dat
OutputDir          ./galaxy_out/
OutputListFilename ./out/output_list.txt
SnapFormat         3  %to select the HDF5 format
TimeBegin           0.0        % Begin of the simulation
TimeMax             40.0        % End of the simulation

% Output frequency
TimeBetSnapshot        0.1% original 0.5 &amp;lt;/pre&amp;gt;
    
    
Now we should go to the sources folder and compile the code with    
&amp;lt;pre&amp;gt;cd ../Gadget2
make -f 2012-01-07-Gadget2-galaxy_test_01/galaxy.Makefile
cp Gadget2 ../2012-01-07-Gadget2-galaxy_test_01/Gadget2
make clean
cd 2012-01-07-Gadget2-galaxy_test_01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command clean the build leaving only the sources files, so we are ready for a new build.&lt;br /&gt;
We can also create a script for automatize all this steps, something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
dir=$1
ics=$2
param_file=$3
mk_file=$4
CPUs=$5

if [ $# -lt 5 ] ; then
  echo &amp;quot;usage: gadget_set directory_name initial_conditions_file
parameters_file make_file number_of_CPUs&amp;quot;
  exit 0
fi

echo &amp;quot;Assuming to use $dir as the run folder,&amp;quot; 
echo &amp;quot;$ics as initial conditions,&amp;quot;
echo &amp;quot;$paramfile as parameter file, &amp;quot;
echo &amp;quot;$mk_file as makefile &amp;quot;
echo &amp;quot;and to run on $CPUs CPUs.&amp;quot;

mkdir $dir
cd $dir
mkdir out
cp ../ICs/$ics ./
cp ../Gadget2/parameterfiles/$param_file
../Gadget2/parameterfiles/mk_file ./
cd ../Gadget2
make -f ../$dir/$mk_file
cp Gadget2 ../$dir/Gadget2
make clean
cd $dir
mpirun -np $CPUs ./Gadget2 $param_file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very raw and untested script, but it&amp;rsquo;s just to give an idea.&lt;/p&gt;

&lt;p&gt;Now we are ready to start the simulation with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mpirun -np 2 ./Gadget2 galaxy.param
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;-np&lt;/code&gt; sets the number of processes/processors to be used in parallel.&lt;br /&gt;
When the simulation stops we can analyze it with the tools provided in the &lt;code&gt;Analysis&lt;/code&gt; folder or, if you like me don&amp;rsquo;t own an IDL license and don&amp;rsquo;t feel comfortable with IDL/Fortran/C for the data analysis, with something like (to be run in out/plots/):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import sys, os
from subprocess import Popen, PIPE
from multiprocessing import Process, Queue

import numpy as np
import tables as tb
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

&amp;quot;&amp;quot;&amp;quot;This script will plot in parallel the .h5 snapshots created by Gadget2 test
runs one after the other!:).
FIXME: i need a way to wait for the final time count the end of the processes
and a way to print the status
&amp;quot;&amp;quot;&amp;quot;

# Set the max number of processes
n_procs = 3

# Set the number of snapshot to be plotted
n_snap = 401

t = time.time()

print &amp;quot;Defining workers...&amp;quot;

def worker(input, output):
    while input.qsize() != 0:
        item = input.get()
        if item[0]= 10 and item[0]&amp;amp;lt;100: j=&amp;quot;0&amp;quot;+str(item[0])
        else: j=str(item[0])
        try:
#     print &amp;quot;considering file ../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;
#     print &amp;quot;open file &amp;quot;
            h5 = tb.openFile(&amp;quot;../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;, &#39;r&#39;)
#     print &amp;quot;file opened, set variables&amp;quot;
            halo = h5.root.PartType1
            disk = h5.root.PartType2
#            print &amp;quot;setted, inizialize figure&amp;quot;
            fig2 = plt.figure()
            ax = Axes3D(fig2)
            ax.scatter(disk.Coordinates[:,0], 
                       disk.Coordinates[:,1],
                       disk.Coordinates[:,2],
                       color=&#39;red&#39;, s=0.5)
            ax.scatter(halo.Coordinates[:,0], 
                       halo.Coordinates[:,1],
                       halo.Coordinates[:,2],
                       color=&#39;blue&#39;, s=0.01)
            plt.savefig(&#39;snap_&#39;+j)
#            print &amp;quot;done, closing file&amp;quot;
            h5.close()  
#            print &amp;quot;closed&amp;quot;

        except:
            print &amp;quot;Work &amp;quot;+j+&amp;quot; not done, exit...&amp;quot;
            sys.exit()

def fill_queue(task_queue):
    for i in range(n_snap):
        task_queue.put([i])
    return task_queue

def status(proc):
    if proc.is_alive==True:
        return &#39;alive&#39;
    elif proc.is_alive==False:
        return &#39;dead&#39;
    else:
        return proc.is_alive()

print &amp;quot;Define queues...&amp;quot;

input_queue = Queue()
output_queue = Queue()

try:
    input_queue = fill_queue(input_queue)
except:
    print &amp;quot;Queue not filled, exit...&amp;quot;
    sys.exit()

procs = []

try:
    for i in range(n_procs):
        procs.append(Process(target=worker, args=(input_queue,
output_queue)))
except:
    print &amp;quot;Creating processes not complete, exit...&amp;quot;
    sys.exit()

try:
    for i in procs:
        i.start()
except:
    print &amp;quot;Start processes not complete, exit...&amp;quot;
    sys.exit()

for i in procs:
    print &amp;quot;Process &amp;quot;, i,&amp;quot; @ &amp;quot; , i.pid, &amp;quot; is &amp;quot;, status(i)

print &amp;quot;Done in &amp;quot;+str(time.time()-t)+&amp;quot; seconds.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have one image for each snapshot, and if we are interested we can produce a video with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mencoder mf://*.png -mf fps=25:type=png -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -vf scale=720:360 -oac copy -o output.mp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;youtube http://www.youtube.com/watch?v=b7HyafKMkxI&amp;amp;amp;w=560&amp;amp;amp;h=315&#34;&gt;This&lt;/a&gt; is the first basic video, with logarithmic time and perhaps there&amp;rsquo;s something wrong with the coordinates on the axes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GIF2 substructures coordinates correction</title>
      <link>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</link>
      <pubDate>Sun, 04 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</guid>
      <description>&lt;p&gt;I used this script to change the coordinates of the substructures in the GIF2 simulation output from the center of mass coordinates to the global ones. The substructures were stored in our server in files referring to the index of the halo to which they belong and their coordinates were respect to the center of the halo. For each halo this script read the subhaloes center of mass coordinates in kpc and change them to global coordinates in Mpc managing the periodic boundary conditions .&lt;/p&gt;

&lt;!-- TEASER_END --&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import numpy as np
import tables as tb
import time
import sys

&amp;quot;&amp;quot;&amp;quot;Legge il file con id e centri degli aloni con sottostrutture, per
ogni alone (id) apre il file Sub.3..053.gv, legge le coordinate
alle colonne 8, 9, 10 (in kpc!!!) e le corregge (tenendo conto delle
condizioni periodiche) con le coordinate del centro prese dalla lista
degli aloni iniziale.  Le coordinate vengono aggiunte ad un vettore
che alla fine viene salvato in un file hdf5.
&amp;quot;&amp;quot;&amp;quot;

t = time.time()
print &amp;quot;Start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As usual: imports, documentation and timing initialization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;halo_centers_file = &#39;haloes_with_substructures_centers.h5&#39;

h5 = tb.openFile(halo_centers_file, &#39;r&#39;)
haloes = h5.root.data.read()
h5.close()

haloes = haloes.astype(float)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code reads the halo centers from a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;substructure_coord = np.empty((0, 3))
files_num = haloes[:, 0].shape[0]
void_files = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we create the first (void) record of the substructures coordinates table, find the number of haloes and create a variable which will tell us how many haloes without subtructures we have.&lt;/p&gt;

&lt;p&gt;Now, for each halo we&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in xrange(files_num):
    print &amp;quot;Loop &amp;quot;, i, &amp;quot; di &amp;quot;, files_num 
    sub_file = &amp;quot;cm/Sub.3.&amp;quot;+&#39;%07d&#39;%int(haloes[i,0])+&amp;quot;.053.gv&amp;quot;
    ````
    
open the related substructures file     
````python
try:
        sub_coord = np.genfromtxt(sub_file, dtype=&#39;float&#39;, usecols=(8, 9, 10))
        file_check = True
    except:
        print &amp;quot;Void file &amp;quot;, sub_file
        file_check = False
        void_files += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;try to read the substructures coordinates, if it fails, we assume that the file is empty (and the halo has no substructures). In this case we increment the counter of the empy files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if file_check:
        if sub_coord.ndim == 1:
            sub_coord = sub_coord.reshape((1, sub_coord.shape[0]))
            #print &amp;quot;sh min &amp;quot;, np.amin(sub_coord, 0)
            #print &amp;quot;sh min &amp;quot;, np.amax(sub_coord, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This reshape the array in case we have only one subhalo: in this case (I hope I remember correct!:P) instead of one row and three columns we should have three values, so we have to reshape the array to pass it to the rest of the code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
            sub_x = sub_coord[:, 0]/1000. + haloes[i,1]
            if not np.all(sub_x &amp;amp;gt; 0):
                sub_x[sub_x&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_x 110]-=110
            if not (np.all(sub_x &amp;amp;gt; 0) and np.all(sub_x  0)
                print sub_x 
                print haloes[i, 1:3]
                sys.exit()
            sub_y = sub_coord[:, 1]/1000. + haloes[i,2]
            if not np.all(sub_y &amp;amp;gt; 0):
                sub_y[sub_y&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_y 110]-=110
            if not (np.all(sub_y &amp;amp;gt; 0) and np.all(sub_y  0)
                print sub_y
                print haloes[i, 1:3]
                sys.exit()
            sub_z = sub_coord[:, 2]/1000. + haloes[i,3]
            if not np.all(sub_z &amp;amp;gt; 0):
                sub_z[sub_z&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_z 110]-=110
            if not (np.all(sub_z &amp;amp;gt; 0) and np.all(sub_z  0)
                print sub_z 
                print haloes[i, 1:3]
                sys.exit()
            substructure_coord = np.vstack((substructure_coord, np.hstack((sub_x.reshape((sub_x.shape[0], 1)), 
                                                                           sub_y.reshape((sub_y.shape[0], 1)), 
                                                                           sub_z.reshape((sub_z.shape[0], 1))))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This corrects the coordinates keeping in mind the periodic boundary conditions and add the new substructures coordinates to the corrected substructures array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;except:
            print &amp;quot;file &amp;quot;, sub_file
            print &amp;quot;sub_coord.shape &amp;quot;, sub_coord.shape
            print &amp;quot;sub_coord &amp;quot;, sub_coord
            print &amp;quot;haloes coord &amp;quot;, haloes[i, :]
            print &amp;quot;exit&amp;quot;
            sys.exit()
    else:
        pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If something goes wrong, we handle the failure printing some information.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5 = tb.openFile(&#39;sub_haloes_global_coords.h5&#39;, &#39;w&#39;)
h5.createArray(h5.root, &#39;data&#39;, substructure_coord)
h5.flush()
h5.close()
print &amp;quot;Done in &amp;quot;, time.time()-t, &amp;quot; with &amp;quot;, void_files, &amp;quot; void files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, we save the array in an HDF5 file!:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #6: adding gas!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-6-adding-gas/</link>
      <pubDate>Tue, 29 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-6-adding-gas/</guid>
      <description>&lt;p&gt;As we have seen, cosmological N-body simulations consider only gravitational interactions. This is good for large scale distribution of matter such as galaxy distribution or cluster formation, but if we are interested in smaller details we have to add gas physics.&lt;br /&gt;
There are two main approaches, completely different one from the other.&lt;/p&gt;

&lt;p&gt;The first approach uses a grid to solve fluid equations. Fluid interactions are short range ones so we can use information from few nearby points to compute and evolve the quantities we are interested in at a point. Fluid must also respond to the gravitational field of the matter distribution. One can easily study shocks and discontinuities with this type of code.&lt;br /&gt;
It is also possible to improve the resolution using mesh refinement but it&amp;rsquo;s important to bear in mind that the resolution of mass particles and of hydrodynamics should be improved together. Grid codes can be expanded to include other effects such as magnetohydrodynamics.&lt;br /&gt;
The second way is the Smoothed Particles Hydrodynamics (SPH). In these type of algorithms the fluid properties (pressure, density, temperature, &amp;hellip;) at any point can be found by averaging over particles in a region using a weight function. Because of this smoothing functions it has poor resolution of shocks and discontinuities and low resolution in low density regions. However these codes are quite easily to implement and has high resolution in high density regions.&lt;/p&gt;

&lt;p&gt;In both types of codes effects such as elementary chemical reactions, e.g. formation of hydrogen, molecules, cooling, heating, etc can be added because they are local effects. On the contrary star formation, feedback from stellar and other source, radiation transport, and so on are non-local and include a big range of scales so are difficult to implement.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #5: initial conditions!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-5-initial-conditions/</link>
      <pubDate>Wed, 16 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-5-initial-conditions/</guid>
      <description>

&lt;p&gt;After we had a look on why we need cosmological simulations, what they are and how they are performed, it&amp;rsquo;s time to learn more about the preparation of a simulation, in particular what are initial conditions and how we build them.&lt;/p&gt;

&lt;p&gt;Usually, for a cosmological simulation, all the fields are linear so we can use linear theory to compute them. In linear theory the density contrast evolves as a combination of a growing and a decaying mode, but only the first survives, so we can set the initial system considering only the growing mode, and because until linearity is verified the density field and velocities are related to the gravitational potential we only need to generate the gravitational field and with it we can produce density and velocity fields.&lt;br /&gt;
Once we have the initial potential there are two ways to generate the initial conditions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;we can uniformly distribute the particles in the simulation box and choose the masses according to the gravitational field; velocities can be zero, in which case we increase the potential to account for the decaying mode, or non-zero and appropriate for the growing mode&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;we can displace uniformly distributed particles using the velocities of the linear theory; the displacement should be smaller than the interparticle separation or we can obtain a wrong power spectrum because a bigger displacement would lead the trajectories to cross and in this case, formally, the density contrast grows to infinite (this is not acceptable in linear approximation) and after that it decreases while the particles go away and this is not physical&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The relation that linking velocity and gravitational potential in the second entry:&lt;br /&gt;
$\frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t}+2\frac{\mathrm{d}a/\mathrm{d}t}{a}\mathbf{v}\propto \mathbf{g}=-\frac{\nabla\phi}{a}$
is the same both in linear theory and in the Zel&amp;rsquo;dovich approximation (this takes the non-linear equations together with the acceleration given by the linear theory to obtain results in the quasi-linear regime), so it&amp;rsquo;s often said that the Zel&amp;rsquo;dovich approximation is used to set up initial conditions.&lt;/p&gt;

&lt;p&gt;Given that, the homogeneity of the initial unperturbed distribution it&amp;rsquo;s very important because any inhomogeneity would combine with density perturbations modifying initial conditions. This homogeneity can be thought in different ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a homogeneous but not random way is to put particles on a cubic grid but it could produce visible features due to the regular distribution&lt;/li&gt;
&lt;li&gt;we can consider putting particles at random positions but the $\sqrt{n}$ fluctuations will result in spurious clustering that will eventually dominate the fluctuations we want to simulate&lt;/li&gt;
&lt;li&gt;a good compromise could be placing particles in lattice cells with a random displacement from the center: this remove regularity but maintain uniformity&lt;/li&gt;
&lt;li&gt;the last possibility are &amp;ldquo;glass initial conditions&amp;rdquo; obtained by evolving an arbitrary distribution of particles in an n-body simulation with a repulsive force; it was invented by Simon White and the name refers to the molecular structure of glass, uniform but not regular&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Considering the last two possibilities, it&amp;rsquo;s worth noting that the former still has a lot of noise, and the second is better than the grid only for aesthetic reasons.&lt;/p&gt;

&lt;h2 id=&#34;the-initial-gravitational-potential:111&#34;&gt;The initial gravitational potential&lt;/h2&gt;

&lt;p&gt;In standard cosmological models the initial perturbations density field is Gaussian. Since the linear evolution doesn&amp;rsquo;t modify the statistics of density fields except for evolving the amplitude of perturbations, and because the potential and the density contrast are linked by a linear relation, the gravitational potential is also a Gaussian random field, and such a field is completely described in terms of the power spectrum of density perturbations. The Fourier components of a Gaussian random field (both real and imaginary part) are random numbers with a normal distribution and variance proportional to the power spectrum at that wave number. Also the phase of this random numbers has to be random, and changing it the resulting field is completely different.
We can generate the gravitational field in the Fourier space just using these properties and inverse transform it (or the force) to obtain the initial potential in real space. If we use adaptive mesh refinement codes we need Gaussian random fields with a variable resolution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #3: force calculation!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-3-force-calculation/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-3-force-calculation/</guid>
      <description>

&lt;p&gt;In the previous posts we wrote about cosmological simulations, why we need them, how they are performed and which are the important things to care about.
Now we are ready to learn the algorithms developed to compute gravitational forces between the particles.&lt;/p&gt;

&lt;h3 id=&#34;direct-summation-pp-method:113&#34;&gt;Direct summation: PP method&lt;/h3&gt;

&lt;p&gt;The raw approach is to calculate forces between all pairs in the simulation. This works well for less than $10^3$. The number of pairs, and therefore the computational time, scales as $N^2$, so this method that is usually unacceptable given the number of particles in a simulation.&lt;br /&gt;
It&amp;rsquo;s difficult to implement periodic boundary conditions in this method (because you have to manage it by hand) and the only convenient way to do this is to use the Ewald summation.&lt;br /&gt;
This method is mostly used to test other methods.&lt;/p&gt;

&lt;h3 id=&#34;tree-method:113&#34;&gt;Tree method&lt;/h3&gt;

&lt;p&gt;The disadvantage of the pp-method comes from having to compute the interaction of any new particle with all the others. The tree method, on the contrary, approximate the force of a distant group of particles with the force exerted by only one particle, in the center of mass of the group, with mass equal the total mass of the group. In this way the computation scales as $N\log N$.&lt;br /&gt;
To create the groups the particles are divided and arranged in a tree structure, that is, the initial volume is divided, and every resulting volume divided again. This procedure goes on until in the resulting volumes there&amp;rsquo;s only one particle. In this situation it&amp;rsquo;s very important the &amp;ldquo;cell acceptance criterion&amp;rdquo; that decide whether or not a cell is far enough to be used as it is or if it has to be divided into its subvolumes.&lt;br /&gt;
The accuracy and the speed of this method can be improved storing some information about the particles in the volumes (such as moments of the mass distribution, usually the quadrupole).&lt;br /&gt;
Moreover, close particles can share information about distant groups because the force is very similar, and the tree can be parallelized efficiently. However, it&amp;rsquo;s not so easy to consider periodic boundary conditions.&lt;/p&gt;

&lt;h3 id=&#34;fast-multipole-method:113&#34;&gt;Fast multipole method&lt;/h3&gt;

&lt;p&gt;This method is an improved version of the tree method. It is based on including higher moments of the mass distribution in cells and some other optimizations. This method has a computational costs which scales as $N$.&lt;/p&gt;

&lt;p&gt;However all these method has problems with the open boundary conditions and it&amp;rsquo;s difficult to adapt it to the cosmological needs. An extension of the tree method with explicit momentum conservation has been also developed.&lt;/p&gt;

&lt;h3 id=&#34;particle-mesh-method:113&#34;&gt;Particle-mesh method&lt;/h3&gt;

&lt;p&gt;This is the first method used extensively for cosmological simulations and the first used for simulation with order of $10^5$ particles. The PM method solve the Poisson equation (partial differential equation that links the gravitational field with the mass distribution) in the Fourier space, where it is a simple algebraic equation, using the FFT (Fast Fourier Transform) routine to change from coordinate space to the Fourier space and viceversa&amp;nbsp;. FFT requires to sample the functions at uniformly spaced points and here we use a grid (mesh). We compute density at the grid points using weight functions on the particles representing the density and velocity fields.&lt;br /&gt;
The use of Fourier methods automatically include periodic boundary conditions without no additional effort (because of the nature of periodic function in the Fourier space) and the use of the mesh automatically soften the force at small scales (because of the interpolation needed to compute the gravitational field on the grid) but it underestimate the force at scales larger (toughly up to 3 times) than the softening length.&lt;br /&gt;
This method is parallelized using parallel FFT and dividing the volume among the processors.&lt;br /&gt;
Softening at the mesh scale give collisionless evolution but the code cannot resolve structure at scales smaller than the mesh scale, moreover the mesh makes the force anisotropic at small scales. This happens because the grid points don&amp;rsquo;t sample very well the filed on such small scales.&lt;/p&gt;

&lt;h3 id=&#34;adaptive-mesh-refinement:113&#34;&gt;Adaptive-mesh refinement&lt;/h3&gt;

&lt;p&gt;In AMR methods the mesh is refined in high density regions using a finer grid, but it&amp;rsquo;s important to pay attention to the conservation of momentum and angular momentum switching from a grid to another.&lt;/p&gt;

&lt;h3 id=&#34;p3m-particle-particle-particle-mesh:113&#34;&gt;P3M: particle-particle+particle-mesh&lt;/h3&gt;

&lt;p&gt;The idea is to add a correction to the force calculated with the PM method by using the PP method on the closest particles. The correction is assumed to be isotropic and depend only upon the distance. Usually it&amp;rsquo;s considered a distance of the order of 2 times the internode mesh distance. This method has been parallelized but with some problems, also because load balancing is quite difficult to achieve.&lt;/p&gt;

&lt;p&gt;Some other problems are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the correction to the force is isotropic but the PM method has anisotropies at small scales&lt;/li&gt;
&lt;li&gt;the correction is at scales up to 2 times the grid scale but the PM method underestimate the force on scales larger than these&lt;/li&gt;
&lt;li&gt;the refined interparticle softening is at scales smaller than the interparticle separation and this can lead to two-body scattering and relaxation&lt;/li&gt;
&lt;li&gt;P3M simulations slow down at late times when the distribution of particles becomes highly clustered because the short range force dominate the compute operations&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tree-pm-tpm:113&#34;&gt;Tree+PM: TPM&lt;/h3&gt;

&lt;p&gt;Some hybrid codes has been developed trying to combine the PM and the Tree methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Grid of Trees PM (GOTPM)replaces the PP part of P3M with a local tree in each region. This resolve only the last problem of the P3M.&lt;/li&gt;
&lt;li&gt;TPM correct the short range force only if the particle is in a highly dense region and with a tree to compute the forces.&lt;/li&gt;
&lt;li&gt;TreePM divide the force in long-range and short-range instead of correcting the PM force at a certain scale, greater than that of the P3M. The short-range forces are calculated with a global tree.&lt;/li&gt;
&lt;li&gt;The Adaptive TreePM (ATreePM) try to resolve the two-body relaxation and scattering using an adaptive softening length, determined by the local density. To ensure momentum conservation force is simmetrized for particles closer than the softening length. The softening correspond to consider the particles with a density profile and not only mass points and if the softening length is different for two close particles, the force they feel is different between them. This happens &amp;nbsp;because particle A feel a force due to its entire mass and a certain fraction of particle B mass, but particle B feels a force due to its own mass and a different fraction of the mass of particle A. In this case the forces are symmetrized taking the mean of the two forces.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To compare different methods we can consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dynamic range, that is the range of scales over which the force is computed reliably. Usually the limit is at small scales rather than at large scales.&lt;/li&gt;
&lt;li&gt;The code should integrate the equation of motion in a reproducible way and momentum should be conserved.&lt;/li&gt;
&lt;li&gt;The code should be efficient and run with the minimum possible time&lt;/li&gt;
&lt;li&gt;Requirement of memory and other resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #7: Limitations and some considerations</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-7-limitations-and-some-considerations/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-7-limitations-and-some-considerations/</guid>
      <description>

&lt;h2 id=&#34;limitations:114&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;In the previous posts we encountered some of the limitations of cosmological&lt;br /&gt;
simulations. Let&amp;rsquo;s review these in detail.&lt;br /&gt;
First, we can consider a simulation composed of a finite box in a bigger space but to represent a real system, this box shouldn&amp;rsquo;t be isolated so we use the periodic boundary conditions (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-2-how.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #2: how?&#34;&gt;here&lt;/a&gt;). This means that all the space around the box is filled with images of the box itself: a particle that leaves the box from one side will come in&lt;br /&gt;
from the opposite side.&lt;/p&gt;

&lt;p&gt;Second, the mass inside the box is not continuous. Instead, it is made by particles of mass of the order of $10^9$ solar masses. These particles represent collisionless fluid elements (made by a huge quantity of real particles) with a certain&lt;br /&gt;
volume and can&amp;rsquo;t be treated as solid spheres. When two simulation particles are&lt;br /&gt;
separated by a distance smaller than the radius of the volumes they represents&lt;br /&gt;
they must feel less than the force coming from the entire mass (thanks to the&lt;br /&gt;
Gauss/Birkhoff&amp;rsquo;s theorem). To do this we soften (read &amp;ldquo;we reduce&amp;rdquo;) the force at&lt;br /&gt;
such small scales (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-2-how.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #2: how?&#34;&gt;here&lt;/a&gt;). Third, time is not continuous and its discreteness was also treated (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-4-moving.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations: #4: Moving the particles!&#34;&gt;here&lt;/a&gt;) with some&lt;br /&gt;
criteria to decide the time steps.&lt;br /&gt;
Until now, however, we haven&amp;rsquo;t consider the effects of taking into account&lt;br /&gt;
initial density fluctuations over a range of scales that is finite. In&lt;br /&gt;
addition to this, the finite size of the box pose a limit on the force&lt;br /&gt;
resolution, because fluctuations on scales bigger than the box side will not&lt;br /&gt;
included in the simulation due to the way the Fourier transforms act on a&lt;br /&gt;
period box. Some tests in literature show that the exclusion of small&lt;br /&gt;
scales shouldn&amp;rsquo;t affect too much large scales when they reach the non linear&lt;br /&gt;
regime but this not holds for the exclusion of large scales, those scales bigger&lt;br /&gt;
than the box side. Following Bagla, the large scale exclusion should not&lt;br /&gt;
disturb the formation of small haloes but could change their distribution.&lt;br /&gt;
This effect will appear as an underestimation of the correlation function. Bagla&lt;br /&gt;
finds that the best way of quantifying the effects of long wave modes is to&lt;br /&gt;
check whether including them in the simulation will change the number of&lt;br /&gt;
massive haloes or not and this can be estimated using the Press-Schecther mass&lt;br /&gt;
function.&lt;br /&gt;
In Tormen&amp;amp;Bertschinger (1996) the missing power on large scales will cause&lt;br /&gt;
something like a statistical cosmic bias decreasing the number of high-density&lt;br /&gt;
regions, the strength of the clustering and the amplitude of the peculiar&lt;br /&gt;
velocities.&lt;br /&gt;
Methods have been developed to take the missing &amp;ldquo;larger than the box&amp;rdquo; wave modes&lt;br /&gt;
into account and we will have a look on these in a future post.&lt;/p&gt;

&lt;h2 id=&#34;some-considerations:114&#34;&gt;Some considerations&lt;/h2&gt;

&lt;p&gt;As we have seen (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-1-why-and-what.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #1: why and what?&#34;&gt;here&lt;/a&gt;) N-body cosmological simulations&lt;br /&gt;
are useful to understand aspects of non-linear gravitational clustering,&lt;br /&gt;
since it&amp;rsquo;s not possible to carry out laboratory experiments in gravitational&lt;br /&gt;
dynamics and the analytic models fail when the system reach the non linear&lt;br /&gt;
regime, i.e. when the density contrast overcome the unity. Related with&lt;br /&gt;
cosmological simulations there are a pair of aspects that Bagla underlines in its&lt;br /&gt;
articles that interesting to consider.&lt;br /&gt;
The first issue is whether or not the gravitational clustering&lt;br /&gt;
erase memory of initial conditions. Is there a one-to-one correspondence between&lt;br /&gt;
some characterization of initial perturbations and the final state?&lt;br /&gt;
N-body simulations shows that gravitational clustering does not erase memory of&lt;br /&gt;
the initial conditions, the final power spectrum is a function of the initial&lt;br /&gt;
power spectrum and this relationship can be written as a one-step mapping and&lt;br /&gt;
the functional form of this mapping depends on the initial power spectrum.&lt;br /&gt;
However density profiles of massive haloes have a form independent of&lt;br /&gt;
initial conditions but there is a considerable scatter in density profiles&lt;br /&gt;
obtained from N-body simulations and it is difficult to state whether a given&lt;br /&gt;
functional form is always the best fit or not. I must admit that these last concepts are not very clear to me at the moment, and that I trust Bagla but I will deepen them as soon as possible to be able to comfortably master them.&lt;br /&gt;
The second question is if it is possible to predict the masses and distribution&lt;br /&gt;
of haloes that form as a result of gravitational clustering.&lt;br /&gt;
The initial density field is taken to be a Gaussian random field and for&lt;br /&gt;
hierarchical models the simple assumption that each peak undergoes collapse&lt;br /&gt;
independent of the surrounding density distribution can be used to estimate the&lt;br /&gt;
mass function and several related quantities but N-body simulations shows that&lt;br /&gt;
this simple set of approximations is incorrect. However, the resulting mass&lt;br /&gt;
function estimation is fairly accurate over a wide range of masses. Merger rates&lt;br /&gt;
can be thus computed using the extended Press-Schecther formalism. Modifying&lt;br /&gt;
some of this assumption can lead to improved predictions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://iopscience.iop.org/0004-637X/472/1/14&#34; target=&#34;_blank&#34; title=&#34;G. Tormen and E. Bertschinger, Adding long wavelenght modes to an N-body simulation&#34;&gt;Giuseppe Tormen and Edmund Bertschinger, Adding long wavelenght modes to an N-body simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/full/1997MNRAS.286...38C&#34; target=&#34;_blank&#34; title=&#34;S. Cole, Adding long-wavelength power to N-body simulations&#34;&gt;S. Cole, Adding long-wavelength power to N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations: #4: Moving the particles!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-4-moving-the-particles/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-4-moving-the-particles/</guid>
      <description>

&lt;h3 id=&#34;equations-of-motion:115&#34;&gt;Equations of motion&lt;/h3&gt;

&lt;p&gt;Once we have learned how to calculate gravitational forces and decided which way is best for us, it&amp;rsquo;s time to move particles according to the force field.&lt;br /&gt;
To have an idea of what happens we can look at a simplified version of the pp-method, where the new coordinates and velocities are expressed starting from the previous values:&lt;/p&gt;

&lt;p&gt;$\mathbf{v}_i^{new}=\mathbf{v}_i^{old}+\frac{\mathbf{F}_i}{m_i}\Delta t$
$\mathbf{x}_i^{new}=\mathbf{x}_i^{old}+\mathbf{v}_i\Delta t$&lt;/p&gt;

&lt;p&gt;More in general we start from the Newtonian equation of motion for a set of particles interacting only through gravity, in particular we consider the Euler and Poisson equations. We express these in comoving coordinates because in this way we can focus on density and velocity perturbations, while the expansion of the universe is included in the scale factor $a$ (obtained from the Friedmann equations):&lt;/p&gt;

&lt;p&gt;$\frac{\mathrm{d}^2\mathbf{x}}{\mathrm{d}t^2}+2\frac{\mathrm{d}a/\mathrm{d}t}{a}\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=-\frac{1}{a^2}\nabla_x\phi$&lt;/p&gt;

&lt;p&gt;$\nabla_x^2\phi=4\pi Ga^2\bar \rho\delta = \frac{3}{2}H_0^2\Omega_0\frac{\delta}{a}$&lt;/p&gt;

&lt;p&gt;Here $\mathbf{x}=\frac{\mathbf{r}}{a(t)}$ is the comoving coordinate of a particle, with $\mathbf{r}$ the physical coordinate; $\phi$ is the gravitational potential due to the density perturbations, $H_0$ the Hubble constant and $\Omega_0$ the density parameter at the present time. In real simulations these equations are modified changing variables to simplify the integration but the idea remain the same. They are valid for non-relativistic matter and on scales smaller than the Hubble radius $c/H_0$. The expansion of the universe act as a viscous force that opposes to the gravitational infall slowing down the growth of perturbations.&lt;br /&gt;
Usually to integrate (read it as &amp;ldquo;to solve&amp;rdquo;!:P) the equations and obtain the new positions and velocities we use the Leap-Frog method because it minimizes the number of force evaluations. This is the most time consuming part of the code: Leap-Frog only requires one evaluation and the corresponding error is of the order of $\Delta t^3$. In this method positions and accelerations are computed at integer time steps and velocities at half time step: for example, positions are calculated at $t$ and velocities at $t+\Delta t/2$.&lt;/p&gt;

&lt;h3 id=&#34;time-stepping:115&#34;&gt;Time stepping&lt;/h3&gt;

&lt;p&gt;Choosing the simulation time step, that is the intervals between two calculation of positions and/or velocities, is crucial both for performances of the code and errors generation. Shorter time steps give smaller errors, but slow down the simulation.&lt;br /&gt;
Time steps depend on the distribution of particles and may change during the evolution of the simulation. Use of individual time steps for each particle may speed up calculations, because we can tune the time step on for different particles, increasing the time resolution for particles in denser regions and decreasing it in low density regions. In this way larger $\Delta t$ are sufficient. When we decide the time step we have also to take care of the conservation of momentum and evolution of energy if it&amp;rsquo;s not conserved (it can evolve according to the Irvine-Layzer equation).&lt;br /&gt;
There are different possibility for how to calculate the time step, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;we can take care of the validity of the Irvine-Layzer during the simulation; in this case we have to compute the force at mesh points and then interpolate it at particles positions but, because the force doesn&amp;rsquo;t equal the gradient of the potential and the correction requires a direct sum on all the particles, this is not a good method&lt;/li&gt;
&lt;li&gt;we can look at the convergence of velocities and final positions of particles using different time steps: reducing their duration we should approach the correct values&lt;/li&gt;
&lt;li&gt;the reproducibility of initial conditions may be it&amp;rsquo;s the most stringent criterion because it ensures that the results are correct: under certain conditions (linearity) running the particles forward and then back again we should get back the initial positions&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #1: M*</title>
      <link>http://brunettoziosi.eu/posts/phd-question-1-m/</link>
      <pubDate>Fri, 11 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-1-m/</guid>
      <description>&lt;p&gt;In parallel with the series &amp;ldquo;Cosmological simulations&amp;rdquo; I&amp;rsquo;m starting now another
series of posts about cosmology, astro/physics and related arguments.
It may happen that your PhD advisor ask you a question about something and you
are supposed to know the answer&amp;hellip; but you don&amp;rsquo;t! Or it may happen that you have
to pass an admittance/qualification exam to enter your PhD student career on
general astrophysical knowledge but you can&amp;rsquo;t even remember some arguments exist!
Because of these consideration and for my remembrance I will &amp;nbsp;write
down some of these questions and I will try to answer.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;These posts don&amp;rsquo;t pretend to be nor totally correct neither complete, but they
reflect the answers I have found with, maybe, some corrections by other students or professors.&lt;br /&gt;
&lt;br /&gt;
So let&amp;rsquo;s start with the first question: what is, how it is defined and how you can calculate &lt;code&gt;$M_\*$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;$M_*$ is the typical non-linear mass collapsing at the redshift we are considering.
This means that &lt;code&gt;$M_\*$&lt;/code&gt; is the typical mass of a perturbation that, at the time we
are looking, has the associated liner density contrast &lt;code&gt;$\delta(\mathbf{x})\sim1$&lt;/code&gt;,
or, in the formalism of the excursion set, pass the barrier of &lt;code&gt;$\delta_c=1.686$&lt;/code&gt;.&lt;br /&gt;
Starting from this and with the results of the linear theory we can obtain some
qualitative laws for the non-linear evolution.&lt;br /&gt;
From the linear theory we have that perturbations grow in a self-similar way
(&lt;code&gt;$\delta(\mathbf{x},t)=\delta(\mathbf{x})D(t)$&lt;/code&gt;, with &lt;code&gt;$D(t)$&lt;/code&gt; the growth factor)
and, in an Einstein-de Sitter universe (a spatially flat universe containing only
matter, the Friedmann universe in which the density exactly matches the critical one),
the growth factor is proportional to the scale factor.&lt;br /&gt;
Now, if we make a choice for the spectrum (scale-free spectrum), we can define the
typical non-linear mass that is collapsing as&lt;br /&gt;
&lt;code&gt;$$M_\*(t)\propto D(t)^{6/(3+n)}$$&lt;/code&gt;&lt;br /&gt;
that, in an Einstein-de Sitter universe, becomes&lt;br /&gt;
&lt;code&gt;$$M_\*(t)\propto a^{6/(3+n)}\propto (1+z)^{-6/(3+n)}$$&lt;/code&gt;&lt;br /&gt;
where &lt;code&gt;$n$&lt;/code&gt; is the spectral index.&lt;br /&gt;
From this we can derive other relations (still in the case of a EdS universe):&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;code&gt;$t_\*\propto (1+z)^{-3/2}$&lt;/code&gt; the typical time of formation for a structure of mass &lt;code&gt;$M_\*$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\rho_\*\propto (1+z)^3$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$R\propto M_\*^{1/3}(1+z)^{-1}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\langle v\rangle^2M_\*^{2/3}(1+z)$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>