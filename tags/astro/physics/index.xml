<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Astro/Physics on Post It!</title>
    <link>http://brunettoziosi.eu/tags/astro/physics/</link>
    <description>Recent content in Astro/Physics on Post It!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2015 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://brunettoziosi.eu/tags/astro/physics/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Not only Big Data</title>
      <link>http://brunettoziosi.eu/posts/not-only-big-data/</link>
      <pubDate>Thu, 08 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/not-only-big-data/</guid>
      <description>&lt;p&gt;Recently I&amp;rsquo;ve been at a &lt;a href=&#34;http://www.cineca.it/en&#34;&gt;Cineca&lt;/a&gt; workshop focused on tools to deal with big data analysis.
We had a taste of MapReduce/Hadoop/Spark and friends and we used Docker.
I wrote a small presentation to update my collegues and it become a in-progress presentation of the tools I think are
useful in our everyday work. This presentation aims of acting both as a showcase of what can help us and as a cheat sheet.
You can find it after the break.&lt;/p&gt;

&lt;iframe src=&#34;http://rawgit.com/brunetto/my-public-talks/master/2014-12-cinecaBigData/index.html&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>AGN for Dummies</title>
      <link>http://brunettoziosi.eu/posts/agn-for-dummies/</link>
      <pubDate>Tue, 20 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/agn-for-dummies/</guid>
      <description>&lt;p&gt;Another infographic attempt from one of my PhD exams!&lt;/p&gt;

&lt;p&gt;See the the &lt;a href=&#34;infographics-attempts.html&#34;&gt;other ones too&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/very_unified_model_bck.png&#34;&gt;&lt;img alt=&#34;AGN for Dummies&#34; src=&#34;../../files/very_unified_model_bck.png&#34; title=&#34;AGN for Dummies&#34; width=&#34;800&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD students&#39; Journal Clubs and Sozi</title>
      <link>http://brunettoziosi.eu/posts/phd-students-journal-clubs-and-sozi/</link>
      <pubDate>Mon, 07 May 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-students-journal-clubs-and-sozi/</guid>
      <description>&lt;p&gt;Some times ago I have been intrigued by &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/infographics-attempts/&#34; title=&#34;Infographics attempts!&#34;&gt;infographics&lt;/a&gt; and different ways of communicating. The first software that catchad my attention was &lt;a href=&#34;http://prezi.com/&#34; target=&#34;_blank&#34; title=&#34;Prezi Homepage&#34;&gt;Prezi&lt;/a&gt; but its technology (Flash) and the fact that it&amp;rsquo;s no opensource and has a lot of limitations let me move to &lt;a href=&#34;http://sozi.baierouge.fr/wiki/en:welcome&#34; target=&#34;_blank&#34; title=&#34;Sozi Homepage&#34;&gt;Sozi&lt;/a&gt;, an &lt;a href=&#34;http://inkscape.org/&#34; target=&#34;_blank&#34; title=&#34;Inkscape Homepage&#34;&gt;Inkscape&lt;/a&gt; extension.&lt;br /&gt;
I&amp;rsquo;ve tried it in making my presentation for the PhD students Journal Club, that is a meeting for PhD students and other researchers and professors taking place every Monday. It&amp;rsquo;s purpose is to let us practice on making lectures and to be up to date with recent development in other research areas.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve also ported my first JC presentation (that on my PhD project) to Inkscape but loosing part of the beauty of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Beamer_(LaTeX)&#34; target=&#34;_blank&#34;&gt;Beamer&lt;/a&gt; original theme.&lt;/p&gt;

&lt;p&gt;These are the two presentations. Right click and &amp;ldquo;Open in new tab&amp;rdquo; will let you able to view the presentations, navigatin with left and right arrows. Mid-click to view the summary and up/down arrow to navigate without any effect.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../../files/first_JC_with_proposal.svg&#34;&gt;PhD project presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../files/2012-05-07-Atacama_telescope_constrain_on_primordial_power_spectrum.svg&#34;&gt;JC on the ACT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Infographics attempts!</title>
      <link>http://brunettoziosi.eu/posts/infographics-attempts/</link>
      <pubDate>Mon, 23 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/infographics-attempts/</guid>
      <description>&lt;p&gt;Recently I am interested in &lt;a href=&#34;http://en.wikipedia.org/wiki/Information_graphics&#34; target=&#34;_blank&#34; title=&#34;Infographics on wikipedia&#34;&gt;infographics&lt;/a&gt; (check also &lt;a href=&#34;http://www.coolinfographics.com/&#34; target=&#34;_blank&#34; title=&#34;Coolinfographics&#34;&gt;this&lt;/a&gt;) and if it can help communicating science. There are also two interesting projects related to this, &lt;a href=&#34;http://prezi.com/&#34; target=&#34;_blank&#34; title=&#34;Prezi homepage&#34;&gt;Prezi&lt;/a&gt; and &lt;a href=&#34;http://sozi.baierouge.fr/wiki/en:welcome&#34; target=&#34;_blank&#34; title=&#34;Sozi homepage&#34;&gt;Sozi&lt;/a&gt;. I&amp;rsquo;m not an artist but I want to try to change my way of communicating and to save lectures notes. This is my first attempt, the first lesson of the Astroparticles course for PhD students! It is not really &amp;ldquo;graphic&amp;rdquo;, but it&amp;rsquo;s my first step!:P&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/astropartycles_1.png&#34;&gt;&lt;img alt=&#34;Astroparticles notes 1&#34; height=&#34;800&#34; src=&#34;../../files/astropartycles_1.png&#34; title=&#34;Astroparticles First Aid 1&#34; width=&#34;600&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/astropartycles_2.png&#34;&gt;&lt;img alt=&#34;Astroparticles notes 2&#34; height=&#34;800&#34; src=&#34;../../files/astropartycles_2.png&#34; title=&#34;Astroparticles First Aid 2&#34; width=&#34;600&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #4: calculate the value of M*</title>
      <link>http://brunettoziosi.eu/posts/phd-question-4-calculate-the-value-of-m/</link>
      <pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-4-calculate-the-value-of-m/</guid>
      <description>

&lt;p&gt;Some post ago &lt;a href=&#34;phd-question-1-m.html&#34;&gt;I&amp;rsquo;ve written about M*&lt;/a&gt;, the typical non-linear mass collapsing at the redshift we are considering. Now I have to find a value for it.&lt;/p&gt;

&lt;p&gt;I said that &lt;code&gt;$M^*$&lt;/code&gt; is the typical mass of a perturbation that, at the time we are looking, has the associated liner density contrast &lt;code&gt;$\delta(\mathbf{x})\sim1$, or, in the formalism of the excursion set, pass the barrier of &amp;amp;nbsp;$\delta_c=1.686$.     
This means that we are looking for a perturbation with&lt;/code&gt;$\sigma\simeq1.686$` and trying to quantify the mass it contains.&lt;/p&gt;

&lt;h2 id=&#34;sigma-and-r:96&#34;&gt;&lt;code&gt;$\sigma$&lt;/code&gt; and R&lt;/h2&gt;

&lt;p&gt;First of all we need to find the radius of a perturbation whose &lt;code&gt;$\sigma$&lt;/code&gt; reached the value of 1.686. To do this we can use the &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/the-initial-conditions-saga/&#34; target=&#34;_blank&#34; title=&#34;The “Initial Conditions” saga&#34;&gt;code&lt;/a&gt; developed to manage the CAMB files in order to find the matter power spectrum and its normalization. Then we add few lines to &amp;ldquo;sample&amp;rdquo; the &lt;code&gt;$\sigma(R)$&lt;/code&gt; distribution and find the radius of the perturbation reaching the excursion set barrier for the collapse.&lt;/p&gt;

&lt;h2 id=&#34;m:96&#34;&gt;&lt;code&gt;$M^*$&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Once we have the radius for which &lt;code&gt;$\sigma = \delta_c$&lt;/code&gt; we need to know the mean density in the universe to find &lt;code&gt;$M^*$&lt;/code&gt; with:&lt;/p&gt;

&lt;p&gt;$M^* = \frac{4}{3}\pi R_*^3\rho_m$`&lt;/p&gt;

&lt;p&gt;&lt;del&gt;I don&amp;rsquo;t know why we only need to use &lt;code&gt;$\rho_m$&lt;/code&gt; and not &lt;code&gt;$\rho_m\delta$&lt;/code&gt; or something similar is not clear to me, but it&amp;rsquo;s correct.&lt;/del&gt;We use this formula because &lt;code&gt;$M^*$&lt;/code&gt; is a quantity related to the linear perturbations. It&amp;rsquo;s correct because the difference between a linear and a non-linear perturbation is the value of the density contrast, but the mass is the same. In other words, the mass of a perturbation is the same both in the linear and in the non-linear evolution, but linear perturbations have smaller density contrasts and larger radii, non-linear perturbations instead have larger density contrasts and smaller radii. To be precise, the previous equation can be written:&lt;br /&gt;
$M^* = \frac{4}{3}\pi R&lt;em&gt;*^3\rho&lt;/em&gt;{bg}(1+1.686) = \frac{4}{3}\pi R&lt;em&gt;{vir}^3\rho&lt;/em&gt;{bg}(1+200)$.&lt;br /&gt;
To obtain &lt;code&gt;$\rho_m$&lt;/code&gt; we find the value of the critical density &lt;code&gt;$\rho_c$&lt;/code&gt; and multiply it for `$\Omega = \rho_m / \rho_c$. These two values can be obtained from books (&lt;a href=&#34;http://www.amazon.com/Cosmology-Prof-Peter-Coles/dp/0471489093/ref=ntt_at_ep_dpt_2&#34; target=&#34;_blank&#34; title=&#34;Coles &amp;amp; Lucchin&#34;&gt;Lucchin&lt;/a&gt;, &lt;a href=&#34;http://www.amazon.com/Galaxy-Formation-Evolution-Houjun-Mo/dp/0521857937/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1333458377&amp;amp;sr=1-1&#34; target=&#34;_blank&#34; title=&#34;Mo, van den Bosch &amp;amp; White&#34;&gt;Mo&amp;amp;White&lt;/a&gt; for example) or in the &lt;a href=&#34;http://lambda.gsfc.nasa.gov/product/map/dr2/params/lcdm_wmap.cfm&#34; target=&#34;_blank&#34; title=&#34;WMAP data page&#34;&gt;WMAP data page&lt;/a&gt;. In the second case we prefer to use the single data fit because it&amp;rsquo;s simpler to refer to it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import time
import numpy as np
import matplotlib.pyplot as plt
import random as rnd
from scipy import integrate

&amp;quot;&amp;quot;&amp;quot; Calculate M* . M* is propto the mass contained in the radius for which 
s_8=delta_c refer to: 
http://www.brunettoziosi.eu/blog/wordpress/the-initial-conditions-saga/
http://www.brunettoziosi.eu/blog/wordpress/phd-question-3-calculate-the-value-of-m/
&amp;quot;&amp;quot;&amp;quot;

t = time.time()

#===============================================================================
#    Compute sigma
#===============================================================================

### Load data from the nasa-CAMB file 

# matterpower is the file with the k and the total matter spectrum
# transfer is the file with the k and the transfer function for the various 
#species, the 6th column is for the total matter (baryons+DM)
transfer = np.genfromtxt(&#39;camb_88704620_transfer_out_z0.dat&#39;, usecols = (0,6))
#matterpower = np.genfromtxt(&#39;2012-01-30_data/camb_88704620_matterpower_z0.dat&#39;)

# CAMB CDM transfer output
camb_k = transfer[:,0]
camb_tf = transfer[:,1] 

R = 8 #Mpc/h
s_8 = 0.9#0.8118405 #from WMAP7 but we need the values for the Millennium-2, so 
# we use its s_8
sp_ind = 1
delta_c = 1.686

### Calculate the amplitude to normalize the spectrum:
### P(k) = Ak^nT^2(k) 

# FT of the window function (spherical top-hat)
def FTW(R, k): 
	&amp;quot;&amp;quot;&amp;quot; Return the Fourier transform of the window function 
	(spherical top-hat)
	&amp;quot;&amp;quot;&amp;quot;
	return 3.*(np.sin(k*R)-k*R*np.cos(k*R)) / (k*R)**3

def spectrum():
	&amp;quot;&amp;quot;&amp;quot;Calculate the power spectrum given the transfer function and the FT of the window 
	function.
	&amp;quot;&amp;quot;&amp;quot;
	# camb_k**(2+sp_ind) that is k^(2+n) because d^3k=4pi k^2dk
	amp_integrand = camb_k**(2+sp_ind)*camb_tf**2 * FTW(R, camb_k)**2
	amp_integral = integrate.trapz(amp_integrand, camb_k)
	# Amplitude for s_8 = 1
	amp_0 = 2*np.pi**2/amp_integral
	# Amplitude
	amp = amp_0*s_8**2  # 9.9197881817e-09
	#print amp
	# Calculate the power spectrum
	return camb_k**sp_ind*camb_tf**2 * amp

# Calculate the power spectrum
ps = spectrum()

# Calculate sigma on the radii
def sigma(R):
	&amp;quot;&amp;quot;&amp;quot;Return the sigma for the current radius.
	&amp;quot;&amp;quot;&amp;quot;
	return pow(integrate.trapz(camb_k**2 * ps * FTW(R, camb_k)**2, camb_k)/(2*np.pi**2), 0.5)

#===============================================================================
#    Find the radius containing M*
#===============================================================================

# Initialize some variables
neigh = np.ones(2) # two nearest neighbours sigmas
r_min = 10**(-2) # min r to sample
r_max = 10**2 # max r ti sample
i = 0 # loop counter

# While stops when the computed sigma is less then 0.001 from delta_c
while np.abs(np.amin(delta_c+neigh)) &amp;amp;gt; 0.001:
print &amp;quot;Loop &amp;quot;, i
i+=1
print &amp;quot;Condition start &amp;quot;, np.abs(np.amin(delta_c+neigh))
# radii to be sampled
r = np.linspace(r_min, r_max, num=100)
# Compute sigma for those radii, the minus sign is to avoid resorting of the
# array to be used by np.searchsorted
s_r = -np.asarray(map(sigma, r))
# Find the two nearest neighbours
neigh[0] = np.amax(s_r[s_r  -delta_c])
# Find the corresponding radii
r_min = r[np.searchsorted(s_r, neigh).min()]
r_max = r[np.searchsorted(s_r, neigh).max()]
print &amp;quot;Sigmas&amp;quot;, -neigh[0], -neigh[1]
print &amp;quot;Radii [Mpc/h] &amp;quot;, r_min, r_max
print &amp;quot;Condition end &amp;quot;, np.abs(np.amin(delta_c+neigh))

# Selected values
s_star = neigh[np.argmin(delta_c+neigh)]
r_star = r[np.searchsorted(s_r, s_star)]
deviation = np.abs(np.amin(delta_c+neigh))

print &amp;quot;############################################&amp;quot;

print &amp;quot;Selected sigma &amp;quot;, -s_star
print &amp;quot;Selected radius [Mpc/h] &amp;quot;, r_star

print &amp;quot;Calculate M* using:&amp;quot;
print &amp;quot;Gt4.299 x 10^(-9) Mpc /M_sun (km/s)^2tfrom Mo&amp;amp;amp;White&amp;quot;
print &amp;quot;Ht100*h^2&amp;quot;
print &amp;quot;Omega_mt0.25tfrom the Millennium-2 simulation&amp;quot;
print &amp;quot;Omega_mt0.241tfrom WMAP7&amp;quot;

#===============================================================================
#    Cosmological parameters and find the mean density in the Universe
#===============================================================================

H = 100
h = 0.732 #WMAP http://lambda.gsfc.nasa.gov/product/map/dr2/params/lcdm_wmap.cfm
G = 4.299*10**(-9)
omega_m_mill = 0.25
omega_m_WMAP = 0.241

# Until here it&#39;s correct
rho_c = 3*H**2/(8*np.pi*G) # 2.7766040316101764 * h**2 x 10^11 M_sun/Mpc^3
	# 2.778 from Lucchin book
	# 2.775 from Mo&amp;amp;amp;White book

rho_mean_mill = rho_c * omega_m_mill # 6.94151007903 * h**2 x 10^10 M_sun/Mpc^3 = 3.71942769658 x 10^10 M_sun/Mpc^3
	rho_mean_WMAP = rho_c * omega_m_WMAP # 6.69161571618 * h**2 x 10^10 M_sun/Mpc^3 = 3.58552829951 x 10^10 M_sun/Mpc^3

print &amp;quot;rho_c = 3H^2/8 pi G = &amp;quot;, rho_c,&amp;quot; h^2 M_sun/Mpc^3&amp;quot;
print &amp;quot;Millennium-2 rho_mean = omega_m_mill * rho_c = &amp;quot;, rho_mean_mill, &amp;quot; h^2 M_sun/Mpc^3 = &amp;quot;, rho_mean_mill*h**2
print &amp;quot;WMAP7 rho_mean = omega_m_WMAP * rho_c = &amp;quot;, rho_mean_WMAP, &amp;quot; h^2 M_sun/Mpc^3 = &amp;quot;, rho_mean_WMAP*h**2 

#===============================================================================
#    Compute M*
#===============================================================================

M_star_mill = np.pi * r_star**3 * rho_mean_mill * 4./3#* (delta_c + 1)
M_star_WMAP = np.pi * r_star**3 * rho_mean_WMAP * 4./3#* (delta_c + 1)

print &amp;quot;M* Millennium-2 &amp;quot;, M_star_mill, &amp;quot; M_sun/h&amp;quot; # 4.81467115575e+12 M_sun/h
print &amp;quot;M* WMAP &amp;quot;, M_star_WMAP, &amp;quot; M_sun/h&amp;quot; # 4.64134299414e+12 M_sun/h

#===============================================================================
#    Compare with Hayashi&amp;amp;amp;White 2008 article
#===============================================================================

print &amp;quot;Hayashi&amp;amp;amp;White&#39;s value:&amp;quot;

M_star_white = 6.15*10**(12)
omega_m_white = 3.*M_star_white/(4*np.pi*r_star**3*rho_c)

print &amp;quot;M*: &amp;quot;, M_star_white
print &amp;quot;Omega_m&amp;quot;, omega_m_white

r_s = pow(3.*M_star_white/(4*np.pi*rho_mean_mill), 1./3)
s_white = sigma(r_s)

print &amp;quot;As alternative:&amp;quot;
print &amp;quot;R* &amp;quot;, r_s
print &amp;quot;Sigma &amp;quot;, s_white

#===============================================================================
#    Summary
#===============================================================================

print &amp;quot;&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;SUMMARY&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;&amp;quot;
print &amp;quot;WhottttM*ttR*ttOmegattSigmattSigma-delta_c&amp;quot;
print &amp;quot;--------------------------------------------------------------------------------------------------------------&amp;quot;
print &amp;quot;Hayashi&amp;amp;amp;White given Rtt{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_white,r_star,omega_m_white,-s_star,deviation)
print &amp;quot;Hayashi&amp;amp;amp;White given Omegat{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_white,r_s,omega_m_mill,s_white,np.abs(s_white-delta_c))
print &#39;Me WMAP datattt{:e}t{:e}t{:e}t{:e}t{:e}&#39;.format(M_star_WMAP,r_star,omega_m_WMAP,-s_star,deviation)
print &amp;quot;Me Millennium-2 datatt{:e}t{:e}t{:e}t{:e}t{:e}&amp;quot;.format(M_star_mill,r_star,omega_m_mill,-s_star,deviation)
print &amp;quot;&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;##############################################################################################################&amp;quot;
print &amp;quot;&amp;quot;
print &amp;quot;Done in &amp;quot;, time.time()-t
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The differences between the different values of &lt;code&gt;$M^*$&lt;/code&gt; are acceptable and probably depends on different integration boundaries for `$\sigma$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #3: Monte Carlo Markov chain</title>
      <link>http://brunettoziosi.eu/posts/phd-question-3-monte-carlo-markov-chain/</link>
      <pubDate>Thu, 15 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-3-monte-carlo-markov-chain/</guid>
      <description>

&lt;p&gt;After &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/phd-question-1-latexm_latex/&#34; target=&#34;_blank&#34; title=&#34;PhD question #1: M*&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/phd-question-2-spherical-collapse/&#34; target=&#34;_blank&#34; title=&#34;PhD question #2: spherical collapse&#34;&gt;&lt;/a&gt;, in this post I write about the Monte Carlo Markov chain. This post is the Wordpress transposition of a page in my private and local Mediawiki installation, so maybe has not a perfect structure and look&amp;amp;feel. Also, the Latex formulas embedded in &lt;a href=&#34;http://www.mediawiki.org/wiki/MediaWiki&#34; target=&#34;_blank&#34; title=&#34;Mediawiki homepage&#34;&gt;Mediawiki&lt;/a&gt; and &lt;a href=&#34;http://wordpress.com/&#34; target=&#34;_blank&#34; title=&#34;Wordpress homepage&#34;&gt;Wordpress&lt;/a&gt; look horrible.&lt;br /&gt;
Most of this post is made by paragraphs from &lt;a href=&#34;http://en.wikipedia.org/wiki/Main_Page&#34; target=&#34;_blank&#34; title=&#34;Wikipedia homepage&#34;&gt;Wikipedia&lt;/a&gt; and from &lt;a href=&#34;http://arxiv.org/abs/0712.3028&#34; target=&#34;_blank&#34; title=&#34;Licia Verde&#39;s article page&#34;&gt;A practical guide to Basic Statistical Techniques for Data Analysis in Cosmology&lt;/a&gt; by &lt;a href=&#34;http://icc.ub.edu/~liciaverde/Home%20Page.html&#34; target=&#34;_blank&#34; title=&#34;Licia Verde homepage&#34;&gt;Licia Verde&lt;/a&gt; that I rearranged or try to explain and summarize.&lt;/p&gt;

&lt;p&gt;Essentially you can think a Markov chain as a set of states of physical system, or a series of values, depending on some parameters, in which every step/status depends only on the present status and not on the past. A Monte Carlo Markov chain is a Markov chain that proceeds trying random steps and tends to converge to an equilibrium that should be the best set of parameters we are looking for.&lt;br /&gt;
To improve performances and precision usually more chains are used, starting from different and well-separated points, for a single estimation and a convergence criterion define when to stop.&lt;br /&gt;
The steady state, or the equilibrium distribution, if it exists, roughly is the combination of parameters, o the vector for the probabilities to be in one status, that emerge to be constant in the long term. This means that if the system (the chain) at some time has a probability vector &lt;code&gt;$q$&lt;/code&gt; for the next status, it will have probability vector &lt;code&gt;$q$&lt;/code&gt; forever. &lt;code&gt;$q$&lt;/code&gt; is an eigenvector of the transition matrix with eingenvalue 1.&lt;/p&gt;

&lt;h2 id=&#34;markov-chain:99&#34;&gt;Markov chain&lt;/h2&gt;

&lt;p&gt;A Markov chain is a random process (sequence of random variables) that satisfy the Markov property. Usually the term &amp;ldquo;Markov chain&amp;rdquo; is used to mean a Markov process which has a discrete (finite or countable) state-space defined on a discrete set times.&lt;br /&gt;
nevertheless &amp;ldquo;time&amp;rdquo; can take continuous values and the use of the term in Monte Carlo Markov Chain (MCMC) refers to cases where the process is defined on a discrete time-set (discrete algorithm steps) and a continuous state space.&lt;/p&gt;

&lt;p&gt;The Markov property states that the conditional probability distribution for the system at the next step (and in fact at all future steps) depends only on the current state of the system, and not additionally on the state of the system at previous steps.&lt;br /&gt;
Formally, given a sequence of random variables &lt;code&gt;$X_1, X_2, X_3, \dots$&lt;/code&gt; we have&lt;/p&gt;

&lt;p&gt;&lt;code&gt;${\rm Pr} (X_{n+1}=x|X_1=x_1, X_2=x_2, \dots, X_n=x_n) = {\rm Pr} (X_{n+1}=x|X_n=x_n).,$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The possible values &lt;code&gt;$x_i$&lt;/code&gt; of &lt;code&gt;$X_i$&lt;/code&gt; form a countable set &lt;code&gt;$S$&lt;/code&gt; called the &lt;strong&gt;state space&lt;/strong&gt; of the chain.&lt;/p&gt;

&lt;p&gt;The changes of state of the system are called transitions, and the probabilities associated with various state-changes are called transition probabilities. The set of all states and transition probabilities for a given step form the &lt;strong&gt;transition matrix&lt;/strong&gt; `$p_{ij}$. The transition matrices completely characterizes a Markov chain.&lt;/p&gt;

&lt;h2 id=&#34;types-of-chains:99&#34;&gt;Types of chains&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time-homogeneous Markov chains&lt;/strong&gt; (or &lt;strong&gt;stationary Markov chains&lt;/strong&gt;) are processes where    &lt;code&gt;$Pr(X_{n+1}=x|X_n=y) = Pr(X_n=x|X_{n-1}=y)$&lt;/code&gt;    for all &lt;code&gt;$n$&lt;/code&gt;. The probability of the transition is independent of &lt;code&gt;$n$&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A **Markov chain of order ** &lt;code&gt;$m$&lt;/code&gt; (or a Markov chain with memory&amp;nbsp;$m$&amp;nbsp;, where&amp;nbsp;$m$&amp;nbsp;is finite, is a process satisfying&lt;br /&gt;
&lt;code&gt;$Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots , X_1=x_1)$&lt;/code&gt;    that is    &lt;code&gt;$Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots, X_{n-m}=x_{n-m})\text{ for }n &amp;gt; m$&lt;/code&gt;    In other words, the future state depends on the past &lt;code&gt;$m$&lt;/code&gt; states.&lt;/li&gt;
&lt;li&gt;An &lt;strong&gt;additive Markov chain&lt;/strong&gt;&amp;nbsp;&lt;/li&gt;
&lt;li&gt;of order &lt;code&gt;$m$&lt;/code&gt; is a sequence of random variables &lt;code&gt;$X_1, X_2, X_3, \dots$&lt;/code&gt;, possessing the following property: the probability that a random variable &lt;code&gt;$X_n$&lt;/code&gt; has a certain value &lt;code&gt;$x_n$&lt;/code&gt; under the condition that the values of all previous variables are fixed depends on the values of &lt;code&gt;$m$&lt;/code&gt; previous variables only (Markov chain of order `$m$), and the influence of previous variables on a generated one is additive,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots, X_{n-m}=x_{n-m}) = \sum_{r=1}^{m} f(x_n,x_{n-r},r)$&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;predictions:99&#34;&gt;Predictions&lt;/h2&gt;

&lt;p&gt;Since the system changes randomly, it is generally impossible to predict with certainty the state of a Markov chain at a given point in the future. However, the statistical properties of the system&amp;rsquo;s future can be predicted.&lt;br /&gt;
The probability of going from state &lt;code&gt;$i$&lt;/code&gt; to state &lt;code&gt;$j$&lt;/code&gt; in &lt;code&gt;$n$&lt;/code&gt; time steps is&lt;br /&gt;
&lt;code&gt;$p_{ij}^{(n)} = Pr(X_n=j mid X_0=i) ,$&lt;/code&gt;&lt;br /&gt;
and the single-step transition is&lt;br /&gt;
&lt;code&gt;$p_{ij} = Pr(X_1=jmid X_0=i). ,$&lt;/code&gt;&lt;br /&gt;
For a time-homogeneous Markov chain:&lt;br /&gt;
&lt;code&gt;$p_{ij}^{(n)} = Pr(X_{k+n}=j mid X_{k}=i) ,$&lt;/code&gt;&lt;br /&gt;
and&lt;br /&gt;
&lt;code&gt;$p_{ij} = Pr(X_{k+1}=j mid X_k=i). ,$&lt;/code&gt;&lt;br /&gt;
The &lt;code&gt;$n$&lt;/code&gt;-step transition probabilities satisfy the Chapman–Kolmogorov equation, that for any &lt;code&gt;$k$&lt;/code&gt; such that &lt;code&gt;$0&amp;lt;k&amp;lt;n$&lt;/code&gt;,&lt;br /&gt;
&lt;code&gt;$p_{ij}^{(n)} = \sum_{r in S} p_{ir}^{(k)} p_{rj}^{(n-k)}$&lt;/code&gt;&lt;br /&gt;
where &lt;code&gt;$S$&lt;/code&gt; is the state space of the Markov chain.&lt;br /&gt;
The marginal distribution &lt;code&gt;$Pr(X_n=x)$&lt;/code&gt; is the distribution over states at time &lt;code&gt;$n$&lt;/code&gt;. The initial distribution is &lt;code&gt;$Pr(X_0=x)$&lt;/code&gt;. The evolution of the process through one time step is described by&lt;br /&gt;
&lt;code&gt;$ Pr(X_{n}=j) = \sum_{r in S} p_{rj} Pr(X_{n-1}=r) = \sum_{r in S} p_{rj}^{(n)} Pr(X_0=r).$&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;example-a-very-simple-weather-model:99&#34;&gt;Example: a very simple weather model&lt;/h2&gt;

&lt;p&gt;The probabilities of weather conditions (modeled as either rainy or sunny), given the weather on the preceding day,&lt;br /&gt;
can be represented by a transition matrix:&lt;br /&gt;
&lt;code&gt;$P = \begin{bmatrix}0.9 &amp;amp; 0.1 \ 0.5 &amp;amp; 0.5\end{bmatrix}$&lt;/code&gt;&lt;br /&gt;
The matrix &lt;code&gt;$P$&lt;/code&gt; represents the weather model in which a sunny day is 90%&lt;br /&gt;
likely to be followed by another sunny day, and a rainy day is 50% likely to&lt;br /&gt;
be followed by another rainy day. The columns can be labelled &amp;ldquo;sunny&amp;rdquo; and&lt;br /&gt;
&amp;ldquo;rainy&amp;rdquo; respectively, and the rows can be labelled in the same order.&lt;br /&gt;
&lt;code&gt;$P_{ij}$&lt;/code&gt; is the probability that, if a given day is of type &lt;code&gt;$i$&lt;/code&gt;, it will be&lt;br /&gt;
followed by a day of type &lt;code&gt;$j$&lt;/code&gt;.&lt;br /&gt;
Notice that the rows of &lt;code&gt;$P$&lt;/code&gt; sum to 1: this is because &lt;code&gt;$P$&lt;/code&gt; is a stochastic matrix.&lt;/p&gt;

&lt;h2 id=&#34;predicting-the-weather:99&#34;&gt;Predicting the weather&lt;/h2&gt;

&lt;p&gt;The weather on day 0 is known to be sunny. This is represented by a vector in which the &amp;ldquo;sunny&amp;rdquo; entry is 100%, and the &amp;ldquo;rainy&amp;rdquo; entry is 0%:&lt;br /&gt;
&lt;code&gt;$ \mathbf{x}^{(0)} = \begin{bmatrix} 1 &amp;amp; 0 \end{bmatrix}$&lt;/code&gt;&lt;br /&gt;
The weather on day 1 can be predicted by:&lt;br /&gt;
&lt;code&gt;$ \mathbf{x}^{(1)} = \mathbf{x}^{(0)} P = \begin{bmatrix}1 &amp;amp; 0 \end{bmatrix} \begin{bmatrix} 0.9 &amp;amp; 0.1 .5 &amp;amp; 0.5\end{bmatrix} = \begin{bmatrix} 0.9 &amp;amp; 0.1 \end{bmatrix} $&lt;/code&gt;&lt;br /&gt;
Thus, there is an 90% chance that day 1 will also be sunny.&lt;br /&gt;
The weather on day 2 can be predicted in the same way:&lt;br /&gt;
&lt;code&gt;$ \mathbf{x}^{(2)} =\mathbf{x}^{(1)} P = \mathbf{x}^{(0)} P^2 = \begin{bmatrix} 1 &amp;amp; 0 \end{bmatrix}\begin{bmatrix}0.9 &amp;amp; 0.1 \ 0.5 &amp;amp; 0.5 \end{bmatrix}^2= \begin{bmatrix} 0.86 &amp;amp; 0.14\end{bmatrix} $&lt;/code&gt;&lt;br /&gt;
or&lt;br /&gt;
&lt;code&gt;$ \mathbf{x}^{(2)} =\mathbf{x}^{(1)} P = \begin{bmatrix}0.9 &amp;amp; 0.1\end{bmatrix} \begin{bmatrix}0.9 &amp;amp; 0.1 .5 &amp;amp; 0.5\end{bmatrix}= \begin{bmatrix} 0.86 &amp;amp; 0.14 \end{bmatrix} $&lt;/code&gt;&lt;br /&gt;
General rules for day &lt;code&gt;$n$&lt;/code&gt; are:&lt;br /&gt;
&lt;code&gt;$ \mathbf{x}^{(n)} = \mathbf{x}^{(n-1)} P $&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;$ \mathbf{x}^{(n)} = \mathbf{x}^{(0)} P^n $&lt;/code&gt;&lt;br /&gt;
&lt;div&gt;
    &lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;steady-state-of-the-weather:99&#34;&gt;Steady state of the weather&lt;/h2&gt;

&lt;p&gt;In this example, predictions for the weather on more distant days are increasingly&lt;br /&gt;
inaccurate and tend towards a steady state vector. This vector represents&lt;br /&gt;
the probabilities of sunny and rainy weather on all days, and is independent&lt;br /&gt;
of the initial weather.&lt;br /&gt;
The steady state vector is defined as:&lt;br /&gt;
&lt;code&gt;$ \mathbf{q} = \lim_{n \rightarrow \infty} \mathbf{x}^{(n)}$&lt;/code&gt;&lt;br /&gt;
but only converges to a strictly positive vector if &lt;code&gt;$P$&lt;/code&gt; is a regular transition matrix (that is, there&lt;br /&gt;
is at least one &lt;code&gt;$P^n$&lt;/code&gt; with all non-zero entries).&lt;br /&gt;
Since the &lt;code&gt;$q$&lt;/code&gt; is independent from initial conditions, it must be unchanged when transformed by &lt;code&gt;$P$&lt;/code&gt;. This makes it an eigenvector (with eigenvalue 1), and means it can be derived from &lt;code&gt;$P$&lt;/code&gt;. For the weather example:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$\begin{matrix} P &amp;amp; = &amp;amp; \begin{bmatrix} 0.9 &amp;amp; 0.1 \\ 0.5 &amp;amp; 0.5 \end{bmatrix}\\ \mathbf{q} P &amp;amp; = &amp;amp; \mathbf{q}  \mbox{(} \mathbf{q} \mbox{ is unchanged by } P \mbox{.)} \\ &amp;amp; = &amp;amp; \mathbf{q}I (P - I) &amp;amp; = &amp;amp; \mathbf{0} \\ &amp;amp; = &amp;amp;\\ \mathbf{q} \left(\begin{bmatrix} 0.9 &amp;amp; 0.1 \\ 0.5 &amp;amp; 0.5 \end{bmatrix} - \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 1 \end{bmatrix}\right) &amp;amp; = &amp;amp; \mathbf{q} \begin{bmatrix} - 0.1 &amp;amp; 0.1 \\ 0.5 &amp;amp; -0.5 \end{bmatrix} \end{matrix}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$ \begin{bmatrix} q_1 &amp;amp; q_2 \end{bmatrix} \begin{bmatrix} -0.1 &amp;amp; 0.1 \\ 0.5 &amp;amp; -0.5 \end{bmatrix} = \begin{bmatrix} 0 &amp;amp; 0 \end{bmatrix}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;So&lt;br /&gt;
&lt;code&gt;$ -0.1 q_1 + 0.5 q_2 = 0$&lt;/code&gt;&lt;br /&gt;
and since they are a probability vector we know that&lt;br /&gt;
&lt;code&gt;$q_1 + q_2 = 1.$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Solving this pair of simultaneous equations gives the steady state distribution:&lt;br /&gt;
&lt;code&gt;$ \begin{bmatrix} q_1 &amp;amp; q_2 \end{bmatrix} = \begin{bmatrix} 0.833 &amp;amp; 0.167 \end{bmatrix}$&lt;/code&gt;&lt;br /&gt;
In conclusion, in the long term, 83% of days are sunny.&lt;/p&gt;

&lt;h2 id=&#34;monte-carlo-markov-chain-mcmc:99&#34;&gt;Monte Carlo Markov Chain (MCMC)&lt;/h2&gt;

&lt;p&gt;Markov chain Monte Carlo (MCMC) methods (which include random walk Monte Carlo methods) are a class of algorithms for sampling from probability distributions based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. The state of the chain after a large number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.&lt;br /&gt;
Usually it is not hard to construct a Markov chain with the desired properties. The more difficult problem is to determine how many steps are needed to converge to the stationary distribution within an acceptable error. A good chain will have rapid mixing—the stationary distribution is reached quickly starting from an arbitrary position—described further under Markov chain mixing time.&lt;br /&gt;
Typical use of MCMC sampling can only approximate the target distribution, as there is always some residual effect of the starting position. More sophisticated MCMC-based algorithms such as coupling from the past can produce exact samples, at the cost of additional computation and an unbounded (though finite in expectation) running time.&lt;br /&gt;
The most common application of these algorithms is numerically calculating multi-dimensional integrals. In these methods, an ensemble of &amp;ldquo;walkers&amp;rdquo; moves around randomly. At each point where the walker steps, the integrand value at that point is counted towards the integral. The walker then may make a number of tentative steps around the area, looking for a place with reasonably high contribution to the integral to move into next. Random walk methods are a kind of random simulation or Monte Carlo method. However, whereas the random samples of the integrand used in a conventional Monte Carlo integration are statistically independent, those used in MCMC are correlated. A Markov chain is constructed in such a way as to have the integrand as its equilibrium distribution. Surprisingly, this is often easy to do.&lt;/p&gt;

&lt;h2 id=&#34;random-walk-algorithms:99&#34;&gt;Random walk algorithms&lt;/h2&gt;

&lt;p&gt;Many Markov chain Monte Carlo methods move around the equilibrium distribution in relatively small steps, with no tendency for the steps to proceed in the same direction. These methods are easy to implement and analyze, but unfortunately it can take a long time for the walker to explore all of the space. The walker will often double back and cover ground already covered.  Here are some random walk MCMC methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metropolis–Hastings algorithm: Generates a random walk using a proposal density and a method for rejecting proposed moves.&lt;/li&gt;
&lt;li&gt;Gibbs sampling: Requires that all the conditional distributions of the target distribution can be sampled exactly.  Popular partly because when this is so, the method does not require any &amp;lsquo;tuning&amp;rsquo;.&lt;/li&gt;
&lt;li&gt;Slice sampling: Depends on the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function.  This method alternates uniform sampling in the vertical direction with uniform sampling from the horizontal &amp;lsquo;slice&amp;rsquo; defined by the current vertical position.&lt;/li&gt;
&lt;li&gt;Multiple-try Metropolis: A variation of the Metropolis–Hastings algorithm that allows multiple trials at each point. This allows the algorithm to generally take larger steps at each iteration, which helps combat problems intrinsic to large dimensional problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;avoiding-random-walks:99&#34;&gt;Avoiding random walks&lt;/h2&gt;

&lt;p&gt;More sophisticated algorithms use some method of preventing the walker from doubling back. These algorithms may be harder to implement, but may exhibit faster convergence (i.e. fewer steps for an accurate result).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Successive over-relaxation: A Monte Carlo version of this technique can be seen as a variation on Gibbs sampling; it sometimes avoids random walks.&lt;/li&gt;
&lt;li&gt;Hybrid Monte Carlo (HMC): Tries to avoid random walk behaviour by introducing an auxiliary momentum vector and implementing Hamiltonian dynamics where the potential function is the target density.  The momentum samples are discarded after sampling.  The end result of Hybrid MCMC is that proposals move across the sample space in larger steps and are therefore less correlated and converge to the target distribution more rapidly.&lt;/li&gt;
&lt;li&gt;Some variations on slice sampling also avoid random walks.&lt;/li&gt;
&lt;li&gt;Langevin MCMC and other methods that rely on the gradient (and possibly second derivative) of the log posterior avoid random walks by making proposals that are more likely to be in the direction of higher probability density.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;monte-carlo-markov-chain-in-cosmology-cmb-example:99&#34;&gt;Monte Carlo Markov chain in Cosmology: CMB example&lt;/h2&gt;

&lt;p&gt;In some cases, mapping the likelihood or the posterior distribution can be very time-expensive, so a MCMC can be used to investigate the likelihood space. The MCMC generates random draws (simulations) from the posterior distribution that are a “fair” sample of the likelihood surface. A properly derived and implemented MCMC draws from the joint posterior density&lt;br /&gt;
&lt;code&gt;$P(\alpha|x)$&lt;/code&gt; once it has converged to the stationary distribution. The primary consideration in&lt;br /&gt;
implementing MCMC is determining when the chain has converged. After an initial “burn-in” period, all further samples can be thought of as coming from the stationary distribution.&lt;br /&gt;
In other words the chain has no dependence on the starting location.&lt;br /&gt;
Another fundamental problem of inference from Markov chains is that there are always&lt;br /&gt;
areas of the target distribution that have not been covered by a finite chain, it is thus crucial that the chain achieves&lt;br /&gt;
good “mixing” so it can explore the support of the target distribution rapidly.&lt;br /&gt;
It is important to have a convergence&lt;br /&gt;
criterion and a mixing diagnostic. Plots of the sampled MCMC parameters or likelihood&lt;br /&gt;
values versus iteration number are commonly used to provide such criteria. However, samples from a chain are typically serially correlated; very high auto-correlation leads to little movement of the chain and thus makes the chain to “appear” to&lt;br /&gt;
have converged. Using a MCMC that has not fully&lt;br /&gt;
explored the likelihood surface for determining cosmological parameters will yield wrong&lt;br /&gt;
results.&lt;/p&gt;

&lt;h2 id=&#34;in-practice:99&#34;&gt;In practice&lt;/h2&gt;

&lt;p&gt;Here are the necessary steps to run a simple MCMC for the CMB temperature power&lt;br /&gt;
spectrum. It is straightforward to generalize these instructions to include the temperature-&lt;br /&gt;
polarization power spectrum and other datasets. The MCMC is essentially a random walk in&lt;br /&gt;
parameter space, where the probability of being at any position in the space is proportional&lt;br /&gt;
to the posterior probability.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start with a set of cosmological parameters &lt;code&gt;${\alpha_1}$&lt;/code&gt;, compute the &lt;code&gt;$Cl$&lt;/code&gt; and the likelihood    &lt;code&gt;$L_1 = L(Cl |Cl )$&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Take a random step in parameter space to obtain a new set of cosmological parameters    &lt;code&gt;${\alpha_2 }$&lt;/code&gt;. The probability distribution of the step is taken to be Gaussian in each direction    i with r.m.s given by &lt;code&gt;$\sigma_1$&lt;/code&gt; . We will refer below to σi as the “step size”. The choice of    the step size is important to optimize the chain efficiency&lt;/li&gt;
&lt;li&gt;Compute the &lt;code&gt;$Cl$&lt;/code&gt; for the new set of cosmological parameters and their likelihood &lt;code&gt;$L_2$&lt;/code&gt;.    4.a) If &lt;code&gt;$L_2 /L_1 \geq 1$&lt;/code&gt;, “take the step” i.e. save the new set of cosmological parameters &lt;code&gt;${\alpha_2}$&lt;/code&gt;    as part of the chain, then go to step 2 after the substitution &lt;code&gt;${\alpha_1 } \rightarrow {\alpha2 }$&lt;/code&gt;.    4.b) If &lt;code&gt;$L_2 /L_1 &amp;lt; 1$&lt;/code&gt;, draw a random number x from a uniform distribution from 0 to 1. If    &lt;code&gt;$x \geq L_2 /L_1$&lt;/code&gt; “do not take the step”, i.e. save the parameter set &lt;code&gt;${\alpha_1 }$&lt;/code&gt; as part of the    chain and return to step 2. If `$x &amp;lt; L_2 /L_1$, “ take the step”, i.e. do as in 4.a).&lt;/li&gt;
&lt;li&gt;For each cosmological model run four chains starting at randomly chosen, well-separated points in parameter space. When the convergence criterion is satisfied and    the chains have enough points to provide reasonable samples from the a posteriori    distributions (i.e. enough points to be able to reconstruct the 1- and 2-σ levels of the    marginalized likelihood for all the parameters) stop the chains.

&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;appendix:99&#34;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&#34;random-process:99&#34;&gt;Random process&lt;/h3&gt;

&lt;p&gt;In probability theory, a stochastic process, or sometimes random process, is the counterpart to a deterministic process (or deterministic system). Instead of dealing with only one possible way the process might develop over time (as in the case, for example, of solutions of an ordinary differential equation), in a stochastic or random process there is some indeterminacy described by probability distributions. This means that even if the initial condition (or starting point) is known, there are many possibilities the process might go to, but some paths may be more probable and others less so.&lt;/p&gt;

&lt;h3 id=&#34;bayesian-statistic-likelihood-priori-and-posteriori-distribution:99&#34;&gt;Bayesian statistic, likelihood, priori and posteriori distribution&lt;/h3&gt;

&lt;p&gt;From &lt;a href=&#34;http://www.thphys.uni-heidelberg.de/~amendola/statistics-ws2011.html&#34;&gt;http://www.thphys.uni-heidelberg.de/~amendola/statistics-ws2011.html&lt;/a&gt;, to be completed.&lt;br /&gt;
We have a theory and some data, the Bayesian approach says that&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P(T;D)=\frac{P(D;T)P(T)}{P(D)}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$P(T;D)$&lt;/code&gt; is the &amp;ldquo;conditional&amp;rdquo; probability of having the theory (theoretical parameters) given the data (that is the probability distribution of the parameters given the data observed and the prior knowledge on the parameters themselves) and it is called &lt;strong&gt;posterior distribution&lt;/strong&gt; or sometimes likelihood&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(D;T)$&lt;/code&gt; is the &amp;ldquo;conditional&amp;rdquo; probability of having the data given the theory and it is also called &lt;strong&gt;likelihood&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(T)$&lt;/code&gt; is the probability of having the theory, that is the prior knowledge we have on the parameters, it&amp;rsquo;s the &lt;strong&gt;prior distribution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P(D)$&lt;/code&gt; is the probability of find the data, that is a normalization, it does not help in finding the parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The prior distribution is often unknown but can be of two kind: it can come from previous experiments or it can be the exclusion of some regions of the parameter space. The final result strongly depends on it.&lt;/p&gt;

&lt;h3 id=&#34;references:99&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.lorisbazzani.info/papers/old_projects/HMM_MPES.pdf&#34;&gt;http://www.lorisbazzani.info/papers/old_projects/HMM_MPES.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book&#34;&gt;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;www.mathce.it/archivio/markov.pdf&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Markov_chain&#34;&gt;http://en.wikipedia.org/wiki/Markov_chain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Stochastic_process&#34;&gt;http://en.wikipedia.org/wiki/Stochastic_process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.vinz.info/catene-di-markov%3a-spam-che-piace-ai-motori-pt-1-4e1b78613a5d29b75200000a.html/li&amp;gt;&#34;&gt;http://www.vinz.info/catene-di-markov%3a-spam-che-piace-ai-motori-pt-1-4e1b78613a5d29b75200000a.html/li&amp;gt;&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.thphys.uni-heidelberg.de/~amendola/statistics-ws2011.html&#34;&gt;http://www.thphys.uni-heidelberg.de/~amendola/statistics-ws2011.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/0712.3028&#34;&gt;http://arxiv.org/abs/0712.3028&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #9: Gadget-2 (N-body part)</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</link>
      <pubDate>Mon, 20 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</guid>
      <description>&lt;p&gt;Here I would like to do a brief presentation of the main features of Gadget-2.&lt;br /&gt;
Gadget-2 (&lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; target=&#34;_blank&#34; title=&#34;Gadget2 homepage&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/my-first-gadget2-tests/&#34; target=&#34;_blank&#34; title=&#34;My first Gadget-2 tests&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2005.09655.x/abstract;jsessionid=DED86CDB5CD8A572F3631F0C42828086.d01t03&#34; target=&#34;_blank&#34; title=&#34;Gadget-2 paper&#34;&gt;here&lt;/a&gt;) is a cosmological simulation code developed primarily by &lt;a href=&#34;http://www.mpa-garching.mpg.de/~volker/&#34; target=&#34;_blank&#34; title=&#34;Volker Springel&#39;s homepage&#34;&gt;Volker Springel&lt;/a&gt;. It is a &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/cosmological-simulations-3-calculating-the-force/&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #3: force calculation!&#34;&gt;TreePM&lt;/a&gt; code so it splits forces between long-range (PM part) and short-range (tree part using multipole expansion to approximate the force of distant particles groups).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The tree&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gadget-2 uses a&lt;a href=&#34;http://en.wikipedia.org/wiki/Octree&#34; target=&#34;_blank&#34; title=&#34;oct-tree&#34;&gt; BH oct-tree&lt;/a&gt; (see also &lt;a href=&#34;http://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation&#34; target=&#34;_blank&#34; title=&#34;Barnes&amp;amp;Hut simulation on wikipedia&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.artcompsci.org/~makino/softwares/C++tree/index.html&#34; target=&#34;_blank&#34; title=&#34;NBODY, an implementation of Barnes-Hut treecode&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://ifa.hawaii.edu/~barnes/software.html&#34; target=&#34;_blank&#34; title=&#34;Barnes&#39; page&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.cita.utoronto.ca/~dubinski/treecode/treecode.html&#34; target=&#34;_blank&#34; title=&#34;A parallel tree code explenation&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.prism.gatech.edu/~gth716h/BNtree/&#34; target=&#34;_blank&#34; title=&#34;Barnes-Hut Implementation in HTML/Javascript&#34;&gt;here&lt;/a&gt;) to calculate the short-range forces in the real space. This choice was done because this type of tree, compared to other types (KD-Tree, &amp;hellip;), requires the creation of less nodes, that imply that less memory is used. It&amp;rsquo;s characterized by eight sub-nodes for each node and has only one particle in each leaf. The code decides to open a leaf according to a certain leaf opening criterion based on the estimated force error. The force for distant groups of particles is approximated with the multipole (here octopole) of the tree node and the error depends on the dimensions and the distances of the node considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PM part&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The PM part of the code is used to calculate the long-range forces. The algorithm is something like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CIC (cloud-in-cell) assignment is used to construct the mass density field on to the mesh from the information on the particles&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the discrete FT of the mesh is multiplied for the Green function for the potential in periodic boundaries (modified with the exponential truncation for the force splitting)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;deconvolution for the CIC kernel twice: the first for the smoothing effect of CIC assignment, the second for the force interpolation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$\mathrm{FT}^{-1}$ to obtain the potential on the mesh&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;finite differentiate the potential to obtain the forces&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;interpolate the forces to the particles positions using CIC&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: real-to-complex FT are used to save times and memory respect to full complex transforms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time step&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This type of code has a large dynamic range in time scale, from the denser regions where the evolution is rapid to the less denser regions in which the evolution occur slower so we can describe it with larger time resolution. In this scenario evolving all particles with the smallest time-scale is a waste of time and computational resources. Because using different time-steps for each particle add instabilities to the system, Gadget-2 separates time-step between long-range (longer time step) and short-range (shorter time step) force computations. The perturbation of the system for different time-steps is related to the symplectic nature of the system, but I still have not understood what it really means and implies! I know that it refers to the phase space volume and has effect on the information conservation. May be in the future I&amp;rsquo;ll write a post about this!&lt;br /&gt;
Despite these arguments, sometimes individual time step are allowed because they perturb the system but not the symplecticity of the single particle.&lt;br /&gt;
In the normal integration mode time-steps are discretized in a power of two hierarchy and particles can always move to smaller time steps but to longer time steps only in subsequent step, synchronized with higher time-steps. Alternatively the code can populate time-steps discretizeing them as integer multiples of the minimum time-step among the particles set. This lead to a more homogeneous distribution of particles across the time-line which can simplify work load balancing.&lt;br /&gt;
The integration is performed using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Leapfrog_integration&#34; target=&#34;_blank&#34; title=&#34;Leapfrog method&#34;&gt;leapfrog method&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually the parallelization distributes particles across the CPUs using an orthogonal domain decomposition but in this way the trees built-in each domain depend on the domain geometry. Because the force depend on the tree (through the multipole expansion of the mass distribution) the force can be different if you change the number of processors.&lt;br /&gt;
Gadget-2 introduce a space-filling fractal, the Peano-Hilber (PH) curve to map 3D space into a 1D curve that encompasses all the particles. Now the PH curve can be cut and each piece assigned to a CPU and in this way the force is independent of the processors number. If you cut every segment in eight pieces recursively you find again the tree decomposition, so there is a close correspondence between the decomposition obtained with the BH oct-tree and that of the PH curve.&lt;br /&gt;
The PH curve has some remarkably properties, for example points that are close along the 1D PH curve are in general close in 3D space, so the mapping preserves locality and if we cut the PH curve into segments of a certain length we obtain a domain decomposition which has the property that the spatial domains are simply connected and quite &amp;ldquo;compact&amp;rdquo; (i.e., they tend to have small surface-to-volume ratios and low aspect ratio, a highly desirable property for reducing communication costs with neighbouring domains and for speeding up the local computation).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Operations scheme&lt;/strong&gt;&lt;br /&gt;
Here a brief scheme on how the short range force calculation works on multiple processors. The PM computation uses the &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_4.html&#34; target=&#34;_blank&#34; title=&#34;Parallel FFTWs&#34;&gt;parallel FFTWs&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Compute the PH key for each particle&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sort the keys locally and split the PH curve into segments&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adjust the sorted segments to a global sort, splitting and joining segments if needed, with little communication&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Assign the particles to the processes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Construct a BH tree for the particles of each processors representing particles on other processors with pseudo-particles (acting like placeholders)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During the tree traverse (e.g. in processor A) these pseudo-particles cannot be opened, the are flagged and inserted into a list that collects all the particles that are to be sent (=requested) to the other processors (e.g. to processor B)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Processor B traverse again its local tree and send back the resulting force contribution to processor A&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The initial condition saga</title>
      <link>http://brunettoziosi.eu/posts/the-initial-condition-saga/</link>
      <pubDate>Tue, 31 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/the-initial-condition-saga/</guid>
      <description>

&lt;p&gt;If reading the previous posts on N-body simulations you have though that initial conditions are a little and easy task, you were wrong! And me with you!&lt;br /&gt;
The first things I have understood banging against them were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gadget requires initial conditions (ICs) generate by (for example) N-GenIC&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;N-GenIC requires an initial power spectrum from CMBFast that is no longer used&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CMBeasy should substitute CMBFast but it doesn&amp;rsquo;t, it only works for CMB anisotropies&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CAMB should substitute CMBFast&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the &lt;a href=&#34;http://lambda.gsfc.nasa.gov/toolbox/tb_camb_form.cfm&#34;&gt;CAMB interface&lt;/a&gt; is terrible and not very well documentated&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;camb:102&#34;&gt;CAMB&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the
&lt;a href=&#34;http://lambda.gsfc.nasa.gov/toolbox/tb_camb_form.cfm&amp;quot; target&#34;&gt;main CAMB interface&lt;/a&gt;.
If you, like me, need an initial power spectrum as input for your ICs generator
you are interested in just few options of the interface. First, you should select
&amp;ldquo;Transfer functions&amp;rdquo; from the &amp;ldquo;Actions to Perform&amp;rdquo; section. You can leave the
default selection on &amp;ldquo;Scalar Cl&amp;rsquo;s&amp;rdquo; and &amp;ldquo;Linear&amp;rdquo;. After that, check the
&amp;ldquo;Cosmological Parameters&amp;rdquo; section if it&amp;rsquo;s ok for you and maybe, leave the
default &amp;ldquo;Initial Scalar Perturbation Mode&amp;rdquo; that is &amp;ldquo;Adiabatic&amp;rdquo;. The last
things you should be interested in are the maximum &lt;code&gt;$k$&lt;/code&gt;, for me it is &lt;code&gt;$10^4$&lt;/code&gt;,
&amp;ldquo;k per logint&amp;rdquo; (it should be something like the k-sampling, for me, 50), the
number of redshift (1) and the (transfer) redshift (0). Now select between
&amp;ldquo;Interpolated Grid&amp;rdquo; or &amp;ldquo;Calculated Values&amp;rdquo; (this parameter switch between and
interpolated regular grid in log k or array at actual computed values that are
better for later re-interpolation, according with the CAMB README) and choose
if you want high precision computation. When you have finished you can click
on &amp;ldquo;Go!&amp;rdquo;. What you obtain is a page with some links to download:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;camb_*.ini&lt;/code&gt;: the configuration file to run the standalone CAMB code on your own&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*.log&lt;/code&gt;: the calculation log&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_scalcls.dat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_scalcls.fits&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_transfer_out_z0.dat&lt;/code&gt;: this is the file containing the transfer
functions for CDM, baryon, photon, massless neutrino, massive neutrinos,
and total (massive) respectively as function of &lt;code&gt;$k$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camb_*_matterpower_z0.dat&lt;/code&gt;: it contains the conventionally normalized
matter power spectrum (for baryons+cdm+massive neutrinos), in h/Mpc units&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where &lt;code&gt;*&lt;/code&gt; is a number that identifies the computation and z0 can be different
if you choose to use a different redshift.&lt;br /&gt;
You can use &lt;code&gt;camb_*_matterpower_z0.dat&lt;/code&gt; as normalized input power spectrum or
you can calculate (also as a check) it on you own using the first and the last
column of &lt;code&gt;camb_*_transfer_out_z0.dat&lt;/code&gt;. You should use the last column because
DM simulations represents with DM particles all the mass, included that of the
baryons, so the initial power spectrum should be the total power spectrum.&lt;br /&gt;
If you want to understand better how CAMB works you can try to read the
&lt;a href=&#34;http://camb.info/readme.html&#34;&gt;CAMB README&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initial power spectrum theory&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here some theory if you want to understand how to calculate the power spectrum
on your own. Maybe it is well-known and trivial, but for me it wasn&amp;rsquo;t and,
like many of the trivial and well-known things in (not only) cosmology,
it&amp;rsquo;s not easy to find anywhere.&lt;br /&gt;
So, the spectrum is defined as &lt;code&gt;$P(k) = A k^n T(k)^2$&lt;/code&gt;, where k
&lt;code&gt;$n$&lt;/code&gt; is the
&amp;ldquo;primordial spectral index&amp;rdquo; and is taken near the unity. This is a
&amp;ldquo;scale-fee&amp;rdquo; spectrum. &lt;code&gt;$T(k)$&lt;/code&gt; is the transfer function that give a
synthetic and parametric description of how the initial spectrum survive
to the microphysic. &lt;code&gt;$A=\left[\frac{D(z_{\rm fin})}{D(z_{\rm in})}\right]^2$&lt;/code&gt; is the amplitude.&lt;br /&gt;
The normalization of the spectrum is given by the value of &lt;code&gt;$\sigma_8$&lt;/code&gt;,
that is the mean square amplitude of the density field filtered on the scale
of 8 Mpc/h. This values comes from the &amp;lsquo;80s, when Peebles and others
(Davis &amp;amp; Peebles 1983) measured &lt;code&gt;$\sigma_{\rm galaxies}$&lt;/code&gt; and &lt;code&gt;$\sigma_{\rm gal}(R=8)\sim1$&lt;/code&gt;
so they took that values as reference.&lt;br /&gt;
&lt;code&gt;$\sigma_8$&lt;/code&gt; is defined by
&lt;code&gt;$\sigma^2(R) = \frac{1}{(2\pi)^3}\int \mathrm{d}^3kP(k)\tilde W(kR)$&lt;/code&gt; with &lt;code&gt;$R=8\mathrm{Mpc/h}$&lt;/code&gt; and
&lt;code&gt;$\tilde W(kR)$&lt;/code&gt; the Fourier transform of the window (filter) function, usually a top-hat in the positions space.&lt;br /&gt;
The last thing we need to know to obtain the spectrum is the value of the amplitude,
and it can be find by imposing its value so that &lt;code&gt;$\sigma_8$&lt;/code&gt; has a certain (observed) value.&lt;br /&gt;
Because&lt;br /&gt;
&lt;code&gt;$$\sigma^2(R) = \frac{1}{(2\pi)^3}\int \mathrm{d}^3kP(k)\tilde W(kR) = \int \mathrm{d}^3k Ak^nT^2(k))\tilde W(kR)$$&lt;/code&gt;&lt;br /&gt;
we have&lt;br /&gt;
&lt;code&gt;$$A_0 = \frac{s^2_R(R=8) }{ \int\mathrm{d}^3k\, k^n T^2(k) \tilde W(8*k)}$$&lt;/code&gt;.&lt;br /&gt;
Usually &lt;code&gt;$A_0$&lt;/code&gt; is calculated for &lt;code&gt;$\sigma_8=1$&lt;/code&gt; and then scaled with
&lt;code&gt;$A = A_0\sigma^2_{\rm 8;obs}$&lt;/code&gt; where &lt;code&gt;$\sigma_{\rm 8;obs}$&lt;/code&gt; is the observed values for
&lt;code&gt;$\sigma_8$&lt;/code&gt;. With the last observations we have &lt;code&gt;$\sigma_8 = 0.8118405$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Just to have an idea you can do these computation with a simple python code.
The code below compare the computation done in Python with the values from
&lt;code&gt;camb_*_transfer_out_z0.dat&lt;/code&gt; with the normalized power spectrum from &lt;code&gt;camb_*_matterpower_z0.dat&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import numpy as np
import matplotlib.pyplot as plt

# Load data
transfer = np.genfromtxt(&#39;camb_88704620_transfer_out_z0.dat&#39;, usecols = (0,6))
matterpower = np.genfromtxt(&#39;camb_88704620_matterpower_z0.dat&#39;)

# Python calculations
camb_k = transfer[:,0]
camb_tf = transfer[:,1] 
R = 8 #Mpc/h
s_8 = 0.8118405
sp_ind = 1

# Define the FT of the window function
def FTW(R, k):
  return 3*(np.sin(k*R)-k*R*np.cos(k*R)) / (k*R)**3

# camb_k**(2+sp_ind) that is k^(2+n) because d^3k=4\pi k^2dk
amp_integrand = camb_k**(2+sp_ind)*camb_tf**2 * FTW(8, camb_k)**2
amp_integral = integrate.trapz(amp_integrand, camb_k)
amp_0 = 2*np.pi**2/amp_integral
amp = amp_0*s_8**2
spectrum = camb_k**sp_ind*camb_tf**2 * amp

ax = fig.add_subplot(111)
ax.set_title(&#39;Fortran/Python CDM initial power spectrum&#39;)
ax.set_xlabel(&#39;k&#39;)
ax.set_ylabel(&#39;P(k)&#39;)
ax.set_xscale(&#39;log&#39;)
ax.set_yscale(&#39;log&#39;)

ax.plot(transfer[:,0], spectrum[:], color = &amp;quot;magenta&amp;quot;, 
           linestyle = &#39;-&#39;, marker = &#39;&#39;, label = &amp;quot;* python amp&amp;quot;) 
ax.plot(matterpower[:,0], matterpower[:,1], color = &amp;quot;black&amp;quot;, 
            linestyle = &#39;--&#39;, marker = &#39;&#39;, label = &amp;quot;matterpower&amp;quot;)
ax.legend(loc=&#39;best&#39;)
ax.grid(True)

# Adjust figure size and save
fig.set_size_inches(20, 20)
plt.savefig(&#39;camb_f90_py_check&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../files/camb_f90_py_check.png&#34; alt=&#34;CAMB vs Python calculated initial power spectrum&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want you can check the power spectrum we have obtained by integrating it
to find &lt;code&gt;$\sigma_8$&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sigma_integrand = camb_k**2 * spectrum * FTW(8, camb_k)**2
s_8_check = pow(integrate.trapz(sigma_integrand, camb_k)/(2*np.pi**2), 0.5)
print &amp;quot;s_8_calculated&amp;quot;, s_8_check
print &amp;quot;s_8 observed&amp;quot;, s_8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;obtaining&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ziosi@uno:~/Code/spettro_CMB$ ./CAMB_check_plot.py
s_8_calculated 0.8118405
s_8 observed 0.8118405
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;N-GenIC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we have the initial power spectrum ready for the ICs generator. After N-GenIC
have been compiled (try to read &lt;a href=&#34;my-first-gadget2-tests&#34;&gt;this&lt;/a&gt;
if you have compilation problems related to the parallel double precision FFTW libraries
or if you want to know how to customize the Makefile) we should have a look at the configuration file.&lt;br /&gt;
We are interested in:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Nmesh&lt;/code&gt;: the size (=the number of nodes) of the FFT grid used to compute the
displacement field, should be &lt;code&gt;Nmesh&lt;/code&gt; &lt;code&gt;$\geq$&lt;/code&gt; &lt;code&gt;Nsample&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Nsample&lt;/code&gt;: this is the maximum k that the code uses, i.e. this effectively
determines the Nyquist frequency that the code assumes, &lt;code&gt;$k_{\rm Nyquist} = 2\cdot \pi/{\rm Box} \cdot  {\rm Nsample}/2$&lt;/code&gt; Normally,
one chooses &lt;code&gt;Nsample&lt;/code&gt; such that &lt;code&gt;${\rm Ntot} =  {\rm Nsample}^3$&lt;/code&gt;, where &lt;code&gt;Ntot&lt;/code&gt; is the total number
of particles. Because the grid sample the particles quantities, Nmesh sets the Nyquist
frequency of Nsample, so it&amp;rsquo;s good if &lt;code&gt;${\rm Nmesh} = 2\cdot {\rm Nsample}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ReNormalizeInputSpectrum&lt;/code&gt;: set it to 0 because we are going to use the previous
spectrum that is already normalized, if you don&amp;rsquo;t remember this the code will have
integration problems&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TileFac&lt;/code&gt;: it represents how many times you need to tile the glass file
(for each dimension) to cover the number of particles you want to use.
The glass file contains 4097 particles. Glass particles positions will be
automatically stretched to cover the box dimension. When you download N-GenIC
you find &lt;code&gt;${\rm Nsample} = 128$&lt;/code&gt; and &lt;code&gt;${\rm TileFac} = 8$&lt;/code&gt;, this is because the total number of
particles is &lt;code&gt;${\rm Ntot} = {\rm Nsample}^3 = 128^3=2097125$&lt;/code&gt; and the number of glass particles
is &lt;code&gt;${\rm TileFac}^3\cdot {\rm Nglass} = 8^3\cdot 4096 = 2097125$&lt;/code&gt;. In practice, if you want to
know what &lt;code&gt;TileFac&lt;/code&gt; should be, and you have &lt;code&gt;Ntot&lt;/code&gt; particles in you simulation,
&lt;code&gt;TileFac&lt;/code&gt; will be &lt;code&gt;$\frac{{\rm Ntot}^{1/3}}{4096} = \frac{{\rm Nsample}}{4096}$&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WhichSpectrum&lt;/code&gt;: let you choose if you want to use an internal spectrum
(calculated with a function) or the spectrum from CAMB&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FileWithInputSpectrum&lt;/code&gt;: it&amp;rsquo;s, obviously, the name of the file containing the spectrum&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The other parameters are cosmological parameters or are related to the folders, the name and the
number of files, the parallelization and to the internal measure units.&lt;br /&gt;
Other options are (more or less) documented with comments in the code or in the README.&lt;br /&gt;
We can now start N-GenIC with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mpiexec -np 2  ./N-GenIC  ics.param
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;-np&lt;/code&gt; let you set the number of processors you want to use in parallel.&lt;br /&gt;
Before using the output files with gadget we should open those and calculate the power
spectrum to check that this realization of it is a good one. This problem arise because
of the sampling of the k-space where few modes are available, but I will deepen on those matter in a future post.&lt;br /&gt;
There is also an improved version of N-GenIC, 2LPTIC, but it need a different
installation of the FFTW so I didn&amp;rsquo;t try it.&lt;/p&gt;

&lt;p&gt;Many of these things can be found &lt;a href=&#34;http://www.annualreviews.org/doi/abs/10.1146/annurev.astro.36.1.599&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD question #2: spherical collapse</title>
      <link>http://brunettoziosi.eu/posts/phd-question-2-spherical-collapse/</link>
      <pubDate>Tue, 10 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/phd-question-2-spherical-collapse/</guid>
      <description>&lt;p&gt;With this post I would like to collect and present in a simple and consistent
form some of the various analytical derivation of the spherical collapse
model, in particular the challenge was to find how to obtain the famous
$\delta_{lin}\sim1.686$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/spherical_collapse2.pdf&#34;&gt;Here&lt;/a&gt; you can find a pdf file with the analytic computation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My first Gadget2 tests</title>
      <link>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</link>
      <pubDate>Sat, 07 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</guid>
      <description>

&lt;p&gt;This post is about my first experience with the cosmological simulation code &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget2&#34;&gt;Gadget2&lt;/a&gt;. To start I followed the instructions found &lt;a href=&#34;http://astrobites.com/2011/04/02/installing-and-running-gadget-2/&#34;&gt;here&lt;/a&gt;. All I&amp;rsquo;m going to write refers to an Ubuntu/Kubuntu 11.10 installation.&lt;/p&gt;

&lt;h2 id=&#34;installation-of-gsl-and-fftw:104&#34;&gt;Installation of GSL and fftw&lt;/h2&gt;

&lt;p&gt;We can download Gadget &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget download&#34;&gt;here&lt;/a&gt;, the GSL (GNU scientific library) &lt;a href=&#34;http://mirror.rit.edu/gnu/gsl/gsl-1.9.tar.gz&#34; title=&#34;GSL download&#34;&gt;here&lt;/a&gt; and the FFTW (fastest Fourier transform in the West library) &lt;a href=&#34;http://www.fftw.org/fftw-2.1.5.tar.gz&#34; title=&#34;FFTW download&#34;&gt;here&lt;/a&gt;. We also need an MPI library (Open-MPI or MPICH, try install it using your package manager).&lt;br /&gt;
Following the Astrobites suggestions let&amp;rsquo;s decompress the archives with &lt;code&gt;tar -xzf &amp;lt;archive name&amp;gt;&lt;/code&gt;. Now we can install the libraries following the Astrobites post:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd gsl-1.9/
goldbaum@~/Documents/code/gsl-1.9: ./configure
snip: lots of diagnostic ouput
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix --enable-float
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As described &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_6.html#SEC69&#34; target=&#34;_blank&#34; title=&#34;FTTW installation and customization&#34;&gt;here&lt;/a&gt; is convenient to install both the single and the double precision version of the FFTW (for example to compile the initial conditions generators) with (that is, without &lt;code&gt;--enable-float&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;play-with-gadget2:104&#34;&gt;Play with Gadget2&lt;/h2&gt;

&lt;p&gt;Now it&amp;rsquo;s time to play with Gadget!:) In this code, for performance reasons, requires to specify some parameters at compile time while other can be set at run time, so that we have to customize the Makefile. This also imply that we should have separate binary files and directories for each simulation.&lt;br /&gt;
To start with something easy, we will customize one of the examples given with the code, the &amp;ldquo;galaxy&amp;rdquo; one. It simulate the collision of two galaxies using 40000 DM particles for the haloes and 20000 baryonic particles for the disks.&lt;/p&gt;

&lt;p&gt;Inside the Gadget directory we have&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Analysis
AUTHORS
COPYING
COPYRIGHT
Documentation
Gadget2
ICs
README
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the &lt;code&gt;Analysis&lt;/code&gt; folder we can fin some analysis routines provided by the author, the &lt;code&gt;Documentation&lt;/code&gt; folder contains the user guide and the original paper, and the &lt;code&gt;AUTHORS, COPYING, COPYRIGHT&lt;/code&gt; self-explanatory. The &lt;code&gt;ICs&lt;/code&gt; folder contains the initial conditions for the example simulations and the &lt;code&gt;Gadget2&lt;/code&gt; folder contains the sources and the html documentation.&lt;/p&gt;

&lt;p&gt;To be tidy and organized is better to have a folder for every simulations, so we will create a (descriptive) with everything we need to customize&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir 2012-01-07-Gadget2-galaxy_test_01
cd 2012-01-07-Gadget2-galaxy_test_01
mkdir out
cp ../ICs/galaxy_littleendian.dat ./
cp ../Gadget2/parameterfiles/galaxy.param ../Gadget2/parameterfiles/galaxy.Makefile ./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the folder Gadget2 we can find the general &lt;code&gt;Makefile&lt;/code&gt; but for now let&amp;rsquo;s use the galaxy&amp;rsquo;s one provided by the author and just copied to our position. Open it with your preferred text editor (for example, in a command line environment, &lt;code&gt;emacs -nw Makefile&lt;/code&gt;).&lt;br /&gt;
This &lt;code&gt;Makefile&lt;/code&gt; is already customized for the galaxy collision simulation and if you want to understand every option you can read the description in the guide, but we need some more customization. Here what I&amp;rsquo;ve changed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPT   +=  -DHAVE_HDF5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so I activate the HDF5 format for the output and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#--------------------------------------- Select target computer

SYSTYPE=&amp;quot;Uno&amp;quot;
\#SYSTYPE=&amp;quot;MPA&amp;quot;
\#SYSTYPE=&amp;quot;Mako&amp;quot;
\#SYSTYPE=&amp;quot;Regatta&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster-gcc&amp;quot;
\#SYSTYPE=&amp;quot;Opteron&amp;quot;

\#--------------------------------------- Adjust settings for target computer

ifeq ($(SYSTYPE),&amp;quot;Uno&amp;quot;)
CC       =  mpicc   
OPTIMIZE =  -O3 -Wall
GSL_INCL =  -I/usr/local/include
GSL_LIBS =  -L/usr/local/lib
FFTW_INCL=  -I/usr/local/include
FFTW_LIBS=  -L/usr/local/lib
MPICHLIB =  -L/usr/lib
HDF5INCL =  
HDF5LIB  =  -lhdf5 -lz 
endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to select the set the options for my system.&lt;br /&gt;
Now we have to customize the &lt;code&gt;run/galaxy.param&lt;/code&gt; file changing it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;InitCondFile      ./galaxy_littleendian.dat
OutputDir          ./galaxy_out/
OutputListFilename ./out/output_list.txt
SnapFormat         3  %to select the HDF5 format
TimeBegin           0.0        % Begin of the simulation
TimeMax             40.0        % End of the simulation

% Output frequency
TimeBetSnapshot        0.1% original 0.5 &amp;lt;/pre&amp;gt;
    
    
Now we should go to the sources folder and compile the code with    
&amp;lt;pre&amp;gt;cd ../Gadget2
make -f 2012-01-07-Gadget2-galaxy_test_01/galaxy.Makefile
cp Gadget2 ../2012-01-07-Gadget2-galaxy_test_01/Gadget2
make clean
cd 2012-01-07-Gadget2-galaxy_test_01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command clean the build leaving only the sources files, so we are ready for a new build.&lt;br /&gt;
We can also create a script for automatize all this steps, something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
dir=$1
ics=$2
param_file=$3
mk_file=$4
CPUs=$5

if [ $# -lt 5 ] ; then
  echo &amp;quot;usage: gadget_set directory_name initial_conditions_file
parameters_file make_file number_of_CPUs&amp;quot;
  exit 0
fi

echo &amp;quot;Assuming to use $dir as the run folder,&amp;quot; 
echo &amp;quot;$ics as initial conditions,&amp;quot;
echo &amp;quot;$paramfile as parameter file, &amp;quot;
echo &amp;quot;$mk_file as makefile &amp;quot;
echo &amp;quot;and to run on $CPUs CPUs.&amp;quot;

mkdir $dir
cd $dir
mkdir out
cp ../ICs/$ics ./
cp ../Gadget2/parameterfiles/$param_file
../Gadget2/parameterfiles/mk_file ./
cd ../Gadget2
make -f ../$dir/$mk_file
cp Gadget2 ../$dir/Gadget2
make clean
cd $dir
mpirun -np $CPUs ./Gadget2 $param_file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very raw and untested script, but it&amp;rsquo;s just to give an idea.&lt;/p&gt;

&lt;p&gt;Now we are ready to start the simulation with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mpirun -np 2 ./Gadget2 galaxy.param
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;-np&lt;/code&gt; sets the number of processes/processors to be used in parallel.&lt;br /&gt;
When the simulation stops we can analyze it with the tools provided in the &lt;code&gt;Analysis&lt;/code&gt; folder or, if you like me don&amp;rsquo;t own an IDL license and don&amp;rsquo;t feel comfortable with IDL/Fortran/C for the data analysis, with something like (to be run in out/plots/):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import sys, os
from subprocess import Popen, PIPE
from multiprocessing import Process, Queue

import numpy as np
import tables as tb
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

&amp;quot;&amp;quot;&amp;quot;This script will plot in parallel the .h5 snapshots created by Gadget2 test
runs one after the other!:).
FIXME: i need a way to wait for the final time count the end of the processes
and a way to print the status
&amp;quot;&amp;quot;&amp;quot;

# Set the max number of processes
n_procs = 3

# Set the number of snapshot to be plotted
n_snap = 401

t = time.time()

print &amp;quot;Defining workers...&amp;quot;

def worker(input, output):
    while input.qsize() != 0:
        item = input.get()
        if item[0]= 10 and item[0]&amp;amp;lt;100: j=&amp;quot;0&amp;quot;+str(item[0])
        else: j=str(item[0])
        try:
#     print &amp;quot;considering file ../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;
#     print &amp;quot;open file &amp;quot;
            h5 = tb.openFile(&amp;quot;../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;, &#39;r&#39;)
#     print &amp;quot;file opened, set variables&amp;quot;
            halo = h5.root.PartType1
            disk = h5.root.PartType2
#            print &amp;quot;setted, inizialize figure&amp;quot;
            fig2 = plt.figure()
            ax = Axes3D(fig2)
            ax.scatter(disk.Coordinates[:,0], 
                       disk.Coordinates[:,1],
                       disk.Coordinates[:,2],
                       color=&#39;red&#39;, s=0.5)
            ax.scatter(halo.Coordinates[:,0], 
                       halo.Coordinates[:,1],
                       halo.Coordinates[:,2],
                       color=&#39;blue&#39;, s=0.01)
            plt.savefig(&#39;snap_&#39;+j)
#            print &amp;quot;done, closing file&amp;quot;
            h5.close()  
#            print &amp;quot;closed&amp;quot;

        except:
            print &amp;quot;Work &amp;quot;+j+&amp;quot; not done, exit...&amp;quot;
            sys.exit()

def fill_queue(task_queue):
    for i in range(n_snap):
        task_queue.put([i])
    return task_queue

def status(proc):
    if proc.is_alive==True:
        return &#39;alive&#39;
    elif proc.is_alive==False:
        return &#39;dead&#39;
    else:
        return proc.is_alive()

print &amp;quot;Define queues...&amp;quot;

input_queue = Queue()
output_queue = Queue()

try:
    input_queue = fill_queue(input_queue)
except:
    print &amp;quot;Queue not filled, exit...&amp;quot;
    sys.exit()

procs = []

try:
    for i in range(n_procs):
        procs.append(Process(target=worker, args=(input_queue,
output_queue)))
except:
    print &amp;quot;Creating processes not complete, exit...&amp;quot;
    sys.exit()

try:
    for i in procs:
        i.start()
except:
    print &amp;quot;Start processes not complete, exit...&amp;quot;
    sys.exit()

for i in procs:
    print &amp;quot;Process &amp;quot;, i,&amp;quot; @ &amp;quot; , i.pid, &amp;quot; is &amp;quot;, status(i)

print &amp;quot;Done in &amp;quot;+str(time.time()-t)+&amp;quot; seconds.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have one image for each snapshot, and if we are interested we can produce a video with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mencoder mf://*.png -mf fps=25:type=png -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -vf scale=720:360 -oac copy -o output.mp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;youtube http://www.youtube.com/watch?v=b7HyafKMkxI&amp;amp;amp;w=560&amp;amp;amp;h=315&#34;&gt;This&lt;/a&gt; is the first basic video, with logarithmic time and perhaps there&amp;rsquo;s something wrong with the coordinates on the axes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GIF2 files Python reader</title>
      <link>http://brunettoziosi.eu/posts/gif2-files-python-reader/</link>
      <pubDate>Tue, 06 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/gif2-files-python-reader/</guid>
      <description>&lt;p&gt;I create this script on the basis of the code to read the Millennium II data (the same used &lt;a href=&#34;http://elbrunz.wordpress.com/2011/12/02/from-binaries-to-hdf5-using-python/&#34; title=&#34;From binaries to HDF5 using Python&#34;&gt;here&lt;/a&gt;) provided by &lt;a href=&#34;http://mbk.ps.uci.edu/index.html&#34; title=&#34;Mike Boylan-Kolchin&#34;&gt;Mike Boylan-Kolchin&lt;/a&gt;. Being allowed to read the Fortran code to write and read the GIF2 files I could adapt this script to exactly fit this problem.&lt;/p&gt;

&lt;!-- TEASER_END--&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import sys

class head:
    def __init__(self, fname):
        import types
        bo_mark=&#39;&amp;gt;&#39;
        # start by reading in header:    
        if type(fname) is types.StringType:
            f=open(fname, &#39;rb&#39;)
        elif type(fname) is not types.FileType:
            raise TypeError(&#39;argument must either be an open file or &#39; + 
                            &#39;a string containing a file name&#39;)
        else:
            f=fname
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the usual imports we define a class to read and contain the file header. These first lines check the &amp;ldquo;filename&amp;rdquo; argument.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.pad = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the unformatted Fortran binary files the &amp;ldquo;pad&amp;rdquo; is a 4-byte space to separate the values of different quantities in the file. &lt;code&gt;bo_mark&lt;/code&gt; contains the endianess (&amp;ldquo;bo&amp;rdquo; means byte order) of the system allowing Python to correctly interpret the numbers from the binary file. &lt;code&gt;count&lt;/code&gt; sets the number of item of type &lt;code&gt;dtype&lt;/code&gt; to be read. For example &lt;code&gt;count=3&lt;/code&gt; and &lt;code&gt;dtype=bo_mark+&#39;i4&#39;&lt;/code&gt; will store three 4-byte integer in the variable, with the byte order expressed by &lt;code&gt;bo_mark&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt; for big-endian, &lt;code&gt;&amp;gt;&lt;/code&gt; for little-endian.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# npart is an array containing the number of particles in the
        # file divided by type (gas, ...)
        self.npart = np.fromfile(f, count=6, dtype=bo_mark+&#39;i4&#39;)
        self.massarr = np.fromfile(f, count=6, dtype=bo_mark+&#39;f8&#39;)
        self.aaa = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.redshift = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.flag_sfr = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.flag_feedback = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.nall = np.fromfile(f, count=6, dtype=bo_mark+&#39;i4&#39;)
        self.cooling_flag = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.numfiles = np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        self.boxsize = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.Omega = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.OmegaL0 = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.Hubblepar = np.fromfile(f, count=1, dtype=bo_mark+&#39;f8&#39;)
        self.version = np.fromfile(f, count=1, dtype=bo_mark+&#39;a96&#39;)
        self.pad2=np.fromfile(f, count=1, dtype=bo_mark+&#39;i4&#39;)
        if type(fname) is types.StringType: f.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These lines read and store the values of the quantities saved in the header of the file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def read_gif2_file(fname):
    bo_mark = &#39;&amp;gt;&#39;
    f=open(fname, &#39;rb&#39;)
    # start by reading in header:    
    ghead=head(f)
    npt=ghead.npart.sum()````
    
This is the function we use to read the files: it set the endianness to &amp;quot;big-endian&amp;quot;, open the file in read-only mode, read the header and extract the total number of particles.    
````python
f.seek(4, 1)
    pos=np.fromfile(f, count=npt*3, dtype=bo_mark + &#39;f4&#39;).reshape((npt, 3))
    return pos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;f.seek(offset, from_what)&lt;/code&gt; move the pointer through the file to read bits from one position to another. The position is computed from adding &lt;code&gt;offset&lt;/code&gt; to a reference point; the reference point is selected by the &lt;code&gt;from_what&lt;/code&gt; argument. A &lt;code&gt;from_what&lt;/code&gt; value of 0 measures from the beginning of the file, 1 uses the current file position, and 2 uses the end of the file as the reference point. &lt;code&gt;from_what&lt;/code&gt; can be omitted and defaults to 0, using the beginning of the file as the reference point (from the &lt;a href=&#34;http://docs.python.org/tutorial/inputoutput.html&#34; title=&#34;Python documentation&#34;&gt;Python documentation&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This last piece of code read the GIF2 galaxy catalogue (an ASCII file with &amp;ldquo;space separated values&amp;rdquo;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#! /usr/bin/env python

file = open(&#39;lcdm_galaxy_cat.z0.00&#39;, &#39;rb&#39;)
i = 0
for riga in file.readlines():
    parole = riga.split()
    if len(parole) == 11:
        print &amp;quot;iterazione &amp;quot;, i 
        print &amp;quot;aggiungo &amp;quot;
        x.append(parole[5])
        y.append(parole[6])
        z.append(parole[7])
    i+=1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>GIF2 substructures coordinates correction</title>
      <link>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</link>
      <pubDate>Sun, 04 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/gif2-substructures-coordinates-correction/</guid>
      <description>&lt;p&gt;I used this script to change the coordinates of the substructures in the GIF2 simulation output from the center of mass coordinates to the global ones. The substructures were stored in our server in files referring to the index of the halo to which they belong and their coordinates were respect to the center of the halo. For each halo this script read the subhaloes center of mass coordinates in kpc and change them to global coordinates in Mpc managing the periodic boundary conditions .&lt;/p&gt;

&lt;!-- TEASER_END --&gt;    

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
import numpy as np
import tables as tb
import time
import sys

&amp;quot;&amp;quot;&amp;quot;Legge il file con id e centri degli aloni con sottostrutture, per
ogni alone (id) apre il file Sub.3..053.gv, legge le coordinate
alle colonne 8, 9, 10 (in kpc!!!) e le corregge (tenendo conto delle
condizioni periodiche) con le coordinate del centro prese dalla lista
degli aloni iniziale.  Le coordinate vengono aggiunte ad un vettore
che alla fine viene salvato in un file hdf5.
&amp;quot;&amp;quot;&amp;quot;

t = time.time()
print &amp;quot;Start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As usual: imports, documentation and timing initialization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;halo_centers_file = &#39;haloes_with_substructures_centers.h5&#39;

h5 = tb.openFile(halo_centers_file, &#39;r&#39;)
haloes = h5.root.data.read()
h5.close()

haloes = haloes.astype(float)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code reads the halo centers from a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;substructure_coord = np.empty((0, 3))
files_num = haloes[:, 0].shape[0]
void_files = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we create the first (void) record of the substructures coordinates table, find the number of haloes and create a variable which will tell us how many haloes without subtructures we have.&lt;/p&gt;

&lt;p&gt;Now, for each halo we&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in xrange(files_num):
    print &amp;quot;Loop &amp;quot;, i, &amp;quot; di &amp;quot;, files_num 
    sub_file = &amp;quot;cm/Sub.3.&amp;quot;+&#39;%07d&#39;%int(haloes[i,0])+&amp;quot;.053.gv&amp;quot;
    ````
    
open the related substructures file     
````python
try:
        sub_coord = np.genfromtxt(sub_file, dtype=&#39;float&#39;, usecols=(8, 9, 10))
        file_check = True
    except:
        print &amp;quot;Void file &amp;quot;, sub_file
        file_check = False
        void_files += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;try to read the substructures coordinates, if it fails, we assume that the file is empty (and the halo has no substructures). In this case we increment the counter of the empy files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if file_check:
        if sub_coord.ndim == 1:
            sub_coord = sub_coord.reshape((1, sub_coord.shape[0]))
            #print &amp;quot;sh min &amp;quot;, np.amin(sub_coord, 0)
            #print &amp;quot;sh min &amp;quot;, np.amax(sub_coord, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This reshape the array in case we have only one subhalo: in this case (I hope I remember correct!:P) instead of one row and three columns we should have three values, so we have to reshape the array to pass it to the rest of the code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
            sub_x = sub_coord[:, 0]/1000. + haloes[i,1]
            if not np.all(sub_x &amp;amp;gt; 0):
                sub_x[sub_x&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_x 110]-=110
            if not (np.all(sub_x &amp;amp;gt; 0) and np.all(sub_x  0)
                print sub_x 
                print haloes[i, 1:3]
                sys.exit()
            sub_y = sub_coord[:, 1]/1000. + haloes[i,2]
            if not np.all(sub_y &amp;amp;gt; 0):
                sub_y[sub_y&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_y 110]-=110
            if not (np.all(sub_y &amp;amp;gt; 0) and np.all(sub_y  0)
                print sub_y
                print haloes[i, 1:3]
                sys.exit()
            sub_z = sub_coord[:, 2]/1000. + haloes[i,3]
            if not np.all(sub_z &amp;amp;gt; 0):
                sub_z[sub_z&amp;amp;lt;0]+=110 # condizioni periodiche
            if not np.all(sub_z 110]-=110
            if not (np.all(sub_z &amp;amp;gt; 0) and np.all(sub_z  0)
                print sub_z 
                print haloes[i, 1:3]
                sys.exit()
            substructure_coord = np.vstack((substructure_coord, np.hstack((sub_x.reshape((sub_x.shape[0], 1)), 
                                                                           sub_y.reshape((sub_y.shape[0], 1)), 
                                                                           sub_z.reshape((sub_z.shape[0], 1))))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This corrects the coordinates keeping in mind the periodic boundary conditions and add the new substructures coordinates to the corrected substructures array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;except:
            print &amp;quot;file &amp;quot;, sub_file
            print &amp;quot;sub_coord.shape &amp;quot;, sub_coord.shape
            print &amp;quot;sub_coord &amp;quot;, sub_coord
            print &amp;quot;haloes coord &amp;quot;, haloes[i, :]
            print &amp;quot;exit&amp;quot;
            sys.exit()
    else:
        pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If something goes wrong, we handle the failure printing some information.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h5 = tb.openFile(&#39;sub_haloes_global_coords.h5&#39;, &#39;w&#39;)
h5.createArray(h5.root, &#39;data&#39;, substructure_coord)
h5.flush()
h5.close()
print &amp;quot;Done in &amp;quot;, time.time()-t, &amp;quot; with &amp;quot;, void_files, &amp;quot; void files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, we save the array in an HDF5 file!:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #6: adding gas!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-6-adding-gas/</link>
      <pubDate>Tue, 29 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-6-adding-gas/</guid>
      <description>&lt;p&gt;As we have seen, cosmological N-body simulations consider only gravitational interactions. This is good for large scale distribution of matter such as galaxy distribution or cluster formation, but if we are interested in smaller details we have to add gas physics.&lt;br /&gt;
There are two main approaches, completely different one from the other.&lt;/p&gt;

&lt;p&gt;The first approach uses a grid to solve fluid equations. Fluid interactions are short range ones so we can use information from few nearby points to compute and evolve the quantities we are interested in at a point. Fluid must also respond to the gravitational field of the matter distribution. One can easily study shocks and discontinuities with this type of code.&lt;br /&gt;
It is also possible to improve the resolution using mesh refinement but it&amp;rsquo;s important to bear in mind that the resolution of mass particles and of hydrodynamics should be improved together. Grid codes can be expanded to include other effects such as magnetohydrodynamics.&lt;br /&gt;
The second way is the Smoothed Particles Hydrodynamics (SPH). In these type of algorithms the fluid properties (pressure, density, temperature, &amp;hellip;) at any point can be found by averaging over particles in a region using a weight function. Because of this smoothing functions it has poor resolution of shocks and discontinuities and low resolution in low density regions. However these codes are quite easily to implement and has high resolution in high density regions.&lt;/p&gt;

&lt;p&gt;In both types of codes effects such as elementary chemical reactions, e.g. formation of hydrogen, molecules, cooling, heating, etc can be added because they are local effects. On the contrary star formation, feedback from stellar and other source, radiation transport, and so on are non-local and include a big range of scales so are difficult to implement.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #5: initial conditions!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-5-initial-conditions/</link>
      <pubDate>Wed, 16 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-5-initial-conditions/</guid>
      <description>

&lt;p&gt;After we had a look on why we need cosmological simulations, what they are and how they are performed, it&amp;rsquo;s time to learn more about the preparation of a simulation, in particular what are initial conditions and how we build them.&lt;/p&gt;

&lt;p&gt;Usually, for a cosmological simulation, all the fields are linear so we can use linear theory to compute them. In linear theory the density contrast evolves as a combination of a growing and a decaying mode, but only the first survives, so we can set the initial system considering only the growing mode, and because until linearity is verified the density field and velocities are related to the gravitational potential we only need to generate the gravitational field and with it we can produce density and velocity fields.&lt;br /&gt;
Once we have the initial potential there are two ways to generate the initial conditions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;we can uniformly distribute the particles in the simulation box and choose the masses according to the gravitational field; velocities can be zero, in which case we increase the potential to account for the decaying mode, or non-zero and appropriate for the growing mode&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;we can displace uniformly distributed particles using the velocities of the linear theory; the displacement should be smaller than the interparticle separation or we can obtain a wrong power spectrum because a bigger displacement would lead the trajectories to cross and in this case, formally, the density contrast grows to infinite (this is not acceptable in linear approximation) and after that it decreases while the particles go away and this is not physical&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The relation that linking velocity and gravitational potential in the second entry:&lt;br /&gt;
$\frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t}+2\frac{\mathrm{d}a/\mathrm{d}t}{a}\mathbf{v}\propto \mathbf{g}=-\frac{\nabla\phi}{a}$
is the same both in linear theory and in the Zel&amp;rsquo;dovich approximation (this takes the non-linear equations together with the acceleration given by the linear theory to obtain results in the quasi-linear regime), so it&amp;rsquo;s often said that the Zel&amp;rsquo;dovich approximation is used to set up initial conditions.&lt;/p&gt;

&lt;p&gt;Given that, the homogeneity of the initial unperturbed distribution it&amp;rsquo;s very important because any inhomogeneity would combine with density perturbations modifying initial conditions. This homogeneity can be thought in different ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a homogeneous but not random way is to put particles on a cubic grid but it could produce visible features due to the regular distribution&lt;/li&gt;
&lt;li&gt;we can consider putting particles at random positions but the $\sqrt{n}$ fluctuations will result in spurious clustering that will eventually dominate the fluctuations we want to simulate&lt;/li&gt;
&lt;li&gt;a good compromise could be placing particles in lattice cells with a random displacement from the center: this remove regularity but maintain uniformity&lt;/li&gt;
&lt;li&gt;the last possibility are &amp;ldquo;glass initial conditions&amp;rdquo; obtained by evolving an arbitrary distribution of particles in an n-body simulation with a repulsive force; it was invented by Simon White and the name refers to the molecular structure of glass, uniform but not regular&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Considering the last two possibilities, it&amp;rsquo;s worth noting that the former still has a lot of noise, and the second is better than the grid only for aesthetic reasons.&lt;/p&gt;

&lt;h2 id=&#34;the-initial-gravitational-potential:113&#34;&gt;The initial gravitational potential&lt;/h2&gt;

&lt;p&gt;In standard cosmological models the initial perturbations density field is Gaussian. Since the linear evolution doesn&amp;rsquo;t modify the statistics of density fields except for evolving the amplitude of perturbations, and because the potential and the density contrast are linked by a linear relation, the gravitational potential is also a Gaussian random field, and such a field is completely described in terms of the power spectrum of density perturbations. The Fourier components of a Gaussian random field (both real and imaginary part) are random numbers with a normal distribution and variance proportional to the power spectrum at that wave number. Also the phase of this random numbers has to be random, and changing it the resulting field is completely different.
We can generate the gravitational field in the Fourier space just using these properties and inverse transform it (or the force) to obtain the initial potential in real space. If we use adaptive mesh refinement codes we need Gaussian random fields with a variable resolution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #3: force calculation!</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-3-force-calculation/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-3-force-calculation/</guid>
      <description>

&lt;p&gt;In the previous posts we wrote about cosmological simulations, why we need them, how they are performed and which are the important things to care about.
Now we are ready to learn the algorithms developed to compute gravitational forces between the particles.&lt;/p&gt;

&lt;h3 id=&#34;direct-summation-pp-method:114&#34;&gt;Direct summation: PP method&lt;/h3&gt;

&lt;p&gt;The raw approach is to calculate forces between all pairs in the simulation. This works well for less than $10^3$. The number of pairs, and therefore the computational time, scales as $N^2$, so this method that is usually unacceptable given the number of particles in a simulation.&lt;br /&gt;
It&amp;rsquo;s difficult to implement periodic boundary conditions in this method (because you have to manage it by hand) and the only convenient way to do this is to use the Ewald summation.&lt;br /&gt;
This method is mostly used to test other methods.&lt;/p&gt;

&lt;h3 id=&#34;tree-method:114&#34;&gt;Tree method&lt;/h3&gt;

&lt;p&gt;The disadvantage of the pp-method comes from having to compute the interaction of any new particle with all the others. The tree method, on the contrary, approximate the force of a distant group of particles with the force exerted by only one particle, in the center of mass of the group, with mass equal the total mass of the group. In this way the computation scales as $N\log N$.&lt;br /&gt;
To create the groups the particles are divided and arranged in a tree structure, that is, the initial volume is divided, and every resulting volume divided again. This procedure goes on until in the resulting volumes there&amp;rsquo;s only one particle. In this situation it&amp;rsquo;s very important the &amp;ldquo;cell acceptance criterion&amp;rdquo; that decide whether or not a cell is far enough to be used as it is or if it has to be divided into its subvolumes.&lt;br /&gt;
The accuracy and the speed of this method can be improved storing some information about the particles in the volumes (such as moments of the mass distribution, usually the quadrupole).&lt;br /&gt;
Moreover, close particles can share information about distant groups because the force is very similar, and the tree can be parallelized efficiently. However, it&amp;rsquo;s not so easy to consider periodic boundary conditions.&lt;/p&gt;

&lt;h3 id=&#34;fast-multipole-method:114&#34;&gt;Fast multipole method&lt;/h3&gt;

&lt;p&gt;This method is an improved version of the tree method. It is based on including higher moments of the mass distribution in cells and some other optimizations. This method has a computational costs which scales as $N$.&lt;/p&gt;

&lt;p&gt;However all these method has problems with the open boundary conditions and it&amp;rsquo;s difficult to adapt it to the cosmological needs. An extension of the tree method with explicit momentum conservation has been also developed.&lt;/p&gt;

&lt;h3 id=&#34;particle-mesh-method:114&#34;&gt;Particle-mesh method&lt;/h3&gt;

&lt;p&gt;This is the first method used extensively for cosmological simulations and the first used for simulation with order of $10^5$ particles. The PM method solve the Poisson equation (partial differential equation that links the gravitational field with the mass distribution) in the Fourier space, where it is a simple algebraic equation, using the FFT (Fast Fourier Transform) routine to change from coordinate space to the Fourier space and viceversa&amp;nbsp;. FFT requires to sample the functions at uniformly spaced points and here we use a grid (mesh). We compute density at the grid points using weight functions on the particles representing the density and velocity fields.&lt;br /&gt;
The use of Fourier methods automatically include periodic boundary conditions without no additional effort (because of the nature of periodic function in the Fourier space) and the use of the mesh automatically soften the force at small scales (because of the interpolation needed to compute the gravitational field on the grid) but it underestimate the force at scales larger (toughly up to 3 times) than the softening length.&lt;br /&gt;
This method is parallelized using parallel FFT and dividing the volume among the processors.&lt;br /&gt;
Softening at the mesh scale give collisionless evolution but the code cannot resolve structure at scales smaller than the mesh scale, moreover the mesh makes the force anisotropic at small scales. This happens because the grid points don&amp;rsquo;t sample very well the filed on such small scales.&lt;/p&gt;

&lt;h3 id=&#34;adaptive-mesh-refinement:114&#34;&gt;Adaptive-mesh refinement&lt;/h3&gt;

&lt;p&gt;In AMR methods the mesh is refined in high density regions using a finer grid, but it&amp;rsquo;s important to pay attention to the conservation of momentum and angular momentum switching from a grid to another.&lt;/p&gt;

&lt;h3 id=&#34;p3m-particle-particle-particle-mesh:114&#34;&gt;P3M: particle-particle+particle-mesh&lt;/h3&gt;

&lt;p&gt;The idea is to add a correction to the force calculated with the PM method by using the PP method on the closest particles. The correction is assumed to be isotropic and depend only upon the distance. Usually it&amp;rsquo;s considered a distance of the order of 2 times the internode mesh distance. This method has been parallelized but with some problems, also because load balancing is quite difficult to achieve.&lt;/p&gt;

&lt;p&gt;Some other problems are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the correction to the force is isotropic but the PM method has anisotropies at small scales&lt;/li&gt;
&lt;li&gt;the correction is at scales up to 2 times the grid scale but the PM method underestimate the force on scales larger than these&lt;/li&gt;
&lt;li&gt;the refined interparticle softening is at scales smaller than the interparticle separation and this can lead to two-body scattering and relaxation&lt;/li&gt;
&lt;li&gt;P3M simulations slow down at late times when the distribution of particles becomes highly clustered because the short range force dominate the compute operations&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tree-pm-tpm:114&#34;&gt;Tree+PM: TPM&lt;/h3&gt;

&lt;p&gt;Some hybrid codes has been developed trying to combine the PM and the Tree methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Grid of Trees PM (GOTPM)replaces the PP part of P3M with a local tree in each region. This resolve only the last problem of the P3M.&lt;/li&gt;
&lt;li&gt;TPM correct the short range force only if the particle is in a highly dense region and with a tree to compute the forces.&lt;/li&gt;
&lt;li&gt;TreePM divide the force in long-range and short-range instead of correcting the PM force at a certain scale, greater than that of the P3M. The short-range forces are calculated with a global tree.&lt;/li&gt;
&lt;li&gt;The Adaptive TreePM (ATreePM) try to resolve the two-body relaxation and scattering using an adaptive softening length, determined by the local density. To ensure momentum conservation force is simmetrized for particles closer than the softening length. The softening correspond to consider the particles with a density profile and not only mass points and if the softening length is different for two close particles, the force they feel is different between them. This happens &amp;nbsp;because particle A feel a force due to its entire mass and a certain fraction of particle B mass, but particle B feels a force due to its own mass and a different fraction of the mass of particle A. In this case the forces are symmetrized taking the mean of the two forces.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To compare different methods we can consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dynamic range, that is the range of scales over which the force is computed reliably. Usually the limit is at small scales rather than at large scales.&lt;/li&gt;
&lt;li&gt;The code should integrate the equation of motion in a reproducible way and momentum should be conserved.&lt;/li&gt;
&lt;li&gt;The code should be efficient and run with the minimum possible time&lt;/li&gt;
&lt;li&gt;Requirement of memory and other resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>