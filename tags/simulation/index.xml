<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Simulation on Post It!</title>
    <link>http://brunettoziosi.eu/tags/simulation/</link>
    <description>Recent content in Simulation on Post It!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Jun 2015 11:55:02 +0200</lastBuildDate>
    <atom:link href="http://brunettoziosi.eu/tags/simulation/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dockerized Starlab</title>
      <link>http://brunettoziosi.eu/posts/dockerized-starlab/</link>
      <pubDate>Mon, 15 Jun 2015 11:55:02 +0200</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/dockerized-starlab/</guid>
      <description>

&lt;p&gt;Once you have &lt;a href=&#34;../docker-installation&#34;&gt;Docker installed&lt;/a&gt;, you may want to see it
in action. Here I demonstrate how to use Docker in a slightly different way respect
what you can find around (AKA use Docker to set up a web-server or something similar).&lt;/p&gt;

&lt;p&gt;I think Docker is a great solution to deal with easiness of installation and reproducibility
in science. LXD would be probable be better. LXD already provide unprivileged containers
AND is more about a container containing more than an application, while Docker is
based on the idea of one container for a single app. However, I still have to try LXD.&lt;/p&gt;

&lt;p&gt;Here I will show you how to use Docker to install and run &lt;a href=&#34;http://brunettoziosi.eu/posts/starlab-gpu-installation/&#34;&gt;Starlab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;NB: nVidia (AKA the most annoying GPU producer in the world) drivers, in addition to be
the worst Linux GPU drivers, require a system-dependent installation. This means that
you can&amp;rsquo;t just download the Docker image from &lt;a href=&#34;https://registry.hub.docker.com/repos/brunetto/&#34;&gt;my Docker registry&lt;/a&gt; and run a container
from it, but you need to download the &lt;code&gt;Dockerfile&lt;/code&gt; and build the image on your own.
You can use the image I provide ONLY if you run the non-GPU StarLab version.
And to do this you need to have a loooot of time to wait for the simulations to finish.&lt;/p&gt;

&lt;h2 id=&#34;create-a-docker-image:e7da96f04bfa3eab144a0265a4f6de5a&#34;&gt;Create a Docker image&lt;/h2&gt;

&lt;p&gt;The image I&amp;rsquo;m going to create contains:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;our modified StarLab version (it contains updated stellar evolution recipes), both GPU and non GPU version&lt;/li&gt;
&lt;li&gt;the same version with an &lt;a href=&#34;http://adsabs.harvard.edu/abs/1991RMxAA..22..255A&#34;&gt;Allen-Santillan&lt;/a&gt; galactic tidal field,
corrected for the non-inertial reference frame used in StarLab
(at the moment this version is not working, probably because a problem in the timestep calculation, but I am working on it!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can use the &lt;a href=&#34;http://www.sns.ias.edu/~starlab/download/&#34;&gt;public version&lt;/a&gt; and correct the
&lt;code&gt;Dockerfile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It is possible to download and extract the StarLab sourced directly from the Internet but I
prefer to have everything already in the folder.&lt;/p&gt;

&lt;p&gt;First of all, create a new empty folder and &lt;code&gt;cd&lt;/code&gt; into it. Then, copy the StarLab sources
and the docker file into that folder.
Mine looks like that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;starlabDocker.tar.gz
	|-sapporo
	|-starlab
	|-starlabAS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;starlabAS&lt;/code&gt; only contains the files that differ from the versio without the Allen-Santillan tidal field.&lt;/p&gt;

&lt;p&gt;Then, you need a &lt;code&gt;Dockerfile&lt;/code&gt;. The &lt;code&gt;Dockerfile&lt;/code&gt; tells Docker what it has to do
in order to create your image. Which base images to use (if any), which packages to download and install and so on.
Mine is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FROM ubuntu:14.04

MAINTAINER brunetto ziosi &amp;lt;my email hehe&amp;gt;

# For the public version of StarLab4.4.4, see http://www.sns.ias.edu/~starlab/

ENV DEBIAN_FRONTEND noninteractive

ENV STARLAB_FILE starlabDocker.tar.gz

# Copy StarLab bundle into the image
COPY $STARLAB_FILE /

# This has to be set by hand and MUST be the same of the host
##############
# longisland #
##############
# ENV CUDA_DRIVER 340.46
# ENV CUDA_INSTALL http://us.download.nvidia.com/XFree86/Linux-x86_64/${CUDA_DRIVER}/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run
# ENV CUDA_TOOLKIT cuda_6.0.37_linux_64.run
# ENV CUDA_TOOLKIT_DOWNLOAD http://developer.download.nvidia.com/compute/cuda/6_0/rel/installers/$CUDA_TOOLKIT
##############
#    uno     #
##############
# ENV CUDA_DRIVER 331.38
# ENV CUDA_INSTALL http://us.download.nvidia.com/XFree86/Linux-x86_64/${CUDA_DRIVER}/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run
# ENV CUDA_TOOLKIT cuda_5.5.22_linux_64.run
# ENV CUDA_TOOLKIT_DOWNLOAD http://developer.download.nvidia.com/compute/cuda/5_5/rel/installers/$CUDA_TOOLKIT
##############
#   spritz   #
##############
ENV CUDA_DRIVER 331.113
ENV CUDA_INSTALL http://us.download.nvidia.com/XFree86/Linux-x86_64/${CUDA_DRIVER}/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run
ENV CUDA_TOOLKIT cuda_5.5.22_linux_64.run
ENV CUDA_TOOLKIT_DOWNLOAD http://developer.download.nvidia.com/compute/cuda/5_5/rel/installers/$CUDA_TOOLKIT
################
#  sfursat     #
# to be tested #
################
# ENV CUDA_DRIVER 270.41.19
# ENV CUDA_INSTALL http://us.download.nvidia.com/XFree86/Linux-x86_64/${CUDA_DRIVER}/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run
# ENV CUDA_TOOLKIT ????
# ENV CUDA_TOOLKIT_DOWNLOAD ????????

# Update and install minimal and clean up packages
RUN apt-get update --quiet &amp;amp;&amp;amp; apt-get install --yes \
 --no-install-recommends --no-install-suggests \
 build-essential module-init-tools wget libboost-all-dev   \
&amp;amp;&amp;amp; apt-get clean &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*

# Install CUDA drivers
RUN wget $CUDA_INSTALL -P /tmp --no-verbose \
      &amp;amp;&amp;amp; chmod +x /tmp/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run \
      &amp;amp;&amp;amp; /tmp/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run -s -N --no-kernel-module \
      &amp;amp;&amp;amp; rm -rf /tmp/*

# Install CUDA toolkit
RUN wget $CUDA_TOOLKIT_DOWNLOAD &amp;amp;&amp;amp; chmod +x $CUDA_TOOLKIT \
&amp;amp;&amp;amp; ./$CUDA_TOOLKIT -toolkit -toolkitpath=/usr/local/cuda-site -silent -override \
&amp;amp;&amp;amp; rm $CUDA_TOOLKIT

# Set env variables
RUN echo &amp;quot;PATH=$PATH:/usr/local/cuda-site/bin&amp;quot; &amp;gt;&amp;gt; .bashrc          \
&amp;amp;&amp;amp; echo &amp;quot;LD_LIBRARY_PATH=/usr/local/cuda-site/lib64&amp;quot; &amp;gt;&amp;gt; .bashrc   \
&amp;amp;&amp;amp; . /.bashrc \
&amp;amp;&amp;amp; ldconfig /usr/local/cuda-site/lib64

# Install StarLab w/ and w/o GPU, w/ and w/o tidal fields
RUN tar -xvf $STARLAB_FILE &amp;amp;&amp;amp; rm $STARLAB_FILE \
&amp;amp;&amp;amp; cp -r starlab starlab-no-GPU               \
&amp;amp;&amp;amp; cp -r starlab starlabAS-no-GPU             \
&amp;amp;&amp;amp; cp -r starlab starlabAS-GPU                \
&amp;amp;&amp;amp; mv starlab starlab-GPU

# Tidal field version only has 5 files different, 
# so we can copy them into a copy of the non TF version:

# starlab/src/node/dyn/util/add_tidal.C
# starlab/src/node/dyn/util/dyn_external.C
# starlab/src/node/dyn/util/dyn_io.C
# starlab/src/node/dyn/util/set_com.C
# starlab/src/node/dyn/util/dyn_story.C

RUN cp starlabAS/*.C starlabAS-no-GPU/src/node/dyn/util/ \
&amp;amp;&amp;amp; cp starlabAS/*.C starlabAS-GPU/src/node/dyn/util/     \
&amp;amp;&amp;amp; cp starlabAS/dyn.h starlabAS-no-GPU/include/          \
&amp;amp;&amp;amp; cp starlabAS/dyn.h starlabAS-GPU/include/             \
&amp;amp;&amp;amp; rm -rf starlabAS

# Compile sapporo
RUN cd sapporo/ &amp;amp;&amp;amp; make &amp;amp;&amp;amp; bash compile.sh &amp;amp;&amp;amp; cd ../

# With and w/o GPU and w/ and w/o AS tidal fields
RUN cd /starlab-GPU/ &amp;amp;&amp;amp; ./configure --with-f77=no &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install &amp;amp;&amp;amp; cd ../ \
&amp;amp;&amp;amp; mv /starlab-GPU/usr/bin slbin-GPU &amp;amp;&amp;amp; rm -rf /starlab-GPU \
&amp;amp;&amp;amp; cd /starlabAS-GPU/ &amp;amp;&amp;amp; ./configure --with-f77=no &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install &amp;amp;&amp;amp; cd ../ \
&amp;amp;&amp;amp; mv /starlabAS-GPU/usr/bin slbinAS-GPU &amp;amp;&amp;amp; rm -rf /starlabAS-GPU \
&amp;amp;&amp;amp; cd /starlab-no-GPU/ &amp;amp;&amp;amp; ./configure --with-f77=no --with-grape=no &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install &amp;amp;&amp;amp; cd ../ \
&amp;amp;&amp;amp; mv /starlab-no-GPU/usr/bin slbin-no-GPU &amp;amp;&amp;amp; rm -rf /starlab-no-GPU \
&amp;amp;&amp;amp; cd /starlabAS-no-GPU/ &amp;amp;&amp;amp; ./configure --with-f77=no --with-grape=no &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install &amp;amp;&amp;amp; cd ../ \
&amp;amp;&amp;amp; mv /starlabAS-no-GPU/usr/bin slbinAS-no-GPU &amp;amp;&amp;amp; rm -rf /starlabAS-no-GPU

# Default command.
ENTRYPOINT [&amp;quot;/bin/bash&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first part of the &lt;code&gt;Dockerfile&lt;/code&gt; specify to use &lt;code&gt;Ubuntu 14.04&lt;/code&gt; as base image
(a special version customized for Docker). Then it lists me as maintainer of the image.
What follows are environment variables needed for the installation.
&lt;code&gt;COPY&lt;/code&gt; copy the Starlab sources from the host folder to the image &lt;code&gt;/&lt;/code&gt; folder.
After that I set the right variables needed to install the right CUDA drivers
and libraries for each system.&lt;/p&gt;

&lt;p&gt;After setting the environment variables, the &lt;code&gt;RUN&lt;/code&gt; command is used to launch
the update of the system indexes and packages and the installations of the needed
build tools.&lt;/p&gt;

&lt;p&gt;Then we can install the CUDA drivers and the CUDA libraries.&lt;/p&gt;

&lt;p&gt;Because Docker add a layer for each of the Docker commands used, I minimize the number of layers
running more that one bash command chaining them with &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following steps are extract the sources, copy the files in the right places and compile
sapporo and Starlab. Sapporo is the library that allow Starlab (developed for GRAPE) to run on the GPUs.&lt;/p&gt;

&lt;p&gt;The final line tells Docker that a container based on this image should start with /bin/bash active.&lt;/p&gt;

&lt;p&gt;To build the image just run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time docker build --force-rm=true -t &amp;lt;your registry name&amp;gt;/starlab-cuda-&amp;lt;driver version&amp;gt;:$(date +&amp;quot;%Y%m%d&amp;quot;) .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is my build line containing&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;time&lt;/code&gt; command, just to know how log does it take to build the image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker build --force-rm=true&lt;/code&gt; build the image removing intermediate layer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-t&lt;/code&gt; to tag the image you create with a name you like, I use my Dcoker Hub username,
the name of the program I&amp;rsquo;m dockerizing, if using cuda, the driver version and the build date,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.&lt;/code&gt; the final dot is not a typo, it tells Docker to build an image using the &lt;code&gt;Dockerfile&lt;/code&gt;
in the current folder.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s assume that the container name is &lt;code&gt;me/starlab-cuda-340.46-6.0.37-2015-06-15&lt;/code&gt;
At the end of the process you can check if the image was successfully created (ok, you can do this also from the errors!)
by running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker images

REPOSITORY                                   TAG                 IMAGE ID            
me/starlab-cuda-340.46-6.0.37-2015-06-15    20150615            b073d414323f        
                                            CREATED             VIRTUAL SIZE
                                            37 minutes ago      5.272 GB
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;run-a-starlab-container:e7da96f04bfa3eab144a0265a4f6de5a&#34;&gt;Run a StarLab container&lt;/h2&gt;

&lt;p&gt;Now that you created the image, it&amp;rsquo;s time to run a container with it.
To create and run a container based on your newly created image run:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -ti --device /dev/nvidia0:/dev/nvidia0 \
                 --device /dev/nvidia1:/dev/nvidia1 \
                 --device /dev/nvidiactl:/dev/nvidiactl \
                 --device /dev/nvidia-uvm:/dev/nvidia-uvm \
                 -v       &amp;lt;abs path to host folder&amp;gt;:&amp;lt;container folder&amp;gt; \
                          me/starlab-cuda-340.46-6.0.37-2015-06-15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker run&lt;/code&gt; is obvious&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-ti&lt;/code&gt; means open a interactive pseudo tty (that is, more or less, give me a terminal inside the
container, once started, where I can run commands)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--device&lt;/code&gt; specify which devices to attach; in this case I am connecting 2 CUDA GPUs
and allow for the Unified Virtual Memory to be used (it works only from CUDA6)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-v &amp;lt;abs path to host folder&amp;gt;:&amp;lt;container folder&amp;gt;&lt;/code&gt; allow to share a folder between host and container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;me/starlab-cuda-340.46-6.0.37-2015-06-15&lt;/code&gt; is the name of the image from which to create the container&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can check by running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker ps [-a]

CONTAINER ID        IMAGE                                              
ccdffc10c680        me/starlab-cuda-340.46-6.0.37-2015-06-15   
                     COMMAND             CREATED             
                     &amp;quot;/bin/bash&amp;quot;         15 seconds ago    
                     STATUS              PORTS               NAMES
                     Up 15 seconds                           adoring_turing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-a&lt;/code&gt; flags tells Docker to show you also the stopped containers. Note that the container
has a random name given by Docker.&lt;/p&gt;

&lt;p&gt;It is also possible to directly run commands just after the container creation,
for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ time echo &amp;quot;Hello world&amp;quot;
Hello world
real    0m0.000s
user    0m0.000s
sys     0m0.000s

$ time docker run ubuntu:14.04 /bin/echo &#39;Hello world&#39;
Hello world
real    0m0.219s
user    0m0.028s
sys     0m0.005s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the second command ran into a docker container.&lt;/p&gt;

&lt;p&gt;We can do something better: we want a script that creates a container,
start it, run some commands and then clean everything.&lt;/p&gt;

&lt;p&gt;This could be quite easy, but we are using StarLab, that makes heavy use of pipes.
I found three solutions to get it works, the last being the better.&lt;/p&gt;

&lt;p&gt;The first attempt is something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash # shebang line to specify the interpreter

set -x # set -x tells bash to echo the command is going to run

# Create a docker container with devices and volumes and give it a name
docker create --name sltest -i -t \
--device /dev/nvidia0:/dev/nvidia0 \
--device /dev/nvidia1:/dev/nvidia1 \
--device /dev/nvidiactl:/dev/nvidiactl \
--device /dev/nvidia-uvm:/dev/nvidia-uvm \
me/starlab-cuda-340.46-6.0.37-2015-06-15

# Start the container
docker start sltest

# Exec commands to create StarLab initial conditions
(docker exec -i sltest /slbin/makeking -n 100 -w 5 -i -u ) &amp;gt; makeking.out
(docker exec -i sltest /slbin/makemass -f 8 -l 0.1 -u 40 ) &amp;lt; makeking.out &amp;gt; makemass.out
(docker exec -i sltest /slbin/add_star -R 1 -Z 0.1       ) &amp;lt; makemass.out &amp;gt; add_star.out
(docker exec -i sltest /slbin/scale    -R 1 -M 1         ) &amp;lt; add_star.out &amp;gt; ics.txt

# Start kira
(docker exec -i sltest /slbin/kira -t 3 -d 1 -D 1 -f 0 -n 10 -e 0 -B -b 1) &amp;lt; ics.txt &amp;gt; out.txt 2&amp;gt; err.txt

# Stop and delete the container
docker stop sltest
docker rm sltest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example make use of the STDIN/ERR/OUT redirection, but does not always work
very well.&lt;/p&gt;

&lt;p&gt;The second attempt, a little better is&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash 
set -x 

# Create env variables for the folders
LOCAL_FOLDER=~/starlab-results
DOCKER_FOLDER=/starlab-results

# Create a docker container with devices and volumes and give it a name
docker create --name sltest -i -t \
--device /dev/nvidia0:/dev/nvidia0 \
--device /dev/nvidia1:/dev/nvidia1 \
--device /dev/nvidiactl:/dev/nvidiactl \
--device /dev/nvidia-uvm:/dev/nvidia-uvm \
-v $LOCAL_FOLDER:$DOCKER_FOLDER \
me/starlab-cuda-340.46-6.0.37-2015-06-15

# Start the container
docker start sltest

# Exec commands to create StarLab initial conditions
(docker exec -i sltest -c &amp;quot;/slbin/makeking -n 100 -w 5 -i -u ) &amp;gt; $DOCKER_FOLDER/makeking.out&amp;quot;
(docker exec -i sltest -c &amp;quot;/slbin/makemass -f 8 -l 0.1 -u 40 ) &amp;lt; $DOCKER_FOLDER/makeking.out &amp;gt; $DOCKER_FOLDER/makemass.out&amp;quot;
(docker exec -i sltest -c &amp;quot;/slbin/add_star -R 1 -Z 0.1       ) &amp;lt; $DOCKER_FOLDER/makemass.out &amp;gt; $DOCKER_FOLDER/add_star.out&amp;quot;
(docker exec -i sltest -c &amp;quot;/slbin/scale    -R 1 -M 1         ) &amp;lt; $DOCKER_FOLDER/add_star.out &amp;gt; $DOCKER_FOLDER/ics.txt&amp;quot;

# Start kira
docker exec -i sltest bash -c &amp;quot;/slbin/kira -t 3 -d 1 -D 1 -f 0 \
 -n 10 -e 0 -B -b 1 &amp;lt; $DOCKER_FOLDER/ics.txt \
 &amp;gt; $DOCKER_FOLDER/out.txt 2&amp;gt; $DOCKER_FOLDER/err.txt&amp;quot;

# Stop and delete the container
docker stop sltest
docker rm sltest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this second example we make use of the internal container folder attached to a host system folder.
We will found our files in &lt;code&gt;~/starlab-results&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;However, the way I prefer is to make the container bash read a script in the
exchange folder. To do this, we need two files.&lt;br /&gt;
The first create the container and launch the second, located into the exchange folder.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat dockerized_starlab.sh

#!/bin/bash
set -x
# Create a docker container with devices and volumes and give it a name
CONTAINER_NAME=test-001

docker create --name $CONTAINER_NAME -i -t \
--device /dev/nvidia0:/dev/nvidia0 \
--device /dev/nvidia1:/dev/nvidia1 \
--device /dev/nvidiactl:/dev/nvidiactl \
--device /dev/nvidia-uvm:/dev/nvidia-uvm \
-v /home/ziosi/tests/$CONTAINER_NAME/results/:/sl-exchanges/ \
me/starlab-cuda-340.46-6.0.37-2015-06-15

# Start the container
docker start $CONTAINER_NAME

# Execute the script in the exchange folder
docker exec -i $CONTAINER_NAME bash -c &amp;quot;/sl-exchanges/run.sh&amp;quot;

docker stop $CONTAINER_NAME
docker rm $CONTAINER_NAME
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second may contain the instructions to run StarLab commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -x

for RUN in $(ls create_*.sh); do
	echo &amp;quot;Run $RUN&amp;quot;;
	/slbin-GPU/makeking -n 1000 -w 5 -i -u  &amp;gt; /sl-exchanges/makeking-$RUN.out;
	/slbin-GPU/makemass -f 8  -l 0.1 -u 150 &amp;lt; /sl-exchanges/makeking-$RUN.out &amp;gt; /sl-exchanges/makemass-$RUN.out;
	/slbin-GPU/add_star -R 1 -Z 0.10        &amp;lt; /sl-exchanges/makemass-$RUN.out &amp;gt; /sl-exchanges/add_star-$RUN.out;
	/slbin-GPU/set_com -r 5 0 0 -v 0 1 0    &amp;lt; /sl-exchanges/add_star-$RUN.out &amp;gt; /sl-exchanges/set_com-$RUN.out;
	/slbin-GPU/scale -R 1 -M 1  &amp;lt; /sl-exchanges/set_com-$RUN.out &amp;gt; /sl-exchanges/ics-$RUN.txt;
	/slbin-GPU/kira -t 500 -d 1 -D 1 -f 0 -n 10 -e 0 -B -b 1 &amp;lt; /sl-exchanges/ics-$RUN.txt &amp;gt; /sl-exchanges/out-$RUN.txt 2&amp;gt; /sl-exchanges/err-$RUN.txt;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where I take advantage of the fact that I wrote a script to loop over different
simulations to be ran.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s it!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Starlab simulations with `sltoos`</title>
      <link>http://brunettoziosi.eu/posts/starlab-simulations-with-sltools/</link>
      <pubDate>Wed, 03 Jun 2015 10:55:02 +0200</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/starlab-simulations-with-sltools/</guid>
      <description>&lt;p&gt;Create on or more simulation configuration(s), for example&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
	&amp;quot;Runs&amp;quot;: 50,
	&amp;quot;Comb&amp;quot;: 86, 
	&amp;quot;Ncm&amp;quot; : 10000,
	&amp;quot;Fpb&amp;quot; : 0.05,
	&amp;quot;W&amp;quot;   : 3,
	&amp;quot;Z&amp;quot;   : 0.10,
	&amp;quot;Rv&amp;quot;: 5, 
	&amp;quot;EndTime&amp;quot; : 500,
	&amp;quot;Machine&amp;quot; : &amp;quot;yourCluster&amp;quot;,
	&amp;quot;UserName&amp;quot; : &amp;quot;yourUserName&amp;quot;,
	&amp;quot;PName&amp;quot;: &amp;quot;project&amp;quot;,  
	&amp;quot;BinFolder&amp;quot;: &amp;quot;/home/yourUserName/bin/&amp;quot;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Runs&lt;/code&gt; is the number of random realizations you want to simulate&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Comb&lt;/code&gt; is the number that identify of this particoular parameter set&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ncm&lt;/code&gt; is the number of center of masses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Fpb&lt;/code&gt; is the primordial binary fraction (how many stars are binaries at the beginning of the simulation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;W&lt;/code&gt; is the central adimensional potential&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Z&lt;/code&gt; is the metallicity in terms of the solar metallicity (only available in the Mapelli+2013 Starlab version)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Rv&lt;/code&gt; is the initial virial radius of the cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EndTime&lt;/code&gt; preliminary timestep when to stop the simulation, you can resume it later&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Machine&lt;/code&gt; name of the machine you are running on&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UserName&lt;/code&gt; is your username on that machine&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PName&lt;/code&gt; is the project your hours are accounted on&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BinFolder&lt;/code&gt; is the path where to find the binaries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sltools createICs -v -A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now sltools will create a folder for each parameter combination, copy the configuration file inside and create
as much script files as you need to create the iitial conditions.&lt;/p&gt;

&lt;p&gt;Each file would resemble something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -xeu
makeking -n 10000 -w 5 -i -u \
| makemass -f 8  -l 0.1 -u 150 \
| makesecondary -f 0.1 -q -l 0.1 \
| add_star -R 1 -Z 0.10 \
| scale -R 3 -M 1\
| makebinary -f 2 -o 1 -l 1 -u 107836.09 \
&amp;gt; ics-comb87-TFno-Rv3-NCM10000-fPB01-W5-Z010-run01-rnd00.txt

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can run the generated script with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;for RUN in $(ls create_*.sh); do bash $RUN; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OR&lt;/p&gt;

&lt;p&gt;you can run them using the docker container you can build starting from [this image]()
or from this [Dockerfile]() or following &lt;a href=&#34;../dockerized-starlab&#34;&gt;these instructions&lt;/a&gt;. If you plan to use the GPUs the second method is recommendend
since a GPU enable docker image is system-dependent (thank you nVidia&amp;hellip;).&lt;/p&gt;

&lt;p&gt;Now you can upload your fresh ICs to the cluster of your choice.&lt;/p&gt;

&lt;p&gt;Once everything is in its place, connect to the cluster and run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sltools css -A -m &amp;lt;machine&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to create the files needed to submit the jobs to the batch system.
&lt;code&gt;machine&lt;/code&gt; is the name of the cluster. Until now only few clusters are
recognized, because I need to set few parameters to match the system configuration.
I&amp;rsquo;ll be glad to add your own machine, or maybe I&amp;rsquo;ll create a template system
able to read from a configuration file.&lt;/p&gt;

&lt;p&gt;Once the batch manager scripts are created, just run everything with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sltools pbsLaunch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and wait for the jobs to complete.
When they finish, you can restart the simulations to make them run untile 100 Myr is
reached. To do this, simply run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sltools relaunch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sltools relaunch -e &amp;lt;end-time&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to specify when the simulation should end.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>StarLab (GPU) old guide</title>
      <link>http://brunettoziosi.eu/pages/research/utils/starlab-gpu-old-guide/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/pages/research/utils/starlab-gpu-old-guide/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;../../../../starlab-gpu-installation&#34;&gt;Click here for the &lt;strong&gt;new guide!!!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;to-install-cuda-you-can-try-with-the-cuda-packages-in-the-ubuntu-repositories:3cdfe2013ffbdc941d30732c89ec1c54&#34;&gt;To install CUDA you can try with the CUDA packages in the Ubuntu repositories.&lt;/h2&gt;

&lt;p&gt;If they fail, you have to download CUDA from ****&lt;/p&gt;

&lt;p&gt;To locate the CUDA files you can try:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;locate cuda | grep nvcc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;locate cuda | grep include&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;locate cuda | grep include | grep toolkit&lt;/code&gt; (for the SDK files of the new release)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;locate cuda | grep lib | grep cudaart&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- TEASER_END --&gt;

&lt;h2 id=&#34;sapporo:3cdfe2013ffbdc941d30732c89ec1c54&#34;&gt;Sapporo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;in setup_sapporo.sh change&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LD_LIBRARY_PATH=/usr/local/cuda-5.0/:/usr/local/cuda-5.0/samples/common/inc:/usr/include/boost/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(installation of the binary drivers from the NVIDIA site) to&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LD_LIBRARY_PATH=/usr/include/:/usr/lib/nvidia-cuda-toolkit/include/:/usr/include/boost/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(ubuntu CUDA distro packages)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;in Makefile put the right path in &lt;code&gt;NVCC := /usr/bin/nvcc&lt;/code&gt; and be sure to have the right
paths in&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDAPATH    := /usr/include/
#/usr/local/cuda-5.0
CUDASDKPATH := /usr/lib/nvidia-cuda-toolkit/include/
#/usr/local/cuda-5.0/samples/common/inc
CUDAINCLUDE := -I$(CUDAPATH)/include -I$(CUDASDKPATH)
# RE - added these path/includes (added to NVCCFLAGS and CXXFLAGS, too)
BOOSTPATH := /usr/include/boost 
BOOSTINCLUDE := -I$(BOOSTPATH)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the commented path refers to the binary installation from the NVIDIA site.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Launch &lt;code&gt;bash ./setup_sapporo.sh&lt;/code&gt; and if you get&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;host_evaluate_gravity.cu:3: fatal error: multithreading.h: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;puth &lt;code&gt;multithreading.h&lt;/code&gt; in the sapporo folder and then in &lt;code&gt;host_evaluate_gravity.cu&lt;/code&gt; change&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;multithreading.h&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;quot;multithreading.h&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so c++ can find the header in the current directory.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;if all is going right, by running again &lt;code&gt;bash setup_sapporo.sh&lt;/code&gt; you should
obtain something like&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/bin/rm -rf *.o *.cu_o libsapporo.a
/bin/rm -rf test_gravity_block test_gravity_N2ngb
g++ -O3 -DNGB -I/usr/include//include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost    -c -o GPUWorker.o GPUWorker.cc
g++ -O3 -DNGB -I/usr/include//include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost  -c sapporo.cpp -o sapporo.o
sapporo.cpp: In member function ‘int sapporo::open(int)’:
sapporo.cpp:40:25: warning: ignoring return value of ‘char* fgets(char*, int, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]
sapporo.cpp:42:25: warning: ignoring return value of ‘char* fgets(char*, int, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]
sapporo.cpp:67:24: warning: ignoring return value of ‘char* fgets(char*, int, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]
g++ -O3 -DNGB -I/usr/include//include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost  -c send_fetch_data.cpp -o send_fetch_data.o
g++ -O3 -DNGB -I/usr/include//include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost  -c sapporoG6lib.cpp -o sapporoG6lib.o

*/usr/bin/nvcc -O0 -g -D_DEBUG  -maxrregcount=64 -I/usr/include//include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost  -c host_evaluate_gravity.cu -o host_evaluate_gravity.cu_o
 Iar qv libsapporo.a GPUWorker.o sapporo.o send_fetch_data.o sapporoG6lib.o host_evaluate_gravity.cu_o
ar: creating libsapporo.a
a - GPUWorker.o
a - sapporo.o
a - send_fetch_data.o
a - sapporoG6lib.o
a - host_evaluate_gravity.cu_o
ranlib libsapporo.a
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;to test the compilation run&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;test_gravity_N2ngb 900
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;test_gravity_block 900
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where 900 is the number of particles involved in the test. You can choose the number you prefer
but the test fail if the number is less than ~800.&lt;/p&gt;

&lt;h2 id=&#34;starlab:3cdfe2013ffbdc941d30732c89ec1c54&#34;&gt;StarLab&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;change configure CUDA lines:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDAINC=&amp;quot;-I/usr/include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost&amp;quot; 
CUDALIB=&amp;quot;-L/usr/libx86_64-linux-gnu/ -lcudart&amp;quot;
LIBS=&amp;quot;$CUDAINC $CUDALIB -lboost_system -lboost_thread -lpthread -DNGB&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;and change local/grape.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDAINC=&amp;quot;-I/usr/include -I/usr/lib/nvidia-cuda-toolkit/include/ -I/usr/include/boost&amp;quot; 
CUDALIB=&amp;quot;-L/usr/libx86_64-linux-gnu/ -lcudart&amp;quot;

# CUDAINC=&amp;quot;-I/usr/local/cuda-5.0/include -I/usr/local/cuda-5.0/samples/common/inc -I/usr/include/boost&amp;quot; 
# CUDALIB=&amp;quot;-L/usr/local/cuda-5.0/lib64/ -lcudart&amp;quot; 

LIBS1=&amp;quot;$CUDAINC $CUDALIB -lboost_system -lboost_thread -lpthread -DNGB&amp;quot;

#g++ -O3 $flags -g -o test_gravity_block test_gravity_block.cpp -L. -lsapporo $CUDAINC $CUDALIB -lboost_thread
g++ -O3 $flags -g -o test_gravity_block test_gravity_block.cpp -L. -lsapporo $CUDAINC $CUDALIB $LIBS1

# Where to find GRAPE libraries:
# GRAPE_LDFLAGS_=&#39;-L/home/mapelli/MICMAP/programmi/sapporo161_release/&#39;
GRAPE_LDFLAGS_=&#39;-L/home/ziosi/Code/Mapelli/starlab/sapporo/sapporo161_release&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;run &lt;code&gt;make clean&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;run &lt;code&gt;./configure --without-fortran&lt;/code&gt; (&lt;code&gt;--without-f77&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;make&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;now you can find the &lt;code&gt;kira&lt;/code&gt; binary in &lt;code&gt;/usr/local/bin&lt;/code&gt; or&lt;code&gt;/usr/bin&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;run&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./kira -t 500 -d 1 -D 1 -b 1 \
             -n 10 -e 0.000 -B   \
	 &amp;lt;  cineca95_bin_N5000_frac01_W5_Z001_IC.txt \
	 &amp;gt; new_cineca95_bin_N5000_frac01_W5_Z001.txt \
	 2&amp;gt; ew_cineca95_bin_N5000_frac01_W5_Z001.txt
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #9: Gadget-2 (N-body part)</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</link>
      <pubDate>Mon, 20 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-9-gadget-2-n-body-part/</guid>
      <description>&lt;p&gt;Here I would like to do a brief presentation of the main features of Gadget-2.&lt;br /&gt;
Gadget-2 (&lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; target=&#34;_blank&#34; title=&#34;Gadget2 homepage&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/my-first-gadget2-tests/&#34; target=&#34;_blank&#34; title=&#34;My first Gadget-2 tests&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2005.09655.x/abstract;jsessionid=DED86CDB5CD8A572F3631F0C42828086.d01t03&#34; target=&#34;_blank&#34; title=&#34;Gadget-2 paper&#34;&gt;here&lt;/a&gt;) is a cosmological simulation code developed primarily by &lt;a href=&#34;http://www.mpa-garching.mpg.de/~volker/&#34; target=&#34;_blank&#34; title=&#34;Volker Springel&#39;s homepage&#34;&gt;Volker Springel&lt;/a&gt;. It is a &lt;a href=&#34;http://www.brunettoziosi.eu/blog/wordpress/cosmological-simulations-3-calculating-the-force/&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #3: force calculation!&#34;&gt;TreePM&lt;/a&gt; code so it splits forces between long-range (PM part) and short-range (tree part using multipole expansion to approximate the force of distant particles groups).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The tree&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gadget-2 uses a&lt;a href=&#34;http://en.wikipedia.org/wiki/Octree&#34; target=&#34;_blank&#34; title=&#34;oct-tree&#34;&gt; BH oct-tree&lt;/a&gt; (see also &lt;a href=&#34;http://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation&#34; target=&#34;_blank&#34; title=&#34;Barnes&amp;amp;Hut simulation on wikipedia&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.artcompsci.org/~makino/softwares/C++tree/index.html&#34; target=&#34;_blank&#34; title=&#34;NBODY, an implementation of Barnes-Hut treecode&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://ifa.hawaii.edu/~barnes/software.html&#34; target=&#34;_blank&#34; title=&#34;Barnes&#39; page&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.cita.utoronto.ca/~dubinski/treecode/treecode.html&#34; target=&#34;_blank&#34; title=&#34;A parallel tree code explenation&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.prism.gatech.edu/~gth716h/BNtree/&#34; target=&#34;_blank&#34; title=&#34;Barnes-Hut Implementation in HTML/Javascript&#34;&gt;here&lt;/a&gt;) to calculate the short-range forces in the real space. This choice was done because this type of tree, compared to other types (KD-Tree, &amp;hellip;), requires the creation of less nodes, that imply that less memory is used. It&amp;rsquo;s characterized by eight sub-nodes for each node and has only one particle in each leaf. The code decides to open a leaf according to a certain leaf opening criterion based on the estimated force error. The force for distant groups of particles is approximated with the multipole (here octopole) of the tree node and the error depends on the dimensions and the distances of the node considered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PM part&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The PM part of the code is used to calculate the long-range forces. The algorithm is something like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CIC (cloud-in-cell) assignment is used to construct the mass density field on to the mesh from the information on the particles&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the discrete FT of the mesh is multiplied for the Green function for the potential in periodic boundaries (modified with the exponential truncation for the force splitting)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;deconvolution for the CIC kernel twice: the first for the smoothing effect of CIC assignment, the second for the force interpolation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$\mathrm{FT}^{-1}$ to obtain the potential on the mesh&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;finite differentiate the potential to obtain the forces&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;interpolate the forces to the particles positions using CIC&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: real-to-complex FT are used to save times and memory respect to full complex transforms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time step&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This type of code has a large dynamic range in time scale, from the denser regions where the evolution is rapid to the less denser regions in which the evolution occur slower so we can describe it with larger time resolution. In this scenario evolving all particles with the smallest time-scale is a waste of time and computational resources. Because using different time-steps for each particle add instabilities to the system, Gadget-2 separates time-step between long-range (longer time step) and short-range (shorter time step) force computations. The perturbation of the system for different time-steps is related to the symplectic nature of the system, but I still have not understood what it really means and implies! I know that it refers to the phase space volume and has effect on the information conservation. May be in the future I&amp;rsquo;ll write a post about this!&lt;br /&gt;
Despite these arguments, sometimes individual time step are allowed because they perturb the system but not the symplecticity of the single particle.&lt;br /&gt;
In the normal integration mode time-steps are discretized in a power of two hierarchy and particles can always move to smaller time steps but to longer time steps only in subsequent step, synchronized with higher time-steps. Alternatively the code can populate time-steps discretizeing them as integer multiples of the minimum time-step among the particles set. This lead to a more homogeneous distribution of particles across the time-line which can simplify work load balancing.&lt;br /&gt;
The integration is performed using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Leapfrog_integration&#34; target=&#34;_blank&#34; title=&#34;Leapfrog method&#34;&gt;leapfrog method&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually the parallelization distributes particles across the CPUs using an orthogonal domain decomposition but in this way the trees built-in each domain depend on the domain geometry. Because the force depend on the tree (through the multipole expansion of the mass distribution) the force can be different if you change the number of processors.&lt;br /&gt;
Gadget-2 introduce a space-filling fractal, the Peano-Hilber (PH) curve to map 3D space into a 1D curve that encompasses all the particles. Now the PH curve can be cut and each piece assigned to a CPU and in this way the force is independent of the processors number. If you cut every segment in eight pieces recursively you find again the tree decomposition, so there is a close correspondence between the decomposition obtained with the BH oct-tree and that of the PH curve.&lt;br /&gt;
The PH curve has some remarkably properties, for example points that are close along the 1D PH curve are in general close in 3D space, so the mapping preserves locality and if we cut the PH curve into segments of a certain length we obtain a domain decomposition which has the property that the spatial domains are simply connected and quite &amp;ldquo;compact&amp;rdquo; (i.e., they tend to have small surface-to-volume ratios and low aspect ratio, a highly desirable property for reducing communication costs with neighbouring domains and for speeding up the local computation).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Operations scheme&lt;/strong&gt;&lt;br /&gt;
Here a brief scheme on how the short range force calculation works on multiple processors. The PM computation uses the &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_4.html&#34; target=&#34;_blank&#34; title=&#34;Parallel FFTWs&#34;&gt;parallel FFTWs&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Compute the PH key for each particle&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sort the keys locally and split the PH curve into segments&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adjust the sorted segments to a global sort, splitting and joining segments if needed, with little communication&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Assign the particles to the processes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Construct a BH tree for the particles of each processors representing particles on other processors with pseudo-particles (acting like placeholders)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During the tree traverse (e.g. in processor A) these pseudo-particles cannot be opened, the are flagged and inserted into a list that collects all the particles that are to be sent (=requested) to the other processors (e.g. to processor B)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Processor B traverse again its local tree and send back the resulting force contribution to processor A&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>My first Gadget2 tests</title>
      <link>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</link>
      <pubDate>Sat, 07 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/my-first-gadget2-tests/</guid>
      <description>

&lt;p&gt;This post is about my first experience with the cosmological simulation code &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget2&#34;&gt;Gadget2&lt;/a&gt;. To start I followed the instructions found &lt;a href=&#34;http://astrobites.com/2011/04/02/installing-and-running-gadget-2/&#34;&gt;here&lt;/a&gt;. All I&amp;rsquo;m going to write refers to an Ubuntu/Kubuntu 11.10 installation.&lt;/p&gt;

&lt;h2 id=&#34;installation-of-gsl-and-fftw:b6a0a5ab44b8eeb8249d7bb237655d64&#34;&gt;Installation of GSL and fftw&lt;/h2&gt;

&lt;p&gt;We can download Gadget &lt;a href=&#34;http://www.mpa-garching.mpg.de/gadget/&#34; title=&#34;Gadget download&#34;&gt;here&lt;/a&gt;, the GSL (GNU scientific library) &lt;a href=&#34;http://mirror.rit.edu/gnu/gsl/gsl-1.9.tar.gz&#34; title=&#34;GSL download&#34;&gt;here&lt;/a&gt; and the FFTW (fastest Fourier transform in the West library) &lt;a href=&#34;http://www.fftw.org/fftw-2.1.5.tar.gz&#34; title=&#34;FFTW download&#34;&gt;here&lt;/a&gt;. We also need an MPI library (Open-MPI or MPICH, try install it using your package manager).&lt;br /&gt;
Following the Astrobites suggestions let&amp;rsquo;s decompress the archives with &lt;code&gt;tar -xzf &amp;amp;lt;archive name&amp;amp;gt;&lt;/code&gt;. Now we can install the libraries following the Astrobites post:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd gsl-1.9/
goldbaum@~/Documents/code/gsl-1.9: ./configure
snip: lots of diagnostic ouput
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix --enable-float
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As described &lt;a href=&#34;http://www.fftw.org/fftw2_doc/fftw_6.html#SEC69&#34; target=&#34;_blank&#34; title=&#34;FTTW installation and customization&#34;&gt;here&lt;/a&gt; is convenient to install both the single and the double precision version of the FFTW (for example to compile the initial conditions generators) with (that is, without &lt;code&gt;--enable-float&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goldbaum@~/Documents/code: cd fftw-2.1.5
goldbaum@~/Documents/code/fftw-2.1.5: ./configure --enable-mpi --enable-type-prefix
snip: lots of diagnostic output
goldbaum@~/Documents/code/gsl-1.9: make
snip: lots of compilation output
goldbaum@~/Documents/code/gsl-1.9: sudo make install
Password:
snip: lots of diagnostic output
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;play-with-gadget2:b6a0a5ab44b8eeb8249d7bb237655d64&#34;&gt;Play with Gadget2&lt;/h2&gt;

&lt;p&gt;Now it&amp;rsquo;s time to play with Gadget!:) In this code, for performance reasons, requires to specify some parameters at compile time while other can be set at run time, so that we have to customize the Makefile. This also imply that we should have separate binary files and directories for each simulation.&lt;br /&gt;
To start with something easy, we will customize one of the examples given with the code, the &amp;ldquo;galaxy&amp;rdquo; one. It simulate the collision of two galaxies using 40000 DM particles for the haloes and 20000 baryonic particles for the disks.&lt;/p&gt;

&lt;p&gt;Inside the Gadget directory we have&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Analysis
AUTHORS
COPYING
COPYRIGHT
Documentation
Gadget2
ICs
README
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the &lt;code&gt;Analysis&lt;/code&gt; folder we can fin some analysis routines provided by the author, the &lt;code&gt;Documentation&lt;/code&gt; folder contains the user guide and the original paper, and the &lt;code&gt;AUTHORS, COPYING, COPYRIGHT&lt;/code&gt; self-explanatory. The &lt;code&gt;ICs&lt;/code&gt; folder contains the initial conditions for the example simulations and the &lt;code&gt;Gadget2&lt;/code&gt; folder contains the sources and the html documentation.&lt;/p&gt;

&lt;p&gt;To be tidy and organized is better to have a folder for every simulations, so we will create a (descriptive) with everything we need to customize&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir 2012-01-07-Gadget2-galaxy_test_01
cd 2012-01-07-Gadget2-galaxy_test_01
mkdir out
cp ../ICs/galaxy_littleendian.dat ./
cp ../Gadget2/parameterfiles/galaxy.param ../Gadget2/parameterfiles/galaxy.Makefile ./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the folder Gadget2 we can find the general &lt;code&gt;Makefile&lt;/code&gt; but for now let&amp;rsquo;s use the galaxy&amp;rsquo;s one provided by the author and just copied to our position. Open it with your preferred text editor (for example, in a command line environment, &lt;code&gt;emacs -nw Makefile&lt;/code&gt;).&lt;br /&gt;
This &lt;code&gt;Makefile&lt;/code&gt; is already customized for the galaxy collision simulation and if you want to understand every option you can read the description in the guide, but we need some more customization. Here what I&amp;rsquo;ve changed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPT   +=  -DHAVE_HDF5  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so I activate the HDF5 format for the output and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#--------------------------------------- Select target computer

SYSTYPE=&amp;quot;Uno&amp;quot;
\#SYSTYPE=&amp;quot;MPA&amp;quot;
\#SYSTYPE=&amp;quot;Mako&amp;quot;
\#SYSTYPE=&amp;quot;Regatta&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster&amp;quot;
\#SYSTYPE=&amp;quot;RZG_LinuxCluster-gcc&amp;quot;
\#SYSTYPE=&amp;quot;Opteron&amp;quot;

\#--------------------------------------- Adjust settings for target computer

ifeq ($(SYSTYPE),&amp;quot;Uno&amp;quot;)
CC       =  mpicc   
OPTIMIZE =  -O3 -Wall
GSL_INCL =  -I/usr/local/include
GSL_LIBS =  -L/usr/local/lib
FFTW_INCL=  -I/usr/local/include
FFTW_LIBS=  -L/usr/local/lib
MPICHLIB =  -L/usr/lib
HDF5INCL =  
HDF5LIB  =  -lhdf5 -lz 
endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to select the set the options for my system.&lt;br /&gt;
Now we have to customize the &lt;code&gt;run/galaxy.param&lt;/code&gt; file changing it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;InitCondFile      ./galaxy_littleendian.dat
OutputDir          ./galaxy_out/
OutputListFilename ./out/output_list.txt
SnapFormat         3  %to select the HDF5 format
TimeBegin           0.0        % Begin of the simulation
TimeMax             40.0        % End of the simulation

% Output frequency
TimeBetSnapshot        0.1% original 0.5 &amp;lt;/pre&amp;gt;
    
    
Now we should go to the sources folder and compile the code with    
&amp;lt;pre&amp;gt;cd ../Gadget2
make -f 2012-01-07-Gadget2-galaxy_test_01/galaxy.Makefile
cp Gadget2 ../2012-01-07-Gadget2-galaxy_test_01/Gadget2
make clean
cd 2012-01-07-Gadget2-galaxy_test_01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command clean the build leaving only the sources files, so we are ready for a new build.&lt;br /&gt;
We can also create a script for automatize all this steps, something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
dir=$1
ics=$2
param_file=$3
mk_file=$4
CPUs=$5

if [ $# -lt 5 ] ; then
  echo &amp;quot;usage: gadget_set directory_name initial_conditions_file
parameters_file make_file number_of_CPUs&amp;quot;
  exit 0
fi

echo &amp;quot;Assuming to use $dir as the run folder,&amp;quot; 
echo &amp;quot;$ics as initial conditions,&amp;quot;
echo &amp;quot;$paramfile as parameter file, &amp;quot;
echo &amp;quot;$mk_file as makefile &amp;quot;
echo &amp;quot;and to run on $CPUs CPUs.&amp;quot;

mkdir $dir
cd $dir
mkdir out
cp ../ICs/$ics ./
cp ../Gadget2/parameterfiles/$param_file
../Gadget2/parameterfiles/mk_file ./
cd ../Gadget2
make -f ../$dir/$mk_file
cp Gadget2 ../$dir/Gadget2
make clean
cd $dir
mpirun -np $CPUs ./Gadget2 $param_file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very raw and untested script, but it&amp;rsquo;s just to give an idea.&lt;/p&gt;

&lt;p&gt;Now we are ready to start the simulation with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mpirun -np 2 ./Gadget2 galaxy.param
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;-np&lt;/code&gt; sets the number of processes/processors to be used in parallel.&lt;br /&gt;
When the simulation stops we can analyze it with the tools provided in the &lt;code&gt;Analysis&lt;/code&gt; folder or, if you like me don&amp;rsquo;t own an IDL license and don&amp;rsquo;t feel comfortable with IDL/Fortran/C for the data analysis, with something like (to be run in out/plots/):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/use/bin/env python
import sys, os
from subprocess import Popen, PIPE
from multiprocessing import Process, Queue

import numpy as np
import tables as tb
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

&amp;quot;&amp;quot;&amp;quot;This script will plot in parallel the .h5 snapshots created by Gadget2 test
runs one after the other!:).
FIXME: i need a way to wait for the final time count the end of the processes
and a way to print the status
&amp;quot;&amp;quot;&amp;quot;

# Set the max number of processes
n_procs = 3

# Set the number of snapshot to be plotted
n_snap = 401

t = time.time()

print &amp;quot;Defining workers...&amp;quot;

def worker(input, output):
    while input.qsize() != 0:
        item = input.get()
        if item[0]= 10 and item[0]&amp;amp;lt;100: j=&amp;quot;0&amp;quot;+str(item[0])
        else: j=str(item[0])
        try:
#     print &amp;quot;considering file ../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;
#     print &amp;quot;open file &amp;quot;
            h5 = tb.openFile(&amp;quot;../snapshot_&amp;quot;+j+&amp;quot;.hdf5&amp;quot;, &#39;r&#39;)
#     print &amp;quot;file opened, set variables&amp;quot;
            halo = h5.root.PartType1
            disk = h5.root.PartType2
#            print &amp;quot;setted, inizialize figure&amp;quot;
            fig2 = plt.figure()
            ax = Axes3D(fig2)
            ax.scatter(disk.Coordinates[:,0], 
                       disk.Coordinates[:,1],
                       disk.Coordinates[:,2],
                       color=&#39;red&#39;, s=0.5)
            ax.scatter(halo.Coordinates[:,0], 
                       halo.Coordinates[:,1],
                       halo.Coordinates[:,2],
                       color=&#39;blue&#39;, s=0.01)
            plt.savefig(&#39;snap_&#39;+j)
#            print &amp;quot;done, closing file&amp;quot;
            h5.close()  
#            print &amp;quot;closed&amp;quot;

        except:
            print &amp;quot;Work &amp;quot;+j+&amp;quot; not done, exit...&amp;quot;
            sys.exit()

def fill_queue(task_queue):
    for i in range(n_snap):
        task_queue.put([i])
    return task_queue

def status(proc):
    if proc.is_alive==True:
        return &#39;alive&#39;
    elif proc.is_alive==False:
        return &#39;dead&#39;
    else:
        return proc.is_alive()

print &amp;quot;Define queues...&amp;quot;

input_queue = Queue()
output_queue = Queue()

try:
    input_queue = fill_queue(input_queue)
except:
    print &amp;quot;Queue not filled, exit...&amp;quot;
    sys.exit()

procs = []

try:
    for i in range(n_procs):
        procs.append(Process(target=worker, args=(input_queue,
output_queue)))
except:
    print &amp;quot;Creating processes not complete, exit...&amp;quot;
    sys.exit()

try:
    for i in procs:
        i.start()
except:
    print &amp;quot;Start processes not complete, exit...&amp;quot;
    sys.exit()

for i in procs:
    print &amp;quot;Process &amp;quot;, i,&amp;quot; @ &amp;quot; , i.pid, &amp;quot; is &amp;quot;, status(i)

print &amp;quot;Done in &amp;quot;+str(time.time()-t)+&amp;quot; seconds.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have one image for each snapshot, and if we are interested we can produce a video with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mencoder mf://*.png -mf fps=25:type=png -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -vf scale=720:360 -oac copy -o output.mp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;youtube http://www.youtube.com/watch?v=b7HyafKMkxI&amp;amp;amp;w=560&amp;amp;amp;h=315&#34;&gt;This&lt;/a&gt; is the first basic video, with logarithmic time and perhaps there&amp;rsquo;s something wrong with the coordinates on the axes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #7: Limitations and some considerations</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-7-limitations-and-some-considerations/</link>
      <pubDate>Sun, 13 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-7-limitations-and-some-considerations/</guid>
      <description>

&lt;h2 id=&#34;limitations:be6e6b0278d604ddee51c801f8740b2c&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;In the previous posts we encountered some of the limitations of cosmological&lt;br /&gt;
simulations. Let&amp;rsquo;s review these in detail.&lt;br /&gt;
First, we can consider a simulation composed of a finite box in a bigger space but to represent a real system, this box shouldn&amp;rsquo;t be isolated so we use the periodic boundary conditions (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-2-how.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #2: how?&#34;&gt;here&lt;/a&gt;). This means that all the space around the box is filled with images of the box itself: a particle that leaves the box from one side will come in&lt;br /&gt;
from the opposite side.&lt;/p&gt;

&lt;p&gt;Second, the mass inside the box is not continuous. Instead, it is made by particles of mass of the order of $10^9$ solar masses. These particles represent collisionless fluid elements (made by a huge quantity of real particles) with a certain&lt;br /&gt;
volume and can&amp;rsquo;t be treated as solid spheres. When two simulation particles are&lt;br /&gt;
separated by a distance smaller than the radius of the volumes they represents&lt;br /&gt;
they must feel less than the force coming from the entire mass (thanks to the&lt;br /&gt;
Gauss/Birkhoff&amp;rsquo;s theorem). To do this we soften (read &amp;ldquo;we reduce&amp;rdquo;) the force at&lt;br /&gt;
such small scales (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-2-how.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #2: how?&#34;&gt;here&lt;/a&gt;). Third, time is not continuous and its discreteness was also treated (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-4-moving.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations: #4: Moving the particles!&#34;&gt;here&lt;/a&gt;) with some&lt;br /&gt;
criteria to decide the time steps.&lt;br /&gt;
Until now, however, we haven&amp;rsquo;t consider the effects of taking into account&lt;br /&gt;
initial density fluctuations over a range of scales that is finite. In&lt;br /&gt;
addition to this, the finite size of the box pose a limit on the force&lt;br /&gt;
resolution, because fluctuations on scales bigger than the box side will not&lt;br /&gt;
included in the simulation due to the way the Fourier transforms act on a&lt;br /&gt;
period box. Some tests in literature show that the exclusion of small&lt;br /&gt;
scales shouldn&amp;rsquo;t affect too much large scales when they reach the non linear&lt;br /&gt;
regime but this not holds for the exclusion of large scales, those scales bigger&lt;br /&gt;
than the box side. Following Bagla, the large scale exclusion should not&lt;br /&gt;
disturb the formation of small haloes but could change their distribution.&lt;br /&gt;
This effect will appear as an underestimation of the correlation function. Bagla&lt;br /&gt;
finds that the best way of quantifying the effects of long wave modes is to&lt;br /&gt;
check whether including them in the simulation will change the number of&lt;br /&gt;
massive haloes or not and this can be estimated using the Press-Schecther mass&lt;br /&gt;
function.&lt;br /&gt;
In Tormen&amp;amp;Bertschinger (1996) the missing power on large scales will cause&lt;br /&gt;
something like a statistical cosmic bias decreasing the number of high-density&lt;br /&gt;
regions, the strength of the clustering and the amplitude of the peculiar&lt;br /&gt;
velocities.&lt;br /&gt;
Methods have been developed to take the missing &amp;ldquo;larger than the box&amp;rdquo; wave modes&lt;br /&gt;
into account and we will have a look on these in a future post.&lt;/p&gt;

&lt;h2 id=&#34;some-considerations:be6e6b0278d604ddee51c801f8740b2c&#34;&gt;Some considerations&lt;/h2&gt;

&lt;p&gt;As we have seen (&lt;a href=&#34;http://brunettoziosi.blogspot.it/2011/11/cosmological-simulations-1-why-and-what.html&#34; target=&#34;_blank&#34; title=&#34;Cosmological simulations #1: why and what?&#34;&gt;here&lt;/a&gt;) N-body cosmological simulations&lt;br /&gt;
are useful to understand aspects of non-linear gravitational clustering,&lt;br /&gt;
since it&amp;rsquo;s not possible to carry out laboratory experiments in gravitational&lt;br /&gt;
dynamics and the analytic models fail when the system reach the non linear&lt;br /&gt;
regime, i.e. when the density contrast overcome the unity. Related with&lt;br /&gt;
cosmological simulations there are a pair of aspects that Bagla underlines in its&lt;br /&gt;
articles that interesting to consider.&lt;br /&gt;
The first issue is whether or not the gravitational clustering&lt;br /&gt;
erase memory of initial conditions. Is there a one-to-one correspondence between&lt;br /&gt;
some characterization of initial perturbations and the final state?&lt;br /&gt;
N-body simulations shows that gravitational clustering does not erase memory of&lt;br /&gt;
the initial conditions, the final power spectrum is a function of the initial&lt;br /&gt;
power spectrum and this relationship can be written as a one-step mapping and&lt;br /&gt;
the functional form of this mapping depends on the initial power spectrum.&lt;br /&gt;
However density profiles of massive haloes have a form independent of&lt;br /&gt;
initial conditions but there is a considerable scatter in density profiles&lt;br /&gt;
obtained from N-body simulations and it is difficult to state whether a given&lt;br /&gt;
functional form is always the best fit or not. I must admit that these last concepts are not very clear to me at the moment, and that I trust Bagla but I will deepen them as soon as possible to be able to comfortably master them.&lt;br /&gt;
The second question is if it is possible to predict the masses and distribution&lt;br /&gt;
of haloes that form as a result of gravitational clustering.&lt;br /&gt;
The initial density field is taken to be a Gaussian random field and for&lt;br /&gt;
hierarchical models the simple assumption that each peak undergoes collapse&lt;br /&gt;
independent of the surrounding density distribution can be used to estimate the&lt;br /&gt;
mass function and several related quantities but N-body simulations shows that&lt;br /&gt;
this simple set of approximations is incorrect. However, the resulting mass&lt;br /&gt;
function estimation is fairly accurate over a wide range of masses. Merger rates&lt;br /&gt;
can be thus computed using the extended Press-Schecther formalism. Modifying&lt;br /&gt;
some of this assumption can lead to improved predictions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://iopscience.iop.org/0004-637X/472/1/14&#34; target=&#34;_blank&#34; title=&#34;G. Tormen and E. Bertschinger, Adding long wavelenght modes to an N-body simulation&#34;&gt;Giuseppe Tormen and Edmund Bertschinger, Adding long wavelenght modes to an N-body simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/full/1997MNRAS.286...38C&#34; target=&#34;_blank&#34; title=&#34;S. Cole, Adding long-wavelength power to N-body simulations&#34;&gt;S. Cole, Adding long-wavelength power to N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #1: why and what?</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-1-why-and-what/</link>
      <pubDate>Wed, 02 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-1-why-and-what/</guid>
      <description>&lt;p&gt;This is the first of a series of posts dedicated to cosmological simulations!&lt;br /&gt;
&lt;br /&gt;
I do this because, as stressed by my PhD advisor,
I need to practice in explaining in a clear way specialized knowledge and in
linking it with its background and motivations. Also I would like to keep track
of my progress and of what I&amp;rsquo;m learning!&lt;br /&gt;
&lt;br /&gt;
So, let&amp;rsquo;s start with &amp;ldquo;Why we need cosmological simulations? What are they?&amp;rdquo;!&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the night sky: if we are so lucky to be in a dark site like
the mountains or a desert, we can see the stars, and our Galaxy, The Milky Way.
With a little telescope we can also see other galaxies, like Andromeda. We can
find them in group of galaxies or clusters of galaxies. On bigger scales these
form sheets and filaments as you can see in the figure (taken from the Millenium
simulation).&lt;br /&gt;
&lt;br /&gt;
&lt;p  align=&#34;center&#34; &gt;&lt;img alt=&#34;&#34; class=&#34;alignnone&#34; height=&#34;400&#34; src=&#34;../../files/supercube.jpg&#34; title=&#34;A figure of the output of the Milleniun Simulation&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;br /&gt;
&lt;br /&gt;
Theoretical models, widely accepted, say that these structures formed from initial
small density fluctuations in the matter, grew under their self gravity, lead by a
special type of matter that can interact only through gravity, called &amp;ldquo;dark matter&amp;rdquo;.&lt;br /&gt;
&lt;br /&gt;
A homogeneous and isotropic universe is described by the
&lt;a href=&#34;http://en.wikipedia.org/wiki/Friedmann_equations&#34; target=&#34;_blank&#34; title=&#34;Friedmann equations&#34;&gt;Friedmann equations&lt;/a&gt; in general relativity. Until the density fluctuations are small we can treat them as perturbations in a Friedmann universe. If the matter under consideration is non-relativistic (and it is!) and on scales smaller than those of the observable universe we can study the evolution of these perturbations in the Newtonian limit. We also consider gravity as the only interaction to be taken into account for now. On large scales (more than some &lt;a href=&#34;http://en.wikipedia.org/wiki/Parsec&#34; target=&#34;_blank&#34; title=&#34;megaparsecs&#34;&gt;kiloparsecs&lt;/a&gt;) this is not a bad approximation as gravity is the only interaction working efficiently in driving the evolution of the fluctuations on that scales.&lt;br /&gt;
&lt;br /&gt;
We have good analytic models for the evolution of these perturbation until the
density contrast (we call density contrast the quantity &lt;code&gt;$\delta(\vec x,t)=(\rho(\vec x, t)-\rho_{bg}(\vec x, t))/\rho_{bg}(\vec x, t)$&lt;/code&gt;
where &lt;code&gt;$\rho_{bg}(\vec x, t)$&lt;/code&gt; is the background density at given position and time)
is smaller than the unity.
This is the &amp;ldquo;linear regime&amp;rdquo;.
We call it &amp;ldquo;linear&amp;rdquo; because we can describe the system using first order
perturbations and the solutions we find are in good agreement with the exact solutions.
When the density contrast reaches and exceeds the unity, perturbations become
&amp;ldquo;non-linear&amp;rdquo; and the analytic models break.&lt;br /&gt;
&lt;br /&gt;
Cosmological N-body simulations are then the only way we have to study perturbations
in the non-linear regime. Note that when the fluctuations collapse forming what we
call a &amp;ldquo;halo&amp;rdquo;, the density contrast is of the order of 100. Cosmological simulations
are called &amp;ldquo;N-body&amp;rdquo; because they involve the calculation of the (gravitational)
force among all the bodies (particles) of the simulation.&lt;br /&gt;
&lt;br /&gt;
With cosmological simulations we can also play with the initial conditions of our
model of the universe, change its contents, &amp;hellip; and see what will happen. It&amp;rsquo;s the
closest thing to a laboratory that we have.&lt;br /&gt;
&lt;br /&gt;
Using different techniques is also possible to include non-gravitational effects
in the simulations, such as gas hydrodynamics, star formation, and so on.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;em&gt;Reference&lt;/em&gt;:&amp;nbsp;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cosmological simulations #2: how?</title>
      <link>http://brunettoziosi.eu/posts/cosmological-simulations-2-how/</link>
      <pubDate>Wed, 02 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://brunettoziosi.eu/posts/cosmological-simulations-2-how/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s try now to understand how simulations can be set up.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;First of all note that dark matter density dominates over ordinary (baryonic) matter at all times and we expect that ordinary matter follows the gravity of dark matter on large scales, so we can start considering only dark matter in our simulation.&lt;br /&gt;
Moreover, cosmological simulations differ from other N-body simulations because they should to be able to manage comoving coordinates, i.e. coordinates that expand with the universe, since the universe is expanding.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In a cosmological simulation every particle represents a large number of dark matter particles, so the interaction between two particles represents the interaction between two &amp;ldquo;fluid elements&amp;rdquo;, and this must be collisionless.&lt;br /&gt;
Moreover, a point mass in the simulation represents a mass in a certain volume, and this volume change during the simulation due to the universe expansion and the clustering. This also imply that, at scales comparable to their physical size, they should feel less gravitational force than two point particles. When the two particles are separated by a distance comparable to the dimension of the fluid elements they represents, these would not feel the gravity of all the mass associated with the particles, as suggested by the Gauss theorem. This is because when you are into a spherical distribution of mass you only experience the gravitational force of the mass inside the radius corresponding to your distance from the center.&lt;br /&gt;
To take care of this we decrease the strength of the force at small scales.&lt;br /&gt;
Some example:&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;GIF2: 6.6 kpc/h softening with a mean interparticle separation 110 Mpc/h / 400 particles = 275 kpc/h&lt;/li&gt;
&lt;br /&gt;
&lt;li&gt;Millennium: 5 kpc/h softening with a mean interparticle separation 500 Mpc/h / 2160 particles = 231 kpc/h&lt;/li&gt;
&lt;br /&gt;
&lt;li&gt;Millennium II: 1 kpc/h softening with a mean interparticle separation 100 Mpc/h / 2160 particles = 46 kpc/h&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is called &amp;ldquo;force softening&amp;rdquo;. There are several ways to implement this and one should pay attention because a softening length smaller than the interparticle separation would lead to two-body relaxation problems. With the term &amp;ldquo;two-body relaxation&amp;rdquo; we mean the effects that arise when to particles mainly feels each other instead of feeling the global field, or feel more mass than what they should (they feel the mass of the entire fluid elements instead of the mass inside the radius corresponding to their distance). Usually the effects are of two types: the two particles can start to orbit faster and faster with decreasing separation, or they can experience a gravitational sling and be pulled apart (two-body scattering).&lt;br /&gt;
The two-body relaxation modifies the density profiles of dark matter haloes (the dark matter structures formed by gravitational collapse of dark matter) making them smoother. The form of the softening is also important: historically there are two main softening, one is Gaussian and the other is a cubic spline.  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now we can start analyzing the structure of a cosmological N-body code. It consists in two main parts: the first computes the force field for a given configuration of particles, the second moves the particles according to the computed force field. The two modules are called at each step to ensure that the force field and the particles trajectories evolve in a self-consistent manner. Before starting the simulation we also need to set up the initial conditions and afterwards we have to write the output.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In setting up our simulation there are some things we should consider.&lt;br /&gt;
Our simulation represents a small box in the whole universe, but it isn&amp;rsquo;t an isolated system so it feels the gravitational field of the rest of the universe. To take account of these we can set up periodic boundary conditions. This implies that space outside the simulation box is tiled it with copies (images) of the box. In this way particles near the edges are attracted not only by the matter in the box but also by the particles in its images.&lt;br /&gt;
&lt;br /&gt;
&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img alt=&#34;&#34; height=&#34;500&#34; src=&#34;http://isaacs.sourceforge.net/phys/images/these-seb/pbc-seb.png&#34; style=&#34;margin-left: auto; margin-right: auto;&#34; title=&#34;Periodic boundary conditions example&#34; width=&#34;500&#34; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;&lt;span style=&#34;font-size: small; text-align: -webkit-auto;&#34;&gt;Periodic boundary conditions example taken from &lt;a href=&#34;http://isaacs.sourceforge.net/phys/pbc.html&#34;&gt;http://isaacs.sourceforge.net/phys/pbc.html&lt;/a&gt;.&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With periodic boundary conditions one should make sure that there isn&amp;rsquo;t a dominant object in its volume to avoid too large influence by its periodic copies.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As written above, we need cosmological simulations to study scales where perturbations are not linear, to compare results with the real universe. To do this we must probe a large range of scales, so the mass of individual particles must be smaller than the mass of the smallest structure of interest. On the other hand the number of particles must be sufficient to cover the range of   masses involved in galaxy clustering. As of 2011 a typical large simulation have of order of billion of particles.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With such large number of particles the most time consuming operation in the simulation is force calculation; therefore a number of algorithms  have been developed to avoid direct calculations, unfeasible even on the most powerful computer.&lt;br /&gt;
&lt;br /&gt;
&lt;em&gt;References&lt;/em&gt;:&lt;br /&gt;
&lt;ul&gt;&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ias.ac.in/currsci/apr102005/1088.pdf&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla, Cosmological N-body simulation: Techniques, scope and status&#34;&gt;J. S. Bagla, Cosmological N-body simulation: Techniques, scope and status&lt;/a&gt;&lt;/li&gt;
&lt;br /&gt;
&lt;li&gt;&lt;a href=&#34;http://adsabs.harvard.edu/abs/1991ComPh...5..164B&#34; target=&#34;_blank&#34; title=&#34;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&#34;&gt;J.S. Bagla and T. Padmanabham, Cosmological N-body simulations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>